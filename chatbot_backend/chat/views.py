from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
import logging
import json
import openai
import anthropic
from groq import Groq
from django.conf import settings
from bs4 import BeautifulSoup
logger = logging.getLogger(__name__)
import re
import json
import logging

def fetch_and_clean_url(url, timeout=10):
    """
    ì£¼ì–´ì§„ URLì˜ HTMLì„ ìš”ì²­í•´, ìŠ¤í¬ë¦½íŠ¸Â·ìŠ¤íƒ€ì¼ ì œê±° í›„ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    resp = requests.get(url, timeout=timeout)
    resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "html.parser")
    # ìŠ¤í¬ë¦½íŠ¸Â·ìŠ¤íƒ€ì¼Â·ë„¤ë¹„ê²Œì´ì…˜ íƒœê·¸ ì œê±°
    for tag in soup(["script", "style", "header", "footer", "nav"]):
        tag.decompose()

    text = soup.get_text(separator="\n")
    # ë¹ˆ ì¤„ ì œê±°
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    return "\n".join(lines)

# Add this function to your ChatBot class or as a standalone function
def sanitize_and_parse_json(text, selected_models, responses):
    """
    Sanitize and parse the JSON response from AI models.
    Handles various edge cases and formatting issues.
    """
    import re
    import json
    import logging
    
    logger = logging.getLogger(__name__)
    
    try:
        # Step 1: Basic cleanup
        text = text.strip()
        
        # Step 2: Handle code blocks
        if text.startswith('```json') and '```' in text:
            text = re.sub(r'```json(.*?)```', r'\1', text, flags=re.DOTALL).strip()
        elif text.startswith('```') and text.endswith('```'):
            text = text[3:-3].strip()
            
        # Step 3: Extract JSON object if embedded in other text
        json_pattern = r'({[\s\S]*})'
        json_matches = re.findall(json_pattern, text)
        if json_matches:
            text = json_matches[0]
            
        # Step 4: Handle escaped backslashes in the text
        # First identify all occurrences of escaped backslashes followed by characters like "_"
        text = re.sub(r'\\([_"])', r'\1', text)
        
        # Step 5: Attempt to parse the JSON
        result = json.loads(text)
        
        # Ensure the required fields exist
        required_fields = ["preferredModel", "best_response", "analysis", "reasoning"]
        for field in required_fields:
            if field not in result:
                if field == "best_response" and "bestResponse" in result:
                    result["best_response"] = result["bestResponse"]
                else:
                    result[field] = "" if field != "analysis" else {}
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        logger.error(f"ì›ë³¸ í…ìŠ¤íŠ¸: {text[:200]}..." if len(text) > 200 else text)
        
        # Advanced recovery attempt for malformed JSON
        try:
            # Remove problematic escaped characters
            fixed_text = text.replace("\\_", "_").replace('\\"', '"')
            
            # Try to fix common issues with JSON (missing quotes, commas, etc.)
            fixed_text = re.sub(r'([{,])\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', fixed_text)
            fixed_text = re.sub(r':\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*([,}])', r':"\1"\2', fixed_text)
            
            # Handle unclosed strings
            for match in re.finditer(r':\s*"([^"\\]*(\\.[^"\\]*)*)', fixed_text):
                if not re.search(r':\s*"([^"\\]*(\\.[^"\\]*)*)"', match.group(0)):
                    pos = match.end()
                    fixed_text = fixed_text[:pos] + '"' + fixed_text[pos:]
            
            result = json.loads(fixed_text)
            logger.info("âœ… Recovered JSON after fixing format issues")
            return result
        except:
            # Last resort: construct a sensible fallback response
            error_analysis = {}
            for model in selected_models:
                model_lower = model.lower()
                error_analysis[model_lower] = {"ì¥ì ": "ë¶„ì„ ì‹¤íŒ¨", "ë‹¨ì ": "ë¶„ì„ ì‹¤íŒ¨"}
            
            # Find the largest response to use as best_response
            best_response = ""
            if responses:
                best_response = max(responses.values(), key=len) 
            
            return {
                "preferredModel": "FALLBACK",
                "best_response": best_response,
                "analysis": error_analysis,
                "reasoning": "ì‘ë‹µ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ìµœì ì˜ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            }

# src/chatbot_backend/chat/bot.py

# import logging
# import openai
# import anthropic
# import base64
# import imghdr
# from groq import Groq
# from io import BytesIO

# logger = logging.getLogger(__name__)

# class ChatBot:
#     def __init__(self, api_key, model, api_type):
#         self.conversation_history = []
#         self.api_type = api_type
#         self.api_key = api_key

#         # Anthropic ë©€í‹°ëª¨ë‹¬ì€ Opus ëª¨ë¸ ê¶Œì¥
#         if api_type == 'anthropic' and not model.startswith('claude-3-opus-20240229'):
#             logger.info(f"Overriding Anthropic model '{model}' to 'claude-3-opus-20240229' for image support")
#             self.model = 'claude-3-opus-20240229'
#         else:
#             self.model = model

#         if api_type == 'openai':
#             openai.api_key = api_key
#         elif api_type == 'anthropic':
#             # Anthropic Python SDK ì´ˆê¸°í™”
#             self.client = anthropic.Client(api_key=api_key)
#         elif api_type == 'groq':
#             self.client = Groq(api_key=api_key)
#         else:
#             raise ValueError(f"Unsupported api_type: {api_type}")

#     def chat(self, prompt=None, user_input=None, image_file=None, analysis_mode=None, user_language=None):
#         """
#         prompt       : í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ (í‚¤ì›Œë“œ)
#         user_input   : í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ (ìœ„ì¹˜ ì¸ì)
#         image_file   : íŒŒì¼ ê°ì²´ (BytesIO, InMemoryUploadedFile ë“±)
#         analysis_mode: 'describe'|'ocr'|'objects'
#         user_language: 'ko','en'
#         """
#         text = prompt if prompt is not None else user_input
#         try:
#             logger.info(f"[{self.api_type}] Received input: {text}")

#             # ëª¨ë¸ë³„ í˜¸ì¶œ
#             if self.api_type == 'openai':
#                 # GPT-4 Vision ì§€ì›
#                 params = {
#                     'model': self.model,
#                     'messages': self.conversation_history + [{"role": "user", "content": text}],
#                     'temperature': 0.7,
#                     'max_tokens': 1024
#                 }
#                 if image_file:
#                     params['files'] = [("image", image_file)]
#                 resp = openai.ChatCompletion.create(**params)
#                 assistant_response = resp.choices[0].message.content

#             elif self.api_type == 'anthropic':
#                 # Claude 3 Opus: ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ì§€ì› via Messages API
#                 messages = []
#                 # í† í° ìˆ˜ ì„¤ì •
#                 max_tokens = 1024 if image_file else 4096
#                 if image_file:
#                     # ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ ì½ê¸° ë° ë¯¸ë””ì–´ íƒ€ì… ìë™ ê°ì§€
#                     image_file.seek(0)
#                     data_bytes = image_file.read()
#                     ext = imghdr.what(None, h=data_bytes) or 'jpeg'
#                     mime_map = {
#                         'jpeg': 'image/jpeg', 'jpg': 'image/jpeg',
#                         'png': 'image/png', 'gif': 'image/gif',
#                         'bmp': 'image/bmp', 'webp': 'image/webp'
#                     }
#                     media_type = mime_map.get(ext, 'image/jpeg')
#                     b64 = base64.b64encode(data_bytes).decode('utf-8')

#                     # ì´ë¯¸ì§€ ë¸”ë¡ê³¼ í…ìŠ¤íŠ¸ ë¸”ë¡ì„ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±
#                     image_block = {
#                         'type': 'image',
#                         'source': {'type': 'base64', 'media_type': media_type, 'data': b64}
#                     }
#                     text_block = {'type': 'text', 'text': text}
#                     content_blocks = [image_block, text_block]

#                     # ë‹¨ì¼ ë©”ì‹œì§€ì— ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ ì „ë‹¬
#                     messages.append({'role': 'user', 'content': content_blocks})
#                 else:
#                     # í…ìŠ¤íŠ¸ ì „ìš© ë©”ì‹œì§€
#                     messages.append({'role': 'user', 'content': [{'type': 'text', 'text': text}]})

#                 # Messages API í˜¸ì¶œ
#                 resp = self.client.messages.create(
#                     model=self.model,
#                     messages=messages,
#                     max_tokens=max_tokens
#                 )
#                 # ì‘ë‹µ ë¸”ë¡ì—ì„œ í…ìŠ¤íŠ¸ë§Œ í•©ì¹˜ê¸°
#                 assistant_response = ' '.join(getattr(block, 'text', '') for block in resp.content)

#             elif self.api_type == 'groq':
#                 # Groq Chat API
#                 resp = self.client.chat.completions.create(
#                     model=self.model,
#                     messages=self.conversation_history + [{"role": "user", "content": text}],
#                     temperature=0.7,
#                     max_tokens=1024
#                 )
#                 assistant_response = resp.choices[0].message.content

#             else:
#                 raise ValueError(f"Unsupported api_type: {self.api_type}")

#             # ì‘ë‹µ ê¸°ë¡ ë° ë°˜í™˜
#             self.conversation_history.append({"role": "assistant", "content": assistant_response})
#             logger.info(f"[{self.api_type}] Response: {assistant_response[:100]}...")
#             return assistant_response

#         except Exception as e:
#             logger.error(f"Error in chat method ({self.api_type}): {e}", exc_info=True)
#             raise



class ChatBot:
   def __init__(self, api_key, model, api_type):
       self.conversation_history = []
       self.model = model
       self.api_type = api_type
       self.api_key = api_key
       
       if api_type == 'openai':
           openai.api_key = api_key
       elif api_type == 'anthropic':
           self.client = anthropic.Anthropic(api_key=api_key)
       elif api_type == 'groq':
           self.client = Groq(api_key=api_key)
   
   def chat(self, user_input, image_file=None, analysis_mode=None, user_language=None):
       try:
           logger.info(f"Processing chat request for {self.api_type}")
           logger.info(f"User input: {user_input}")
           
           # ëŒ€í™” ê¸°ë¡ì— ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
           if image_file:
               # ì˜ˆì‹œë¡œ system ë©”ì‹œì§€ì— ëª¨ë“œì™€ ì–¸ì–´ë¥¼ ë„£ì–´ì¤ë‹ˆë‹¤
               self.conversation_history = [{
                   "role": "system",
                   "content": f"ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë“œ: {analysis_mode}, ì‘ë‹µ ì–¸ì–´: {user_language}"
               }]
               messages = [
                   {"role": "user", "content": user_input}
               ]
           else:
               self.conversation_history.append({"role": "user", "content": user_input})
               messages = self.conversation_history

        #    self.conversation_history.append({"role": "user", "content": user_input})
           
           try:
               if self.api_type == 'openai':
                   response = openai.ChatCompletion.create(
                       model=self.model,
                       messages=self.conversation_history,
                       temperature=0.7,
                       max_tokens=1024
                   )
                   assistant_response = response['choices'][0]['message']['content']
                   # chat ë©”ì†Œë“œì˜ anthropic ë¶€ë¶„ ìˆ˜ì •
               elif self.api_type == 'anthropic':
                    try:
                        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì°¾ê¸°
                        system_message = next((msg['content'] for msg in self.conversation_history 
                                            if msg['role'] == 'system'), '')
                        
                        # ì‚¬ìš©ì ë©”ì‹œì§€ ì°¾ê¸°
                        user_content = next((msg['content'] for msg in self.conversation_history 
                                        if msg['role'] == 'user'), '')

                        message = self.client.messages.create(
                            model=self.model,
                            max_tokens=4096,
                            temperature=0,
                            system=system_message,  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ system íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬
                            messages=[{
                                "role": "user",
                                "content": user_content
                            }]
                        )
                        assistant_response = message.content[0].text
                        logger.info(f"Anthropic response with system message: {system_message[:100]}")
                        
                    except Exception as e:
                        logger.error(f"Anthropic API error: {str(e)}")
                        raise

                # analyze_responses ë©”ì†Œë“œì˜ anthropic ë¶€ë¶„ ìˆ˜ì •
               
               elif self.api_type == 'anthropic':
                   messages = []
                   for msg in self.conversation_history:
                       if msg["role"] != "system":
                           messages.append({
                               "role": msg["role"],
                               "content": msg["content"]
                           })
                   
                   message = self.client.messages.create(
                       model=self.model,
                       max_tokens=4096,
                       temperature=0,
                    #    messages=messages
                   )
                   assistant_response = message.content[0].text
                   
                   
               elif self.api_type == 'groq':
                   response = self.client.chat.completions.create(
                       model=self.model,
                       messages=self.conversation_history,
                       temperature=0.7,
                       max_tokens=1024
                   )
                   assistant_response = response.choices[0].message.content
               
               # ì‘ë‹µ ê¸°ë¡
               self.conversation_history.append({
                   "role": "assistant",
                   "content": assistant_response
               })
               
               logger.info(f"Generated response: {assistant_response[:100]}...")
               return assistant_response
               
           except Exception as e:
               logger.error(f"API error in {self.api_type}: {str(e)}", exc_info=True)
               raise
               
       except Exception as e:
           logger.error(f"Error in chat method: {str(e)}", exc_info=True)
           raise

   def analyze_responses(self, responses, query, user_language, selected_models):


            try:
                logger.info("\n" + "="*100)
                logger.info("ğŸ“Š ë¶„ì„ ì‹œì‘")
                logger.info(f"ğŸ¤– ë¶„ì„ ìˆ˜í–‰ AI: {self.api_type.upper()}")
                logger.info(f"ğŸ” ì„ íƒëœ ëª¨ë¸ë“¤: {', '.join(selected_models)}")
                logger.info("="*100)

                # ì„ íƒëœ ëª¨ë¸ë“¤ë§Œ ë¶„ì„ì— í¬í•¨
                responses_section = ""
                analysis_section = ""
                
                for model in selected_models:
                    model_lower = model.lower()
                    responses_section += f"\n{model.upper()} ì‘ë‹µ: ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì‘ì„± {responses.get(model_lower, 'ì‘ë‹µ ì—†ìŒ')}"
                    
                    analysis_section += f"""
                            "{model_lower}": {{
                                "ì¥ì ": "ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì‘ì„± {model.upper()} ë‹µë³€ì˜ ì¥ì ",
                                "ë‹¨ì ": "ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì‘ì„± {model.upper()} ë‹µë³€ì˜ ë‹¨ì "
                            }}{"," if model_lower != selected_models[-1].lower() else ""}"""

                # The prompt remains the same
                analysis_prompt = f"""ë‹¤ìŒì€ ë™ì¼í•œ ì§ˆë¬¸ì— ëŒ€í•œ {len(selected_models)}ê°€ì§€ AIì˜ ì‘ë‹µì„ ë¶„ì„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
                        ì‚¬ìš©ìê°€ ì„ íƒí•œ ì–¸ì–´ëŠ” '{user_language}'ì…ë‹ˆë‹¤.
                        ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ìµœì ì˜ ë‹µì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
                        ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì¥ì ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
                        ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ë‹¨ì ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
                        ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ë¶„ì„ ê·¼ê±°ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

                        ì§ˆë¬¸: {query}
                        {responses_section}

                        [ìµœì ì˜ ì‘ë‹µì„ ë§Œë“¤ ë•Œ ê³ ë ¤í•  ì‚¬í•­]

                        ëª¨ë“  AIì˜ ë‹µë³€ë“¤ì„ ì¢…í•©í•˜ì—¬ ìµœì ì˜ ë‹µë³€ìœ¼ë¡œ ë°˜ë“œì‹œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.

                        ì¦‰, ê¸°ì¡´ AIì˜ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ ì•ˆë©ë‹ˆë‹¤.

                        ë‹¤ìˆ˜ì˜ AIê°€ ê³µí†µìœ¼ë¡œ ì œê³µí•œ ì •ë³´ëŠ” ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì˜¬ë°”ë¥¸ ì •ë³´ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.

                        íŠ¹ì • AIê°€ ë‹¤ìˆ˜ì˜ AIì™€ ë‹¤ë¥¸ ì •ë³´ë¥¼ ì œê³µí•˜ë©´, ì‹ ë¢°ì„±ì´ ë‚®ì€ ì •ë³´ë¡œ íŒë‹¨í•˜ì—¬ ìµœì ì˜ ë‹µë³€ì—ì„œ ì œì™¸í•˜ê³ , 'ë‹¨ì ' í•­ëª©ì— ë³„ë„ë¡œ ëª…ì‹œí•©ë‹ˆë‹¤.

                        ì—¬ëŸ¬ AIì˜ ë‹µë³€ì—ì„œ ì •í™•í•˜ê³  ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë§Œ ì„ íƒí•˜ì—¬ ë°˜ì˜í•©ë‹ˆë‹¤.

                        ì¤‘ë³µëœ ì •ë³´ê°€ ìˆì„ ê²½ìš° í‘œí˜„ì´ ë” ëª…í™•í•˜ê³  ìƒì„¸í•œ ë‚´ìš©ì„ ìš°ì„  ì„ íƒí•©ë‹ˆë‹¤.

                        ë…¼ë¦¬ì  íë¦„ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ì‘ì„±í•©ë‹ˆë‹¤.

                        ì½”ë“œë¥¼ ë¬»ëŠ” ì§ˆë¬¸ì¼ë•ŒëŠ”, AIì˜ ë‹µë³€ ì¤‘ ì œì¼ ì¢‹ì€ ë‹µë³€ì„ ì„ íƒí•´ì„œ ì¬êµ¬ì„±í•´ì¤˜ 

                        ì½”ë“œëŠ” ë°”ë¡œ ë³µì‚¬í•´ì„œ ì‚¬ìš©ê°€ëŠ¥í•˜ë„ë¡ í•´ì¤˜

                        ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ì„œ ì‹¤í–‰ ë²„íŠ¼ë§Œ ëˆ„ë¥´ë©´ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ ì‘ì„±í•´ì•¼í•©ë‹ˆë‹¤.

                        ì½”ë“œì™€ ì½”ë“œê°€ ì•„ë‹Œ ë¶€ë¶„ì„ êµ¬ë³„ë˜ê²Œ ë³´ì—¬ì¤˜

                        ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

                        ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•œ ê²½ìš° ìœ¡í•˜ì›ì¹™ìœ¼ë¡œ ë‹µë³€í•´ì¤˜

                        [ì¶œë ¥ í˜•ì‹]
                        {{
                            "preferredModel": "{self.api_type.upper()}",
                            "best_response": "ìµœì ì˜ ë‹µë³€ ({user_language}ë¡œ ì‘ì„±)",
                            "analysis": {{
                                {analysis_section}
                            }},
                            "reasoning": "ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì‘ì„± ìµœì ì˜ ì‘ë‹µì„ ì„ íƒí•œ ì´ìœ "
                        }}"""

                # API íƒ€ì…ì— ë”°ë¥¸ ë¶„ê¸° ì²˜ë¦¬
                if self.api_type == 'openai':
                    response = openai.ChatCompletion.create(
                        model=self.model,
                        messages=[
                            {"role": "system", "content": "You are a helpful assistant. Always respond with valid JSON ONLY, no additional text or explanations."},
                            {"role": "user", "content": analysis_prompt}
                        ],
                        temperature=0,
                        max_tokens=4096
                    )
                    analysis_text = response['choices'][0]['message']['content']
                    
                elif self.api_type == 'anthropic':
                    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì—ì„œ ì–¸ì–´ ì„¤ì • ì¶”ì¶œ
                    system_message = next((msg['content'] for msg in self.conversation_history 
                                        if msg['role'] == 'system'), '')
                    
                    message = self.client.messages.create(
                        model=self.model,
                        max_tokens=4096,
                        temperature=0,
                        system=f"{system_message}\nYou must respond with valid JSON only in the specified language. No other text or formatting.",
                        messages=[{
                            "role": "user", 
                            "content": analysis_prompt
                        }]
                    )
                    analysis_text = message.content[0].text.strip()
                
                elif self.api_type == 'groq':
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=[
                            {"role": "system", "content": "You are a helpful assistant. Always respond with valid JSON ONLY, no additional text or explanations."},
                            {"role": "user", "content": analysis_prompt}
                        ],
                        temperature=0,
                        max_tokens=4096
                    )
                    analysis_text = response.choices[0].message.content

                logger.info("âœ… ë¶„ì„ ì™„ë£Œ\n")
                
                # Use our new sanitize_and_parse_json function
                analysis_result = sanitize_and_parse_json(analysis_text, selected_models, responses)
                analysis_result['preferredModel'] = self.api_type.upper()
                
                return analysis_result
            
            except Exception as e:
                logger.error(f"âŒ Analysis error: {str(e)}")
                # Fallback response in case of a major error
                error_analysis = {}
                for model in selected_models:
                    model_lower = model.lower()
                    error_analysis[model_lower] = {"ì¥ì ": "ë¶„ì„ ì‹¤íŒ¨", "ë‹¨ì ": "ë¶„ì„ ì‹¤íŒ¨"}
                
                return {
                    "preferredModel": self.api_type.upper(),
                    "best_response": max(responses.values(), key=len) if responses else "",
                    "analysis": error_analysis,
                    "reasoning": "ì‘ë‹µ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ìµœì ì˜ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                }


# class ChatView(APIView):
#     permission_classes = [AllowAny]

#     def post(self, request, preferredModel):
#         try:
#             logger.info(f"Received chat request for {preferredModel}")
            
#             data = request.data
#             user_message = data.get('message')
#             compare_responses = data.get('compare', True)
            
#             # ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„°: ì„ íƒëœ ëª¨ë¸ë“¤
#             selected_models = data.get('selectedModels', ['gpt', 'claude', 'mixtral'])
            
#             # ì„ íƒëœ ëª¨ë¸ ë¡œê·¸
#             logger.info(f"Selected models: {selected_models}")
            
#             # í† í° ìœ ë¬´ì— ë”°ë¥¸ ì–¸ì–´ ë° ì„ í˜¸ ëª¨ë¸ ì²˜ë¦¬
#             token = request.headers.get('Authorization')
#             if not token:
#                 # ë¹„ë¡œê·¸ì¸: ê¸°ë³¸ ì–¸ì–´ëŠ” ko, ì„ í˜¸ ëª¨ë¸ì€ GPTë¡œ ê³ ì •
#                 user_language = 'ko'
#                 preferredModel = 'gpt'
#             else:
#                 # ë¡œê·¸ì¸: ìš”ì²­ ë°ì´í„°ì˜ ì–¸ì–´ ì‚¬ìš© (í˜¹ì€ ì‚¬ìš©ìì˜ ì„¤ì •ì„ ë”°ë¦„)
#                 user_language = data.get('language', 'ko')
#                 # URLì— ì „ë‹¬ëœ preferredModelì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‚¬ìš©ì ì„¤ì • ë°˜ì˜)

#             logger.info(f"Received language setting: {user_language}")

#             if not user_message:
#                 return Response({'error': 'No message provided'}, 
#                                 status=status.HTTP_400_BAD_REQUEST)

#             # ë¹„ë™ê¸° ì‘ë‹µì„ ìœ„í•œ StreamingHttpResponse ì‚¬ìš©
#             from django.http import StreamingHttpResponse
#             import json
#             import time

#             def stream_responses():
#                 try:
#                     system_message = {
#                         "role": "system",
#                         "content": f"ì‚¬ìš©ìê°€ ì„ íƒí•œ ì–¸ì–´ëŠ” '{user_language}'ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ëª¨ë“  ì‘ë‹µì„ ì´ ì–¸ì–´({user_language})ë¡œ ì œê³µí•´ì£¼ì„¸ìš”."
#                     }
                    
#                     responses = {}
                    
#                     # í˜„ì¬ ìš”ì²­ì— ëŒ€í•œ ê³ ìœ  ì‹ë³„ì ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í™œìš©)
#                     request_id = str(time.time())
                    
#                     # ì„ íƒëœ ëª¨ë¸ë“¤ë§Œ ëŒ€í™”ì— ì°¸ì—¬ì‹œí‚´
#                     selected_chatbots = {model: chatbots.get(model) for model in selected_models if model in chatbots}
                    
#                     # ê° ë´‡ì˜ ì‘ë‹µì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ì¦‰ì‹œ ì‘ë‹µ
#                     for bot_id, bot in selected_chatbots.items():
#                         if bot is None:
#                             logger.warning(f"Selected model {bot_id} not available in chatbots")
#                             yield json.dumps({
#                                 'type': 'bot_error',
#                                 'botId': bot_id,
#                                 'error': f"Model {bot_id} is not available"
#                             }) + '\n'
#                             continue
                            
#                         try:
#                             # ë§¤ë²ˆ ìƒˆë¡œìš´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì´ì „ ë‚´ìš© ì´ˆê¸°í™”)
#                             bot.conversation_history = [system_message]
#                             response = bot.chat(user_message)
#                             responses[bot_id] = response
                            
#                             # ê° ë´‡ ì‘ë‹µì„ ì¦‰ì‹œ ì „ì†¡
#                             yield json.dumps({
#                                 'type': 'bot_response',
#                                 'botId': bot_id,
#                                 'response': response,
#                                 'requestId': request_id  # ìš”ì²­ ID ì¶”ê°€
#                             }) + '\n'
                            
#                         except Exception as e:
#                             logger.error(f"Error from {bot_id}: {str(e)}")
#                             responses[bot_id] = f"Error: {str(e)}"
                            
#                             # ì—ëŸ¬ë„ ì¦‰ì‹œ ì „ì†¡
#                             yield json.dumps({
#                                 'type': 'bot_error',
#                                 'botId': bot_id,
#                                 'error': str(e),
#                                 'requestId': request_id  # ìš”ì²­ ID ì¶”ê°€
#                             }) + '\n'
                    
#                     # ì„ íƒëœ ëª¨ë¸ì´ ìˆê³  ì‘ë‹µì´ ìˆì„ ë•Œë§Œ ë¶„ì„ ìˆ˜í–‰
#                     if selected_models and responses:
#                         # ë¶„ì„(ë¹„êµ)ì€ ë¡œê·¸ì¸ ì‹œ ì‚¬ìš©ìì˜ ì„ í˜¸ ëª¨ë¸ì„, ë¹„ë¡œê·¸ì¸ ì‹œ GPTë¥¼ ì‚¬ìš©
#                         if token:
#                             analyzer_bot = chatbots.get(preferredModel) or chatbots.get('gpt')
#                         else:
#                             analyzer_bot = chatbots.get('gpt')
                        
#                         # ë¶„ì„ìš© ë´‡ë„ ìƒˆë¡œìš´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
#                         analyzer_bot.conversation_history = [system_message]
                        
#                         # ë¶„ì„ ì‹¤í–‰ (í•­ìƒ ìƒˆë¡­ê²Œ ì‹¤í–‰)
#                         analysis = analyzer_bot.analyze_responses(responses, user_message, user_language, selected_models)
                        
#                         # ë¶„ì„ ê²°ê³¼ ì „ì†¡
#                         yield json.dumps({
#                             'type': 'analysis',
#                             'preferredModel': analyzer_bot and analyzer_bot.api_type.upper(),
#                             'best_response': analysis.get('best_response', ''),
#                             'analysis': analysis.get('analysis', {}),
#                             'reasoning': analysis.get('reasoning', ''),
#                             'language': user_language,
#                             'requestId': request_id,  # ìš”ì²­ ID ì¶”ê°€
#                             'timestamp': time.time()  # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
#                         }) + '\n'
#                     else:
#                         logger.warning("No selected models or responses to analyze")
                    
#                 except Exception as e:
#                     logger.error(f"Stream processing error: {str(e)}", exc_info=True)
#                     yield json.dumps({
#                         'type': 'error',
#                         'error': f"Stream processing error: {str(e)}"
#                     }) + '\n'

#             # StreamingHttpResponse ë°˜í™˜
#             response = StreamingHttpResponse(
#                 streaming_content=stream_responses(),
#                 content_type='text/event-stream'
#             )
#             response['Cache-Control'] = 'no-cache'
#             response['X-Accel-Buffering'] = 'no'
#             return response
                
#         except Exception as e:
#             logger.error(f"Unexpected error: {str(e)}", exc_info=True)
#             return Response({
#                 'error': str(e)
#             }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
import logging
import json
from django.http import StreamingHttpResponse
import time
import numpy as np

logger = logging.getLogger(__name__)

# JSON ì§ë ¬í™” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì¶”ê°€
def convert_to_serializable(obj):
    """ëª¨ë“  ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤"""
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, np.integer):  # ì •ìˆ˜í˜• íƒ€ì… ì²˜ë¦¬
        return int(obj)
    elif isinstance(obj, (np.float16, np.float32, np.float64)):  # float_ ì œê±°í•˜ê³  êµ¬ì²´ì ì¸ íƒ€ì…ë§Œ ì‚¬ìš©
        return float(obj)
    elif isinstance(obj, (set, tuple)):
        return list(obj)
    elif hasattr(obj, 'isoformat'):  # datetime ê°ì²´ ì²˜ë¦¬
        return obj.isoformat()
    elif isinstance(obj, dict):
        return {k: convert_to_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_serializable(i) for i in obj]
    else:
        try:
            # str() ì‚¬ìš© ì‹œë„
            return str(obj)
        except:
            return repr(obj)

# class ChatView(APIView):
#     permission_classes = [AllowAny]
    
#     def __init__(self, **kwargs):
#         super().__init__(**kwargs)
#         # SimilarityAnalyzer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
#         self.similarity_analyzer = SimilarityAnalyzer(threshold=0.85)

#     def post(self, request, preferredModel):
#         try:
#             logger.info(f"Received chat request for {preferredModel}")
            
#             data = request.data
#             user_message = data.get('message')
#             compare_responses = data.get('compare', True)
            
#             # ì„ íƒëœ ëª¨ë¸ë“¤
#             selected_models = data.get('selectedModels', ['gpt', 'claude', 'mixtral'])
            
#             # ì„ íƒëœ ëª¨ë¸ ë¡œê·¸
#             logger.info(f"Selected models: {selected_models}")
            
#             # í† í° ìœ ë¬´ì— ë”°ë¥¸ ì–¸ì–´ ë° ì„ í˜¸ ëª¨ë¸ ì²˜ë¦¬
#             token = request.headers.get('Authorization')
#             if not token:
#                 # ë¹„ë¡œê·¸ì¸: ê¸°ë³¸ ì–¸ì–´ëŠ” ko, ì„ í˜¸ ëª¨ë¸ì€ GPTë¡œ ê³ ì •
#                 user_language = 'ko'
#                 preferredModel = 'gpt'
#             else:
#                 # ë¡œê·¸ì¸: ìš”ì²­ ë°ì´í„°ì˜ ì–¸ì–´ ì‚¬ìš© (í˜¹ì€ ì‚¬ìš©ìì˜ ì„¤ì •ì„ ë”°ë¦„)
#                 user_language = data.get('language', 'ko')
#                 # URLì— ì „ë‹¬ëœ preferredModelì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‚¬ìš©ì ì„¤ì • ë°˜ì˜)

#             logger.info(f"Received language setting: {user_language}")

#             if not user_message:
#                 return Response({'error': 'No message provided'}, 
#                                 status=status.HTTP_400_BAD_REQUEST)

#             # ë¹„ë™ê¸° ì‘ë‹µì„ ìœ„í•œ StreamingHttpResponse ì‚¬ìš©
#             def stream_responses():
#                 try:
#                     system_message = {
#                         "role": "system",
#                         "content": f"ì‚¬ìš©ìê°€ ì„ íƒí•œ ì–¸ì–´ëŠ” '{user_language}'ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ëª¨ë“  ì‘ë‹µì„ ì´ ì–¸ì–´({user_language})ë¡œ ì œê³µí•´ì£¼ì„¸ìš”."
#                     }
                    
#                     responses = {}
                    
#                     # í˜„ì¬ ìš”ì²­ì— ëŒ€í•œ ê³ ìœ  ì‹ë³„ì ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í™œìš©)
#                     request_id = str(time.time())
                    
#                     # ì„ íƒëœ ëª¨ë¸ë“¤ë§Œ ëŒ€í™”ì— ì°¸ì—¬ì‹œí‚´
#                     selected_chatbots = {model: chatbots.get(model) for model in selected_models if model in chatbots}
                    
#                     # ê° ë´‡ì˜ ì‘ë‹µì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ì¦‰ì‹œ ì‘ë‹µ
#                     for bot_id, bot in selected_chatbots.items():
#                         if bot is None:
#                             logger.warning(f"Selected model {bot_id} not available in chatbots")
#                             yield json.dumps({
#                                 'type': 'bot_error',
#                                 'botId': bot_id,
#                                 'error': f"Model {bot_id} is not available"
#                             }) + '\n'
#                             continue
                            
#                         try:
#                             # ë§¤ë²ˆ ìƒˆë¡œìš´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì´ì „ ë‚´ìš© ì´ˆê¸°í™”)
#                             bot.conversation_history = [system_message]
#                             response = bot.chat(user_message)
#                             responses[bot_id] = response
                            
#                             # ê° ë´‡ ì‘ë‹µì„ ì¦‰ì‹œ ì „ì†¡
#                             yield json.dumps({
#                                 'type': 'bot_response',
#                                 'botId': bot_id,
#                                 'response': response,
#                                 'requestId': request_id  # ìš”ì²­ ID ì¶”ê°€
#                             }) + '\n'
                            
#                         except Exception as e:
#                             logger.error(f"Error from {bot_id}: {str(e)}")
#                             responses[bot_id] = f"Error: {str(e)}"
                            
#                             # ì—ëŸ¬ë„ ì¦‰ì‹œ ì „ì†¡
#                             yield json.dumps({
#                                 'type': 'bot_error',
#                                 'botId': bot_id,
#                                 'error': str(e),
#                                 'requestId': request_id  # ìš”ì²­ ID ì¶”ê°€
#                             }) + '\n'
                    
#                     # ì‘ë‹µì´ 2ê°œ ì´ìƒì¼ ë•Œë§Œ ìœ ì‚¬ë„ ë¶„ì„ ìˆ˜í–‰
#                     if len(responses) >= 2:
#                         try:
#                             # ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼ ê³„ì‚°
#                             similarity_result = self.similarity_analyzer.cluster_responses(responses)
                            
#                             # ê²°ê³¼ë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
#                             serializable_result = convert_to_serializable(similarity_result)
                            
#                             # ë””ë²„ê¹…ì„ ìœ„í•œ ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼ ë¡œê¹…
#                             logger.info(f"Similarity analysis result: {serializable_result}")
                            
#                             # ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼ ì „ì†¡
#                             yield json.dumps({
#                                 'type': 'similarity_analysis',
#                                 'result': serializable_result,
#                                 'requestId': request_id,
#                                 'timestamp': time.time(),
#                                 'userMessage': user_message  # ì‚¬ìš©ì ë©”ì‹œì§€ í¬í•¨
#                             }) + '\n'
                            
#                         except Exception as e:
#                             logger.error(f"Error in similarity analysis: {str(e)}", exc_info=True)
#                             yield json.dumps({
#                                 'type': 'similarity_error',
#                                 'error': f"Similarity analysis error: {str(e)}",
#                                 'requestId': request_id
#                             }) + '\n'
                    
#                     # ì„ íƒëœ ëª¨ë¸ì´ ìˆê³  ì‘ë‹µì´ ìˆì„ ë•Œë§Œ ë¶„ì„ ìˆ˜í–‰
#                     if selected_models and responses:
#                         # ë¶„ì„(ë¹„êµ)ì€ ë¡œê·¸ì¸ ì‹œ ì‚¬ìš©ìì˜ ì„ í˜¸ ëª¨ë¸ì„, ë¹„ë¡œê·¸ì¸ ì‹œ GPTë¥¼ ì‚¬ìš©
#                         if token:
#                             analyzer_bot = chatbots.get(preferredModel) or chatbots.get('gpt')
#                         else:
#                             analyzer_bot = chatbots.get('gpt')
                        
#                         # ë¶„ì„ìš© ë´‡ë„ ìƒˆë¡œìš´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
#                         analyzer_bot.conversation_history = [system_message]
                        
#                         # ë¶„ì„ ì‹¤í–‰ (í•­ìƒ ìƒˆë¡­ê²Œ ì‹¤í–‰)
#                         analysis = analyzer_bot.analyze_responses(responses, user_message, user_language, selected_models)
                        
#                         # ë¶„ì„ ê²°ê³¼ ì „ì†¡
#                         yield json.dumps({
#                             'type': 'analysis',
#                             'preferredModel': analyzer_bot and analyzer_bot.api_type.upper(),
#                             'best_response': analysis.get('best_response', ''),
#                             'analysis': analysis.get('analysis', {}),
#                             'reasoning': analysis.get('reasoning', ''),
#                             'language': user_language,
#                             'requestId': request_id,  # ìš”ì²­ ID ì¶”ê°€
#                             'timestamp': time.time(),  # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
#                             'userMessage': user_message  # ì‚¬ìš©ì ë©”ì‹œì§€ í¬í•¨
#                         }) + '\n'
#                     else:
#                         logger.warning("No selected models or responses to analyze")
                    
#                 except Exception as e:
#                     logger.error(f"Stream processing error: {str(e)}", exc_info=True)
#                     yield json.dumps({
#                         'type': 'error',
#                         'error': f"Stream processing error: {str(e)}"
#                     }) + '\n'

#             # StreamingHttpResponse ë°˜í™˜
#             response = StreamingHttpResponse(
#                 streaming_content=stream_responses(),
#                 content_type='text/event-stream'
#             )
#             response['Cache-Control'] = 'no-cache'
#             response['X-Accel-Buffering'] = 'no'
#             return response
                
#         except Exception as e:
#             logger.error(f"Unexpected error: {str(e)}", exc_info=True)
#             return Response({
#                 'error': str(e)
#             }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
class ChatView(APIView):
    permission_classes = [AllowAny]

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.similarity_analyzer = SimilarityAnalyzer(threshold=0.85)

    def post(self, request, preferredModel):
        try:
            logger.info(f"Received chat request for {preferredModel}")
            data = request.data
            user_message = data.get('message')
            selected_models = data.get('selectedModels', ['gpt', 'claude', 'mixtral'])
            token = request.headers.get('Authorization')
            user_language = 'ko' if not token else data.get('language', 'ko')
            if not user_message:
                return Response({'error': 'No message provided'}, status=status.HTTP_400_BAD_REQUEST)

            # ë§í¬ë§Œ ìˆì„ ê²½ìš° í˜ì´ì§€ ë‚´ìš©ìœ¼ë¡œ ë¶„ì„ ìš”ì²­
            url_pattern = r'^(https?://\S+)$'
            match = re.match(url_pattern, user_message.strip())
            if match:
                url = match.group(1)
                try:
                    page_text = fetch_and_clean_url(url)
                    if len(page_text) > 10000:
                        page_text = page_text[:5000] + "\n\nâ€¦(ì¤‘ëµ)â€¦\n\n" + page_text[-5000:]
                    user_message = (
                        f"ë‹¤ìŒ ì›¹í˜ì´ì§€ì˜ ë‚´ìš©ì„ ë¶„ì„í•´ ì£¼ì„¸ìš”:\n"
                        f"URL: {url}\n\n"
                        f"{page_text}"
                    )
                except Exception as e:
                    logger.error(f"URL fetch error: {e}")
                    return Response({'error': f"URLì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}"}, status=status.HTTP_400_BAD_REQUEST)

            def stream_responses():
                try:
                    system_message = {
                        'role': 'system',
                        'content': f"ì‚¬ìš©ìê°€ ì„ íƒí•œ ì–¸ì–´ëŠ” '{user_language}'ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì‘ë‹µí•˜ì„¸ìš”."
                    }
                    responses = {}
                    request_id = str(time.time())
                    # ê° ëª¨ë¸ë³„ ì±—ë´‡ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                    selected_chatbots = {m: chatbots.get(m) for m in selected_models if chatbots.get(m)}

                    # ëª¨ë¸ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
                    for bot_id, bot in selected_chatbots.items():
                        try:
                            bot.conversation_history = [system_message]
                            resp_text = bot.chat(user_message)
                            responses[bot_id] = resp_text
                            yield json.dumps({'type':'bot_response','botId':bot_id,'response':resp_text,'requestId':request_id}) + '\n'
                        except Exception as e:
                            yield json.dumps({'type':'bot_error','botId':bot_id,'error':str(e),'requestId':request_id}) + '\n'

                    # ìœ ì‚¬ë„ ë¶„ì„
                    if len(responses) >= 2:
                        sim_res = self.similarity_analyzer.cluster_responses(responses)
                        serial = convert_to_serializable(sim_res)
                        yield json.dumps({'type':'similarity_analysis','result':serial,'requestId':request_id,'timestamp':time.time(),'userMessage':user_message}) + '\n'

                    # ìµœì¢… ë¹„êµ ë° ë¶„ì„
                    analyzer_bot = chatbots.get(preferredModel) or chatbots.get('gpt')
                    analyzer_bot.conversation_history = [system_message]
                    analysis = analyzer_bot.analyze_responses(responses, user_message, user_language, list(responses.keys()))
                    yield json.dumps({
                        'type':'analysis',
                        'preferredModel': analyzer_bot.api_type.upper(),
                        'best_response': analysis.get('best_response',''),
                        'analysis': analysis.get('analysis',{}),
                        'reasoning': analysis.get('reasoning',''),
                        'language': user_language,
                        'requestId': request_id,
                        'timestamp': time.time(),
                        'userMessage': user_message
                    }) + '\n'
                except Exception as e:
                    yield json.dumps({'type':'error','error':f"Stream error: {e}"}) + '\n'

            return StreamingHttpResponse(stream_responses(), content_type='text/event-stream')

        except Exception as e:
            logger.error(f"Unexpected error: {e}", exc_info=True)
            return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# API í‚¤ ì„¤ì •
OPENAI_API_KEY = "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"
ANTHROPIC_API_KEY = "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"
GROQ_API_KEY = "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"

chatbots = {
    'gpt': ChatBot(OPENAI_API_KEY, 'gpt-3.5-turbo', 'openai'),
    'claude': ChatBot(ANTHROPIC_API_KEY, 'claude-3-5-haiku-20241022', 'anthropic'), 
    'mixtral': ChatBot(GROQ_API_KEY, 'llama3-8b-8192', 'groq'),
}
# chatbots = {
#     'gpt': ChatBot(OPENAI_API_KEY, 'gpt-4-turbo', 'openai'),
#     'claude': ChatBot(ANTHROPIC_API_KEY, 'claude-3-opus-20240229', 'anthropic'), 
#     'mixtral': ChatBot(GROQ_API_KEY, 'llama3-8b-8192', 'groq'),
# }


from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import IsAuthenticated
from .models import UserProfile, UserSettings
from .serializers import UserSerializer

class UserSettingsView(APIView):
    permission_classes = [IsAuthenticated]

    def get(self, request):
        try:
            user_settings, created = UserSettings.objects.get_or_create(
                user=request.user,
                defaults={
                    'language': 'ko',
                    'analyzer_bot': 'claude'
                }
            )
            serializer = UserSerializer(user_settings)
            return Response(serializer.data)
        except Exception as e:
            return Response({
                'language': 'ko', 
                'analyzer_bot': 'claude'
            }, status=status.HTTP_200_OK)

    def post(self, request):
        try:
            user_settings, created = UserSettings.objects.get_or_create(
                user=request.user,
                defaults={
                    'language': request.data.get('language', 'ko'),
                    'analyzer_bot': request.data.get('analyzer_bot', 'claude')
                }
            )

            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° ì—…ë°ì´íŠ¸
            if not created:
                user_settings.language = request.data.get('language', user_settings.language)
                user_settings.analyzer_bot = request.data.get('analyzer_bot', user_settings.analyzer_bot)
                user_settings.save()

            serializer = UserSettingsSerializer(user_settings)
            return Response(serializer.data, status=status.HTTP_200_OK)
        except Exception as e:
            return Response({
                'error': str(e)
            }, status=status.HTTP_400_BAD_REQUEST)
from rest_framework import status
from rest_framework.decorators import api_view, permission_classes
from rest_framework.permissions import AllowAny
from rest_framework.response import Response
import requests
import logging
from django.contrib.auth import get_user_model
from .models import SocialAccount

import uuid

logger = logging.getLogger(__name__)
User = get_user_model()

# def generate_unique_username(email, name=None):
#     """ê³ ìœ í•œ username ìƒì„±"""
#     base = name or email.split('@')[0]
#     username = base
#     suffix = 1
    
#     # usernameì´ ê³ ìœ í•  ë•Œê¹Œì§€ ìˆ«ì ì¶”ê°€
#     while User.objects.filter(username=username).exists():
#         username = f"{base}_{suffix}"
#         suffix += 1
    
#     return username

def generate_unique_username(email, name=None):
    """username ìƒì„± - ì´ë©”ì¼ ì•ë¶€ë¶„ ë˜ëŠ” ì´ë¦„ ì‚¬ìš©"""
    if name:
        return name  # ì´ë¦„ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    return email.split('@')[0]  # ì´ë¦„ì´ ì—†ìœ¼ë©´ ì´ë©”ì¼ ì•ë¶€ë¶„ ì‚¬ìš©
from rest_framework.authentication import TokenAuthentication
from rest_framework.permissions import AllowAny
from rest_framework.authtoken.models import Token
from rest_framework import status
from rest_framework.decorators import api_view, authentication_classes, permission_classes
# views.py
from django.db import transaction, IntegrityError

# @api_view(['GET'])
# @authentication_classes([TokenAuthentication])
# @permission_classes([AllowAny])
# @api_view(['GET'])
# @permission_classes([AllowAny])
# def google_callback(request):
#     logger.info("Starting Google callback process")  # ë¡œê¹… ì¶”ê°€
#     try:
#         with transaction.atomic():
#             # 1. ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
#             auth_header = request.headers.get('Authorization', '')
#             access_token = auth_header.split(' ')[1]
            
#             user_info_response = requests.get(
#                 'https://www.googleapis.com/oauth2/v3/userinfo',
#                 headers={'Authorization': f'Bearer {access_token}'}
#             )
            
#             user_info = user_info_response.json()
#             email = user_info.get('email')
#             name = user_info.get('name')

#             logger.info(f"Processing user: {email}")  # ë¡œê¹… ì¶”ê°€

#             # 2. User ê°ì²´ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
#             user = User.objects.filter(email=email).first()
#             if not user:
#                 user = User.objects.create(
#                     username=email,
#                     email=email,
#                     first_name=name or '',
#                     is_active=True
#                 )
#                 logger.info(f"Created new user: {user.id}")
#             else:
#                 logger.info(f"Found existing user: {user.id}")

#             # 3. ê¸°ì¡´ UserSettings ì‚­ì œ (ìˆë‹¤ë©´)
#             UserSettings.objects.filter(user=user).delete()
#             logger.info("Deleted any existing settings")

#             # 4. ìƒˆë¡œìš´ UserSettings ìƒì„±
#             settings = UserSettings.objects.create(
#                 user=user,
#                 language='ko',
#                 preferred_model='default'
#             )
#             logger.info(f"Created new settings for user: {user.id}")

#             # 5. í† í° ìƒì„±
#             token, _ = Token.objects.get_or_create(user=user)
            
#             return Response({
#                 'user': {
#                     'id': user.id,
#                     'email': user.email,
#                     'username': user.username,
#                     'first_name': user.first_name,
#                     'settings': {
#                         'language': settings.language,
#                         'preferred_model': settings.preferred_model
#                     }
#                 },
#                 'access_token': token.key
#             })
#     except Exception as e:
#         logger.error(f"Error in google_callback: {str(e)}")
#         return Response(
#             {'error': str(e)},
#             status=status.HTTP_400_BAD_REQUEST
        # )
@api_view(['GET'])
@authentication_classes([TokenAuthentication])
@permission_classes([AllowAny])
def google_callback(request):
    try:
        # ì•¡ì„¸ìŠ¤ í† í° ì¶”ì¶œ
        auth_header = request.headers.get('Authorization', '')
        if not auth_header.startswith('Bearer '):
            return Response(
                {'error': 'ì˜ëª»ëœ ì¸ì¦ í—¤ë”'}, 
                status=status.HTTP_401_UNAUTHORIZED
            )
        
        access_token = auth_header.split(' ')[1]

        # Google APIë¡œ ì‚¬ìš©ì ì •ë³´ ìš”ì²­
        user_info_response = requests.get(
            'https://www.googleapis.com/oauth2/v3/userinfo',
            headers={'Authorization': f'Bearer {access_token}'}
        )

        if user_info_response.status_code != 200:
            return Response(
                {'error': 'Googleì—ì„œ ì‚¬ìš©ì ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤'}, 
                status=status.HTTP_400_BAD_REQUEST
            )

        user_info = user_info_response.json()
        email = user_info.get('email')
        name = user_info.get('name')
        
        if not email:
            return Response(
                {'error': 'ì´ë©”ì¼ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}, 
                status=status.HTTP_400_BAD_REQUEST
            )

        try:
            # ê¸°ì¡´ ì‚¬ìš©ì ê²€ìƒ‰
            user = User.objects.get(email=email)
        except User.DoesNotExist:
            # ìƒˆë¡œìš´ ì‚¬ìš©ì ìƒì„±
            username = generate_unique_username(email, name)
            user = User.objects.create(
                username=username,
                email=email,
                is_active=True
            )
            
            # ê¸°ë³¸ ë¹„ë°€ë²ˆí˜¸ ì„¤ì • (ì„ íƒì )
            random_password = uuid.uuid4().hex
            user.set_password(random_password)
            user.save()

        # ì†Œì…œ ê³„ì • ì •ë³´ ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸
        social_account, created = SocialAccount.objects.get_or_create(
            email=email,
            provider='google',
            defaults={'user': user}
        )

        if not created and social_account.user != user:
            social_account.user = user
            social_account.save()

        # í† í° ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        token, created = Token.objects.get_or_create(user=user)
        logger.info(f"GOOGLE Token created: {created}")
        logger.info(f"Token key: {token.key}")
        logger.info(f"Token user: {token.user.username}")


        # ì‚¬ìš©ì ë°ì´í„° ë°˜í™˜
        serializer = UserSerializer(user)
        return Response({
            'user': serializer.data,
            'access_token': token.key,  # Django REST Framework Token ë°˜í™˜
            'token_created': created,
            'google_access_token': access_token,  # Google OAuth ì•¡ì„¸ìŠ¤ í† í°

        })

    except Exception as e:
        logger.error(f"Error in google_callback: {str(e)}")
        return Response(
            {'error': str(e)}, 
            status=status.HTTP_500_INTERNAL_SERVER_ERROR
        )
import requests
import json
import requests
from django.conf import settings
from django.http import JsonResponse
from django.shortcuts import redirect
from .models import User  # User ëª¨ë¸ì„ ì„í¬íŠ¸


@api_view(['GET'])
@permission_classes([AllowAny])
def kakao_callback(request):
    try:
        auth_code = request.GET.get('code')
        logger.info(f"Received Kakao auth code: {auth_code}")
        
        # ì¹´ì¹´ì˜¤ í† í° ë°›ê¸°
        token_url = "https://kauth.kakao.com/oauth/token"
        data = {
            "grant_type": "authorization_code",
            "client_id": settings.KAKAO_CLIENT_ID,
            "redirect_uri": settings.KAKAO_REDIRECT_URI,
            "code": auth_code,
        }
        
        token_response = requests.post(token_url, data=data)
        
        if not token_response.ok:
            return Response({
                'error': 'ì¹´ì¹´ì˜¤ í† í° ë°›ê¸° ì‹¤íŒ¨',
                'details': token_response.text
            }, status=status.HTTP_400_BAD_REQUEST)
        
        token_data = token_response.json()
        access_token = token_data.get('access_token')
        
        if not access_token:
            return Response({
                'error': 'ì•¡ì„¸ìŠ¤ í† í° ì—†ìŒ',
                'details': token_data
            }, status=status.HTTP_400_BAD_REQUEST)

        # ì¹´ì¹´ì˜¤ ì‚¬ìš©ì ì •ë³´ ë°›ê¸°
        user_info_url = "https://kapi.kakao.com/v2/user/me"
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-type": "application/x-www-form-urlencoded;charset=utf-8"
        }
        
        user_info_response = requests.get(
            user_info_url,
            headers=headers,
            params={
                'property_keys': json.dumps([
                    "kakao_account.email",
                    "kakao_account.profile",
                    "kakao_account.name"
                ])
            }
        )
        
        if not user_info_response.ok:
            return Response({
                'error': 'ì‚¬ìš©ì ì •ë³´ ë°›ê¸° ì‹¤íŒ¨',
                'details': user_info_response.text
            }, status=status.HTTP_400_BAD_REQUEST)
        
        user_info = user_info_response.json()
        kakao_account = user_info.get('kakao_account', {})
        email = kakao_account.get('email')
        profile = kakao_account.get('profile', {})
        nickname = profile.get('nickname')
        
        logger.info(f"Kakao user info - email: {email}, nickname: {nickname}")
        
        if not email:
            return Response({
                'error': 'ì´ë©”ì¼ ì •ë³´ ì—†ìŒ',
                'details': 'ì¹´ì¹´ì˜¤ ê³„ì •ì˜ ì´ë©”ì¼ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }, status=status.HTTP_400_BAD_REQUEST)

        # ì‚¬ìš©ì ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸
        try:
            user = User.objects.get(email=email)
            logger.info(f"Updated existing user with nickname: {nickname}")
        except User.DoesNotExist:
            unique_username = generate_unique_username(email, nickname)
            user = User.objects.create(
                email=email,
                username=unique_username,
                is_active=True
            )            
            logger.info(f"Created new user with nickname: {nickname}")

        # ì†Œì…œ ê³„ì • ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸
        social_account, created = SocialAccount.objects.update_or_create(
            email=email,
            provider='kakao',
            defaults={
                'user': user,
                'nickname': nickname
            }
        )
        logger.info(f"Social account updated - email: {email}, nickname: {nickname}")

        if not created and social_account.user != user:
            social_account.user = user
            social_account.save()

        # í† í° ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        token, token_created = Token.objects.get_or_create(user=user)
        logger.info(f"KAKAO Token created: {token_created}")
        logger.info(f"Token key: {token.key}")
        logger.info(f"Token user: {token.user.username}")

        serializer = UserSerializer(user)
        return Response({
            'user': serializer.data,
            'access_token': token.key,  # Django REST Framework Token ë°˜í™˜
            'token_created': created,
            'kakao_access_token': access_token,  # Google OAuth ì•¡ì„¸ìŠ¤ í† í°

        })


        
    except Exception as e:
        logger.exception("Unexpected error in kakao_callback")
        return Response({
            'error': str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


from rest_framework.authtoken.models import Token  # Token ëª¨ë¸ ì¶”ê°€

@api_view(['GET'])
@permission_classes([AllowAny])
def naver_callback(request):
    try:
        code = request.GET.get('code')
        state = request.GET.get('state')
        logger.info(f"Received Naver auth code: {code}")

        # ë„¤ì´ë²„ í† í° ë°›ê¸°
        token_url = "https://nid.naver.com/oauth2.0/token"
        token_params = {
            "grant_type": "authorization_code",
            "client_id": settings.NAVER_CLIENT_ID,
            "client_secret": settings.NAVER_CLIENT_SECRET,
            "code": code,
            "state": state
        }

        token_response = requests.get(token_url, params=token_params)

        if not token_response.ok:
            return Response({
                'error': 'ë„¤ì´ë²„ í† í° ë°›ê¸° ì‹¤íŒ¨',
                'details': token_response.text
            }, status=status.HTTP_400_BAD_REQUEST)

        token_data = token_response.json()
        access_token = token_data.get('access_token')

        if not access_token:
            return Response({
                'error': 'ì•¡ì„¸ìŠ¤ í† í° ì—†ìŒ',
                'details': token_data
            }, status=status.HTTP_400_BAD_REQUEST)

        # ë„¤ì´ë²„ ì‚¬ìš©ì ì •ë³´ ë°›ê¸°
        user_info_url = "https://openapi.naver.com/v1/nid/me"
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-type": "application/x-www-form-urlencoded;charset=utf-8"
        }

        user_info_response = requests.get(user_info_url, headers=headers)

        if not user_info_response.ok:
            return Response({
                'error': 'ì‚¬ìš©ì ì •ë³´ ë°›ê¸° ì‹¤íŒ¨',
                'details': user_info_response.text
            }, status=status.HTTP_400_BAD_REQUEST)

        user_info = user_info_response.json()
        response = user_info.get('response', {})
        email = response.get('email')
        nickname = response.get('nickname')
        username = email.split('@')[0]

        if not email:
            return Response({
                'error': 'ì´ë©”ì¼ ì •ë³´ ì—†ìŒ',
                'details': 'ë„¤ì´ë²„ ê³„ì •ì˜ ì´ë©”ì¼ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }, status=status.HTTP_400_BAD_REQUEST)

        # ì‚¬ìš©ì ìƒì„± ë˜ëŠ” ì¡°íšŒ
        user, created = User.objects.get_or_create(
            email=email,
            defaults={'username': generate_unique_username(email, username), 'is_active': True}
        )

        # ì†Œì…œ ê³„ì • ì¡°íšŒ ë° ì—…ë°ì´íŠ¸
        social_account, social_created = SocialAccount.objects.update_or_create(
            provider='naver',
            email=email,
            defaults={'user': user, 'nickname': nickname}
        )

        logger.info(f"Social account updated - email: {email}, nickname: {nickname}")

        # âœ… Django REST Framework Token ìƒì„±
        token, token_created = Token.objects.get_or_create(user=user)
        logger.info(f"Naver Token created: {token_created}")
        logger.info(f"Token key: {token.key}")
        logger.info(f"Token user: {token.user.username}")

        serializer = UserSerializer(user)
        return Response({
            'user': serializer.data,
            'access_token': token.key,  # Django REST Framework Token ë°˜í™˜
            'token_created': created,
            'naver_access_token': access_token,  # ë„¤ì´ë²„ ì•¡ì„¸ìŠ¤ í† í°
        })

    except Exception as e:
        logger.exception("Unexpected error in naver_callback")
        return Response({
            'error': str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# views.py
import logging
from rest_framework.decorators import api_view, permission_classes
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from django.contrib.auth import get_user_model

# logger = logging.getLogger(__name__)

# @api_view(['PUT'])
# @permission_classes([IsAuthenticated])
# def update_user_settings(request):
#     # ì¶”ê°€ ë¡œê¹… ë° ë””ë²„ê¹…
#     logger.info(f"User authentication status: {request.user.is_authenticated}")
#     logger.info(f"User: {request.user}")
#     logger.info(f"Request headers: {request.headers}")
    
#     try:
#         # ì¸ì¦ ìƒíƒœ ëª…ì‹œì  í™•ì¸
#         if not request.user.is_authenticated:
#             logger.error("Unauthenticated user attempt")
#             return Response({
#                 'status': 'error',
#                 'message': 'ì¸ì¦ë˜ì§€ ì•Šì€ ì‚¬ìš©ìì…ë‹ˆë‹¤.'
#             }, status=401)
        
#         # UserProfile ëª¨ë¸ì´ ìˆë‹¤ê³  ê°€ì •
#         user = request.user
#         user_profile = user.userprofile
        
#         # ì„¤ì • ì—…ë°ì´íŠ¸
#         settings_data = request.data
#         user_profile.language = settings_data.get('language', user_profile.language)
#         user_profile.preferred_model = settings_data.get('preferredModel', user_profile.preferred_model)
#         user_profile.save()
        
#         return Response({
#             'status': 'success',
#             'message': 'ì„¤ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.',
#             'settings': {
#                 'language': user_profile.language,
#                 'preferredModel': user_profile.preferred_model
#             }
#         })
    
#     except Exception as e:
#         print("Error:", str(e))  # ì—ëŸ¬ ë¡œê¹…
#         logger.error(f"Settings update error: {str(e)}")
#         return Response({
#             'status': 'error',
#             'message': f'ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
#         }, status=400)
# views.py
# ë°±ì—”ë“œì—ì„œ í† í° í˜•ì‹ í™•ì¸
@api_view(['PUT'])
@authentication_classes([TokenAuthentication])
@permission_classes([IsAuthenticated])
def update_user_settings(request):
    try:
        # í† í° ë¡œê¹… ì¶”ê°€
        token_header = request.headers.get('Authorization')
        if not token_header or not token_header.startswith('Token '):
            return Response({'error': 'ì˜ëª»ëœ í† í° í˜•ì‹'}, status=status.HTTP_401_UNAUTHORIZED)
        
        user = request.user
        if not user.is_authenticated:
            return Response({'error': 'ì¸ì¦ë˜ì§€ ì•Šì€ ì‚¬ìš©ì'}, status=status.HTTP_401_UNAUTHORIZED)
        
        settings_data = request.data
        print(f"Received settings data: {settings_data}")  # ë°ì´í„° ë¡œê¹… ì¶”ê°€
        
        # UserSettings ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒì„±
        settings, created = UserSettings.objects.get_or_create(user=user)
        
        # í•„ë“œ ì—…ë°ì´íŠ¸
        if 'language' in settings_data:
            settings.language = settings_data['language']
        if 'preferredModel' in settings_data:
            settings.preferred_model = settings_data['preferredModel']
        
        settings.save()
        
        return Response({
            'message': 'Settings updated successfully',
            'settings': {
                'language': settings.language,
                'preferredModel': settings.preferred_model
            }
        })
        
    except Exception as e:
        logger.error(f"Settings update error: {str(e)}")
        return Response(
            {'error': str(e)},
            status=status.HTTP_400_BAD_REQUEST
        )
from rest_framework import status
from rest_framework.decorators import api_view, authentication_classes, permission_classes
from rest_framework.authentication import TokenAuthentication
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from django.core.exceptions import ObjectDoesNotExist

@api_view(['PUT'])
@authentication_classes([TokenAuthentication])
@permission_classes([IsAuthenticated])
def update_user_settings(request):
    try:
        user = request.user
        settings_data = request.data
        
        # UserProfile í™•ì¸ ë° ìƒì„±
        try:
            profile = user.userprofile
        except ObjectDoesNotExist:
            profile = UserProfile.objects.create(user=user)
            
        # UserSettings í™•ì¸ ë° ìƒì„±/ì—…ë°ì´íŠ¸
        settings, created = UserSettings.objects.get_or_create(
            user=user,
            defaults={
                'language': settings_data.get('language', 'en'),
                'preferred_model': settings_data.get('preferredModel', 'default')
            }
        )
        
        if not created:
            settings.language = settings_data.get('language', settings.language)
            settings.preferred_model = settings_data.get('preferredModel', settings.preferred_model)
            settings.save()
            
        return Response({
            'message': 'Settings updated successfully',
            'settings': {
                'language': settings.language,
                'preferredModel': settings.preferred_model
            }
        })
            
    except Exception as e:
        print(f"Settings update error: {str(e)}")
        return Response(
            {'error': str(e)},
            status=status.HTTP_400_BAD_REQUEST
        )
# # similarity_analyzer.py
# import logging
# import re
# import math
# import numpy as np
# from collections import Counter
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.metrics.pairwise import cosine_similarity

# logger = logging.getLogger(__name__)

# class SimilarityAnalyzer:
#     """
#     AI ëª¨ë¸ ì‘ë‹µ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ë¶„ì„í•˜ê³  ì‘ë‹µ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤
#     """
    
#     def __init__(self, threshold=0.05):
#         """
#         ì´ˆê¸°í™”
        
#         Args:
#             threshold (float): ìœ ì‚¬ ì‘ë‹µìœ¼ë¡œ ë¶„ë¥˜í•  ì„ê³„ê°’ (0~1)
#         """
#         self.threshold = threshold
#         self.vectorizer = TfidfVectorizer(
#             min_df=1, 
#             analyzer='word',
#             ngram_range=(1, 2),
#             stop_words='english'
#         )
    
#     def preprocess_text(self, text):
#         """
#         í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        
#         Args:
#             text (str): ì „ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
            
#         Returns:
#             str: ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
#         """
#         # ì†Œë¬¸ì ë³€í™˜
#         text = text.lower()
        
#         # ì½”ë“œ ë¸”ë¡ ì œê±° (ë¶„ì„ì—ì„œ ì œì™¸)
#         text = re.sub(r'```.*?```', ' CODE_BLOCK ', text, flags=re.DOTALL)
        
#         # HTML íƒœê·¸ ì œê±°
#         text = re.sub(r'<.*?>', '', text)
        
#         # íŠ¹ìˆ˜ ë¬¸ì ì œê±°
#         text = re.sub(r'[^\w\s]', ' ', text)
        
#         # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¹˜í™˜
#         text = re.sub(r'\s+', ' ', text).strip()
        
#         return text
    
#     def calculate_similarity_matrix(self, responses):
#         """
#         ëª¨ë¸ ì‘ë‹µ ê°„ì˜ ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
        
#         Args:
#             responses (dict): ëª¨ë¸ IDë¥¼ í‚¤ë¡œ, ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
            
#         Returns:ğŸ¤–
#             dict: ëª¨ë¸ ê°„ ìœ ì‚¬ë„ í–‰ë ¬
#         """
#         try:
#             model_ids = list(responses.keys())
            
#             # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
#             preprocessed_texts = [self.preprocess_text(responses[model_id]) for model_id in model_ids]
            
#             # TF-IDF ë²¡í„°í™”
#             tfidf_matrix = self.vectorizer.fit_transform(preprocessed_texts)
            
#             # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
#             similarity_matrix = cosine_similarity(tfidf_matrix)
            
#             # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
#             result = {}
#             for i, model1 in enumerate(model_ids):
#                 result[model1] = {}
#                 for j, model2 in enumerate(model_ids):
#                     result[model1][model2] = float(similarity_matrix[i][j])
            
#             return result
            
#         except Exception as e:
#             logger.error(f"ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
#             # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ í–‰ë ¬ ë°˜í™˜
#             return {model_id: {other_id: 0.0 for other_id in responses} for model_id in responses}
    
#     def cluster_responses(self, responses):
#         """
#         ì‘ë‹µì„ ìœ ì‚¬ë„ì— ë”°ë¼ êµ°ì§‘í™”
        
#         Args:
#             responses (dict): ëª¨ë¸ IDë¥¼ í‚¤ë¡œ, ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
            
#         Returns:
#             dict: êµ°ì§‘í™” ê²°ê³¼
#         """
#         try:
#             model_ids = list(responses.keys())
#             if len(model_ids) <= 1:
#                 return {
#                     "similarGroups": [model_ids],
#                     "outliers": [],
#                     "similarityMatrix": {}
#                 }
            
#             # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
#             similarity_matrix = self.calculate_similarity_matrix(responses)
            
#             # ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
#             clusters = [[model_id] for model_id in model_ids]
            
#             merge_happened = True
#             while merge_happened and len(clusters) > 1:
#                 merge_happened = False
#                 max_similarity = -1
#                 merge_indices = [-1, -1]
                
#                 # ê°€ì¥ ìœ ì‚¬í•œ ë‘ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
#                 for i in range(len(clusters)):
#                     for j in range(i + 1, len(clusters)):
#                         # ë‘ í´ëŸ¬ìŠ¤í„° ê°„ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
#                         cluster_similarity = 0
#                         pair_count = 0
                        
#                         for model1 in clusters[i]:
#                             for model2 in clusters[j]:
#                                 cluster_similarity += similarity_matrix[model1][model2]
#                                 pair_count += 1
                        
#                         avg_similarity = cluster_similarity / max(1, pair_count)
                        
#                         if avg_similarity > max_similarity:
#                             max_similarity = avg_similarity
#                             merge_indices = [i, j]
                
#                 # ì„ê³„ê°’ë³´ë‹¤ ìœ ì‚¬ë„ê°€ ë†’ìœ¼ë©´ í´ëŸ¬ìŠ¤í„° ë³‘í•©
#                 if max_similarity >= self.threshold:
#                     i, j = merge_indices
#                     clusters[i].extend(clusters[j])
#                     clusters.pop(j)
#                     merge_happened = True
            
#             # í´ëŸ¬ìŠ¤í„° í¬ê¸°ì— ë”°ë¼ ì •ë ¬
#             clusters.sort(key=lambda x: -len(x))
            
#             # ì£¼ìš” ê·¸ë£¹ê³¼ ì´ìƒì¹˜ êµ¬ë¶„
#             main_group = clusters[0] if clusters else []
#             outliers = [model for cluster in clusters[1:] for model in cluster]
            
#             # ì‘ë‹µ íŠ¹ì„± ì¶”ì¶œ
#             response_features = {model_id: self.extract_response_features(responses[model_id]) 
#                                 for model_id in model_ids}
            
#             return {
#                 "similarGroups": clusters,
#                 "mainGroup": main_group,
#                 "outliers": outliers,
#                 "similarityMatrix": similarity_matrix,
#                 "responseFeatures": response_features
#             }
            
#         except Exception as e:
#             logger.error(f"ì‘ë‹µ êµ°ì§‘í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
#             # ì˜¤ë¥˜ ë°œìƒ ì‹œ ëª¨ë“  ëª¨ë¸ì„ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ë°˜í™˜
#             return {
#                 "similarGroups": [model_ids],
#                 "mainGroup": model_ids,
#                 "outliers": [],
#                 "similarityMatrix": {},
#                 "responseFeatures": {}
#             }
    
#     def extract_response_features(self, text):
#         """
#         ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì„± ì¶”ì¶œ
        
#         Args:
#             text (str): ì‘ë‹µ í…ìŠ¤íŠ¸
            
#         Returns:
#             dict: ì‘ë‹µ íŠ¹ì„± ì •ë³´
#         """
#         try:
#             # ì‘ë‹µ ê¸¸ì´
#             length = len(text)
            
#             # ì½”ë“œ ë¸”ë¡ ê°œìˆ˜
#             code_blocks = re.findall(r'```[\s\S]*?```', text)
#             code_block_count = len(code_blocks)
            
#             # ë§í¬ ê°œìˆ˜
#             links = re.findall(r'\[.*?\]\(.*?\)', text) or re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)
#             link_count = len(links)
            
#             # ëª©ë¡ í•­ëª© ê°œìˆ˜
#             list_items = re.findall(r'^[\s]*[-*+] |^[\s]*\d+\. ', text, re.MULTILINE)
#             list_item_count = len(list_items)
            
#             # ë¬¸ì¥ ë¶„ë¦¬
#             sentences = re.split(r'[.!?]+', text)
#             sentences = [s.strip() for s in sentences if s.strip()]
            
#             # í‰ê·  ë¬¸ì¥ ê¸¸ì´
#             avg_sentence_length = sum(len(s) for s in sentences) / max(1, len(sentences))
            
#             # ì–´íœ˜ ë‹¤ì–‘ì„± (ê³ ìœ  ë‹¨ì–´ ë¹„ìœ¨)
#             words = re.findall(r'\b\w+\b', text.lower())
#             unique_words = set(words)
#             vocabulary_diversity = len(unique_words) / max(1, len(words))
            
#             return {
#                 "length": length,
#                 "codeBlockCount": code_block_count,
#                 "linkCount": link_count,
#                 "listItemCount": list_item_count,
#                 "sentenceCount": len(sentences),
#                 "avgSentenceLength": avg_sentence_length,
#                 "vocabularyDiversity": vocabulary_diversity,
#                 "hasCode": code_block_count > 0
#             }
            
#         except Exception as e:
#             logger.error(f"ì‘ë‹µ íŠ¹ì„± ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
#             # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
#             return {
#                 "length": len(text),
#                 "codeBlockCount": 0,
#                 "linkCount": 0,
#                 "listItemCount": 0,
#                 "sentenceCount": 1,
#                 "avgSentenceLength": len(text),
#                 "vocabularyDiversity": 0,
#                 "hasCode": False
#             }

import logging
import re
import math
import numpy as np
from collections import Counter
from typing import Dict, List, Union, Any
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
logger = logging.getLogger(__name__)

class SimilarityAnalyzer:
    """
    AI ëª¨ë¸ ì‘ë‹µ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ë¶„ì„í•˜ê³  ì‘ë‹µ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤
    ë‹¤êµ­ì–´ ì§€ì›ì„ ìœ„í•´ paraphrase-multilingual-MiniLM-L12-v2 ëª¨ë¸ ì‚¬ìš©
    """
    
    def __init__(self, threshold=0., use_transformer=True):
        """
        ì´ˆê¸°í™”
        
        Args:
            threshold (float): ìœ ì‚¬ ì‘ë‹µìœ¼ë¡œ ë¶„ë¥˜í•  ì„ê³„ê°’ (0~1)
            use_transformer (bool): SentenceTransformer ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€
        """
        self.threshold = threshold
        self.use_transformer = use_transformer
        
        # ë‹¤êµ­ì–´ SentenceTransformer ëª¨ë¸ ë¡œë“œ
        if use_transformer:
            try:
                self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                logger.info("ë‹¤êµ­ì–´ SentenceTransformer ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"SentenceTransformer ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                self.use_transformer = False
                
        # Fallbackìš© TF-IDF ë²¡í„°ë¼ì´ì €
        from sklearn.feature_extraction.text import TfidfVectorizer
        self.vectorizer = TfidfVectorizer(
            min_df=1, 
            analyzer='word',
            ngram_range=(1, 2),
            stop_words=None  # ë‹¤êµ­ì–´ ì§€ì›ì„ ìœ„í•´ stop_words ì œê±°
        )
    
    def preprocess_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        
        Args:
            text (str): ì „ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
            
        Returns:
            str: ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        # ì†Œë¬¸ì ë³€í™˜ (ì˜ì–´ í…ìŠ¤íŠ¸ë§Œ í•´ë‹¹)
        # ë‹¤êµ­ì–´ ì§€ì›ì„ ìœ„í•´ ì˜ì–´ê°€ ì•„ë‹Œ ê²½ìš° ì›ë˜ ì¼€ì´ìŠ¤ ìœ ì§€
        if text.isascii():
            text = text.lower()
        
        # ì½”ë“œ ë¸”ë¡ ì œê±° (ë¶„ì„ì—ì„œ ì œì™¸)
        text = re.sub(r'```.*?```', ' CODE_BLOCK ', text, flags=re.DOTALL)
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<.*?>', '', text)
        
        # íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬ (ë‹¤êµ­ì–´ ì§€ì›ì„ ìœ„í•´ ì™„ì „ ì œê±°í•˜ì§€ ì•ŠìŒ)
        text = re.sub(r'[^\w\s\u0080-\uFFFF]', ' ', text)
        
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¹˜í™˜
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def calculate_similarity_matrix(self, responses: Dict[str, str]) -> Dict[str, Dict[str, float]]:
        """
        ëª¨ë¸ ì‘ë‹µ ê°„ì˜ ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
        
        Args:
            responses (dict): ëª¨ë¸ IDë¥¼ í‚¤ë¡œ, ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
            
        Returns:
            dict: ëª¨ë¸ ê°„ ìœ ì‚¬ë„ í–‰ë ¬
        """
        try:
            model_ids = list(responses.keys())
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            preprocessed_texts = [self.preprocess_text(responses[model_id]) for model_id in model_ids]
            
            if self.use_transformer and self.model:
                # SentenceTransformerë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„±
                try:
                    embeddings = self.model.encode(preprocessed_texts)
                    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                    similarity_matrix = cosine_similarity(embeddings)
                except Exception as e:
                    logger.error(f"SentenceTransformer ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    # Fallback: TF-IDF ì‚¬ìš©
                    tfidf_matrix = self.vectorizer.fit_transform(preprocessed_texts)
                    similarity_matrix = cosine_similarity(tfidf_matrix)
            else:
                # TF-IDF ë²¡í„°í™”
                tfidf_matrix = self.vectorizer.fit_transform(preprocessed_texts)
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity_matrix = cosine_similarity(tfidf_matrix)
            
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            result = {}
            for i, model1 in enumerate(model_ids):
                result[model1] = {}
                for j, model2 in enumerate(model_ids):
                    result[model1][model2] = float(similarity_matrix[i][j])
            
            return result
            
        except Exception as e:
            logger.error(f"ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ í–‰ë ¬ ë°˜í™˜
            return {model_id: {other_id: 0.0 for other_id in responses} for model_id in responses}
    
      
    def cluster_responses(self, responses):
        """
        ì‘ë‹µì„ ìœ ì‚¬ë„ì— ë”°ë¼ êµ°ì§‘í™”
        
        Args:
            responses (dict): ëª¨ë¸ IDë¥¼ í‚¤ë¡œ, ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
            
        Returns:
            dict: êµ°ì§‘í™” ê²°ê³¼
        """
        try:
            model_ids = list(responses.keys())
            if len(model_ids) <= 1:
                return {
                    "similarGroups": [model_ids],
                    "outliers": [],
                    "similarityMatrix": {}
                }
            
            # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
            similarity_matrix = self.calculate_similarity_matrix(responses)
            
            # ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
            clusters = [[model_id] for model_id in model_ids]
            
            merge_happened = True
            while merge_happened and len(clusters) > 1:
                merge_happened = False
                max_similarity = -1
                merge_indices = [-1, -1]
                
                # ê°€ì¥ ìœ ì‚¬í•œ ë‘ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
                for i in range(len(clusters)):
                    for j in range(i + 1, len(clusters)):
                        # ë‘ í´ëŸ¬ìŠ¤í„° ê°„ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
                        cluster_similarity = 0
                        pair_count = 0
                        
                        for model1 in clusters[i]:
                            for model2 in clusters[j]:
                                cluster_similarity += similarity_matrix[model1][model2]
                                pair_count += 1
                        
                        avg_similarity = cluster_similarity / max(1, pair_count)
                        
                        if avg_similarity > max_similarity:
                            max_similarity = avg_similarity
                            merge_indices = [i, j]
                
                # ì„ê³„ê°’ë³´ë‹¤ ìœ ì‚¬ë„ê°€ ë†’ìœ¼ë©´ í´ëŸ¬ìŠ¤í„° ë³‘í•©
                if max_similarity >= self.threshold:
                    i, j = merge_indices
                    clusters[i].extend(clusters[j])
                    clusters.pop(j)
                    merge_happened = True
            
            # í´ëŸ¬ìŠ¤í„° í¬ê¸°ì— ë”°ë¼ ì •ë ¬
            clusters.sort(key=lambda x: -len(x))
            
            # ì£¼ìš” ê·¸ë£¹ê³¼ ì´ìƒì¹˜ êµ¬ë¶„
            main_group = clusters[0] if clusters else []
            outliers = [model for cluster in clusters[1:] for model in cluster]
            
            # ì‘ë‹µ íŠ¹ì„± ì¶”ì¶œ
            response_features = {model_id: self.extract_response_features(responses[model_id]) 
                                for model_id in model_ids}
            
            return {
                "similarGroups": clusters,
                "mainGroup": main_group,
                "outliers": outliers,
                "similarityMatrix": similarity_matrix,
                "responseFeatures": response_features
            }
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ êµ°ì§‘í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ëª¨ë“  ëª¨ë¸ì„ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ë°˜í™˜
            return {
                "similarGroups": [model_ids],
                "mainGroup": model_ids,
                "outliers": [],
                "similarityMatrix": {},
                "responseFeatures": {}
            }
    
    
    def extract_response_features(self, text: str) -> Dict[str, Union[int, float, bool]]:
        """
        ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì„± ì¶”ì¶œ
        
        Args:
            text (str): ì‘ë‹µ í…ìŠ¤íŠ¸
            
        Returns:
            dict: ì‘ë‹µ íŠ¹ì„± ì •ë³´
        """
        try:
            # ì‘ë‹µ ê¸¸ì´
            length = len(text)
            
            # ì½”ë“œ ë¸”ë¡ ê°œìˆ˜
            code_blocks = re.findall(r'```[\s\S]*?```', text)
            code_block_count = len(code_blocks)
            
            # ë§í¬ ê°œìˆ˜
            links = re.findall(r'\[.*?\]\(.*?\)', text) or re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)
            link_count = len(links)
            
            # ëª©ë¡ í•­ëª© ê°œìˆ˜
            list_items = re.findall(r'^[\s]*[-*+] |^[\s]*\d+\. ', text, re.MULTILINE)
            list_item_count = len(list_items)
            
            # ë¬¸ì¥ ë¶„ë¦¬ (ë‹¤êµ­ì–´ ì§€ì›)
            sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', text)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            # í‰ê·  ë¬¸ì¥ ê¸¸ì´
            avg_sentence_length = sum(len(s) for s in sentences) / max(1, len(sentences))
            
            # ì–´íœ˜ ë‹¤ì–‘ì„± (ê³ ìœ  ë‹¨ì–´ ë¹„ìœ¨)
            words = re.findall(r'\b\w+\b', text.lower())
            unique_words = set(words)
            vocabulary_diversity = len(unique_words) / max(1, len(words))
            
            # ì–¸ì–´ ê°ì§€ (ì¶”ê°€ ê¸°ëŠ¥)
            lang_features = self.detect_language_features(text)
            
            features = {
                "length": length,
                "codeBlockCount": code_block_count,
                "linkCount": link_count,
                "listItemCount": list_item_count,
                "sentenceCount": len(sentences),
                "avgSentenceLength": avg_sentence_length,
                "vocabularyDiversity": vocabulary_diversity,
                "hasCode": code_block_count > 0
            }
            
            # ì–¸ì–´ íŠ¹ì„± ì¶”ê°€
            features.update(lang_features)
            
            return features
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ íŠ¹ì„± ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "length": len(text),
                "codeBlockCount": 0,
                "linkCount": 0,
                "listItemCount": 0,
                "sentenceCount": 1,
                "avgSentenceLength": len(text),
                "vocabularyDiversity": 0,
                "hasCode": False,
                "detectedLang": "unknown"
            }
    
    def detect_language_features(self, text: str) -> Dict[str, Any]:
        """
        í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ íŠ¹ì„± ê°ì§€
        
        Args:
            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            dict: ì–¸ì–´ íŠ¹ì„± ì •ë³´
        """
        try:
            # ì–¸ì–´ íŠ¹ì„± ê°ì§€ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±
            # ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” langdetect ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê¶Œì¥
            
            # í•œêµ­ì–´ íŠ¹ì„± (í•œê¸€ ë¹„ìœ¨)
            korean_chars = len(re.findall(r'[ã„±-ã…ã…-ã…£ê°€-í£]', text))
            
            # ì˜ì–´ íŠ¹ì„± (ì˜ë¬¸ ë¹„ìœ¨)
            english_chars = len(re.findall(r'[a-zA-Z]', text))
            
            # ì¼ë³¸ì–´ íŠ¹ì„± (ì¼ë³¸ì–´ ë¬¸ì ë¹„ìœ¨)
            japanese_chars = len(re.findall(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯]', text))
            
            # ì¤‘êµ­ì–´ íŠ¹ì„± (ì¤‘êµ­ì–´ ë¬¸ì ë¹„ìœ¨)
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
            
            # ê¸°íƒ€ ë¬¸ì (ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì ì œì™¸)
            total_chars = len(re.findall(r'[^\d\s\W]', text))
            
            # ë¹„ìœ¨ ê³„ì‚°
            total = max(1, total_chars)
            korean_ratio = korean_chars / total
            english_ratio = english_chars / total
            japanese_ratio = japanese_chars / total
            chinese_ratio = chinese_chars / total
            
            # ì£¼ìš” ì–¸ì–´ ê²°ì •
            lang_ratios = {
                "ko": korean_ratio,
                "en": english_ratio,
                "ja": japanese_ratio,
                "zh": chinese_ratio,
                "other": 1.0 - (korean_ratio + english_ratio + japanese_ratio + chinese_ratio)
            }
            
            detected_lang = max(lang_ratios.items(), key=lambda x: x[1])[0]
            
            return {
                "detectedLang": detected_lang,
                "langRatios": lang_ratios
            }
            
        except Exception as e:
            logger.error(f"ì–¸ì–´ íŠ¹ì„± ê°ì§€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "detectedLang": "unknown",
                "langRatios": {"unknown": 1.0}
            }
    
    def compare_responses(self, response1: str, response2: str) -> Dict[str, Any]:
        """
        ë‘ ì‘ë‹µ ê°„ì˜ ìœ ì‚¬ë„ì™€ ì°¨ì´ì  ë¶„ì„
        
        Args:
            response1 (str): ì²« ë²ˆì§¸ ì‘ë‹µ
            response2 (str): ë‘ ë²ˆì§¸ ì‘ë‹µ
            
        Returns:
            dict: ìœ ì‚¬ë„ ë° ì°¨ì´ì  ë¶„ì„ ê²°ê³¼
        """
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            text1 = self.preprocess_text(response1)
            text2 = self.preprocess_text(response2)
            
            # ì„ë² ë”© ìƒì„± ë° ìœ ì‚¬ë„ ê³„ì‚°
            if self.use_transformer and self.model:
                embeddings = self.model.encode([text1, text2])
                similarity = float(cosine_similarity([embeddings[0]], [embeddings[1]])[0][0])
            else:
                # TF-IDFë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°
                tfidf_matrix = self.vectorizer.fit_transform([text1, text2])
                similarity = float(cosine_similarity(tfidf_matrix)[0][1])
            
            # ì‘ë‹µ íŠ¹ì„± ë¹„êµ
            features1 = self.extract_response_features(response1)
            features2 = self.extract_response_features(response2)
            
            # íŠ¹ì„± ì°¨ì´ ê³„ì‚°
            feature_diffs = {}
            for key in set(features1.keys()) & set(features2.keys()):
                if isinstance(features1[key], (int, float)) and isinstance(features2[key], (int, float)):
                    feature_diffs[key] = features2[key] - features1[key]
            
            # ì£¼ìš” ì°¨ì´ì  ê³ ìœ  ë‹¨ì–´ ë¶„ì„
            words1 = re.findall(r'\b\w+\b', text1.lower())
            words2 = re.findall(r'\b\w+\b', text2.lower())
            
            counter1 = Counter(words1)
            counter2 = Counter(words2)
            
            unique_to_1 = [word for word, count in counter1.items() if word not in counter2]
            unique_to_2 = [word for word, count in counter2.items() if word not in counter1]
            
            # ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ê³ ìœ  ë‹¨ì–´ (ìµœëŒ€ 10ê°œ)
            top_unique_to_1 = sorted(
                [(word, counter1[word]) for word in unique_to_1], 
                key=lambda x: x[1], 
                reverse=True
            )[:10]
            
            top_unique_to_2 = sorted(
                [(word, counter2[word]) for word in unique_to_2], 
                key=lambda x: x[1], 
                reverse=True
            )[:10]
            
            return {
                "similarity": similarity,
                "isSimilar": similarity >= self.threshold,
                "features1": features1,
                "features2": features2,
                "featureDiffs": feature_diffs,
                "uniqueWordsTo1": top_unique_to_1,
                "uniqueWordsTo2": top_unique_to_2
            }
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ ë¹„êµ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "similarity": 0.0,
                "isSimilar": False,
                "error": str(e)
            }
        
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
import logging
import json
import openai
import anthropic
from groq import Groq
from django.conf import settings
import time


logger = logging.getLogger(__name__)

class TextSimplificationView(APIView):
    """
    í…ìŠ¤íŠ¸ë¥¼ ì‰¬ìš´ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” API ë·°
    íŠ¹ì • ëŒ€ìƒ(ì–´ë¦°ì´, ê³ ë ¹ì, ì™¸êµ­ì¸ í•™ìŠµì ë“±)ì— ë§ì¶° ë³€í™˜
    """
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            logger.info("ì‰¬ìš´ í‘œí˜„ ë³€í™˜ ìš”ì²­ ë°›ìŒ")
            
            data = request.data
            original_text = data.get('message')
            target_audience = data.get('targetAudience', 'general')
            language = data.get('language', 'ko')
            
            if not original_text:
                return Response({'error': 'ë³€í™˜í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.'}, 
                               status=status.HTTP_400_BAD_REQUEST)
            
            # í…ìŠ¤íŠ¸ ë‹¨ìˆœí™” ìˆ˜í–‰
            simplifier = TextSimplifier(
                api_key=settings.OPENAI_API_KEY,
                model="gpt-4-turbo",  # ë˜ëŠ” ì„ í˜¸í•˜ëŠ” GPT ëª¨ë¸
                api_type="openai"
            )
            
            result = simplifier.simplify_text(
                original_text=original_text,
                target_audience=target_audience,
                language=language
            )
            
            return Response(result)
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ë‹¨ìˆœí™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            return Response({
                'error': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class TextSimplifier:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì‰¬ìš´ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤
    ë‹¤ì–‘í•œ AI ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ëŒ€ìƒìë³„ ë§ì¶¤í˜• ë‹¨ìˆœí™” ìˆ˜í–‰
    """
    def __init__(self, api_key, model, api_type):
        self.model = model
        self.api_type = api_type
        self.api_key = api_key
        
        if api_type == 'openai':
            openai.api_key = api_key
        elif api_type == 'anthropic':
            self.client = anthropic.Anthropic(api_key=api_key)
        elif api_type == 'groq':
            self.client = Groq(api_key=api_key)
    
    def simplify_text(self, original_text, target_audience, language='ko'):
        """
        í…ìŠ¤íŠ¸ë¥¼ ë‹¨ìˆœí™”í•˜ì—¬ ë°˜í™˜
        
        Args:
            original_text (str): ì›ë³¸ í…ìŠ¤íŠ¸
            target_audience (str): ëŒ€ìƒì ìœ í˜• (general, child, elderly, foreigner)
            language (str): ì–¸ì–´ (ê¸°ë³¸ê°’: í•œêµ­ì–´)
            
        Returns:
            dict: ë‹¨ìˆœí™” ê²°ê³¼
        """
        try:
            logger.info(f"í…ìŠ¤íŠ¸ ë‹¨ìˆœí™” ì‹œì‘: ëŒ€ìƒ={target_audience}, ì–¸ì–´={language}")
            
            # ëŒ€ìƒìì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._get_simplification_prompt(original_text, target_audience, language)
            
            # AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë‹¨ìˆœí™”
            simplified_text = self._generate_simplified_text(prompt)
            
            # ê²°ê³¼ ë°˜í™˜
            result = {
                'original_text': original_text,
                'simplified_text': simplified_text,
                'target_audience': target_audience,
                'language': language,
                'timestamp': time.time()
            }
            
            logger.info("í…ìŠ¤íŠ¸ ë‹¨ìˆœí™” ì™„ë£Œ")
            return result
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ë‹¨ìˆœí™” ì˜¤ë¥˜: {str(e)}", exc_info=True)
            raise
    
    def _get_simplification_prompt(self, original_text, target_audience, language):
        """ëŒ€ìƒì ë§ì¶¤í˜• ë‹¨ìˆœí™” í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        base_prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë” ì‰¬ìš´ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”:

{original_text}

ëŒ€ìƒì: {target_audience}
ì–¸ì–´: {language}
"""
        
        if target_audience == 'child':
            base_prompt += """
[ì–´ë¦°ì´ìš© ë³€í™˜ ì§€ì¹¨]
1. 7-12ì„¸ ì–´ë¦°ì´ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ë‹¨ì–´ì™€ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.
2. ì§§ê³  ê°„ë‹¨í•œ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì„¸ìš”.
3. ì¶”ìƒì ì¸ ê°œë…ì€ êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ í•¨ê»˜ ì„¤ëª…í•˜ì„¸ìš”.
4. ì¬ë¯¸ìˆê³  í¥ë¯¸ë¡œìš´ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
5. ì–´ë ¤ìš´ ë‹¨ì–´ëŠ” ê°„ë‹¨í•œ ë™ì˜ì–´ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.
6. í•„ìš”í•œ ê²½ìš° ë¹„ìœ ì™€ ì˜ˆì‹œë¥¼ í™œìš©í•˜ì„¸ìš”.
7. ë¬¸ì¥ ì‚¬ì´ì— ì ì ˆí•œ ì¤„ë°”ê¿ˆì„ ì¶”ê°€í•˜ì„¸ìš”.
"""
        elif target_audience == 'elderly':
            base_prompt += """
[ê³ ë ¹ììš© ë³€í™˜ ì§€ì¹¨]
1. ëª…í™•í•˜ê³  ì§ì ‘ì ì¸ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
2. ì™¸ë˜ì–´ë‚˜ ì˜ì–´ í‘œí˜„ì€ ê°€ëŠ¥í•œ í•œêµ­ì–´ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.
3. ë³µì¡í•œ ë¬¸ì¥ êµ¬ì¡°ë¥¼ í”¼í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
4. ì „ë¬¸ ìš©ì–´ëŠ” ì¼ìƒì ì¸ ìš©ì–´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
5. ì¹œìˆ™í•œ ë¹„ìœ ì™€ ì˜ˆì‹œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
6. ì¤‘ìš”í•œ ì •ë³´ëŠ” ë°˜ë³µí•´ì„œ ê°•ì¡°í•˜ì„¸ìš”.
7. ë¬¸ì¥ ì‚¬ì´ì— ì ì ˆí•œ ì¤„ë°”ê¿ˆì„ ì¶”ê°€í•˜ì„¸ìš”.
"""
        elif target_audience == 'foreigner':
            base_prompt += """
[ì™¸êµ­ì¸ í•™ìŠµììš© ë³€í™˜ ì§€ì¹¨]
1. í•œêµ­ì–´ í•™ìŠµì(ì´ˆê¸‰~ì¤‘ê¸‰)ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ê¸°ë³¸ ì–´íœ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
2. ê´€ìš©ì–´, ì†ë‹´, ì€ìœ ì  í‘œí˜„ì„ í”¼í•˜ì„¸ìš”.
3. í•œìì–´ëŠ” ê°€ëŠ¥í•œ ìˆœìš°ë¦¬ë§ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.
4. ë¬¸ë²•ì ìœ¼ë¡œ ë‹¨ìˆœí•œ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
5. ë³µì¡í•œ ì—°ê²°ì–´ë¯¸ë‚˜ ì¡°ì‚¬ ì‚¬ìš©ì„ ìµœì†Œí™”í•˜ì„¸ìš”.
6. ì¤‘ìš”í•œ ê°œë…ì€ ê´„í˜¸ ì•ˆì— ì˜ì–´ë¡œ ë³‘ê¸°í•˜ì„¸ìš”.
7. ë¬¸ì¥ ì‚¬ì´ì— ì ì ˆí•œ ì¤„ë°”ê¿ˆì„ ì¶”ê°€í•˜ì„¸ìš”.
"""
        else:  # general
            base_prompt += """
[ì¼ë°˜ì¸ìš© ë³€í™˜ ì§€ì¹¨]
1. ë³´í¸ì ì¸ êµì–‘ ìˆ˜ì¤€ì˜ ì–´íœ˜ì™€ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
2. ë¶ˆí•„ìš”í•˜ê²Œ ë³µì¡í•œ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ë‹¨ìˆœí™”í•˜ì„¸ìš”.
3. ì „ë¬¸ ìš©ì–´ëŠ” ê°„ë‹¨í•œ ì„¤ëª…ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ì„¸ìš”.
4. ë…¼ë¦¬ì  íë¦„ì„ ìœ ì§€í•˜ë©° ëª…í™•í•˜ê²Œ í‘œí˜„í•˜ì„¸ìš”.
5. ë¹„ìœ ì™€ ì˜ˆì‹œë¥¼ ì ì ˆíˆ í™œìš©í•˜ì„¸ìš”.
6. ì¤‘ìš”í•œ ë‚´ìš©ì„ ê°•ì¡°í•˜ê³  í•µì‹¬ì„ ë¨¼ì € ì œì‹œí•˜ì„¸ìš”.
7. ë¬¸ì¥ ì‚¬ì´ì— ì ì ˆí•œ ì¤„ë°”ê¿ˆì„ ì¶”ê°€í•˜ì„¸ìš”.
"""
            
        return base_prompt
    
    def _generate_simplified_text(self, prompt):
        """AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ìˆœí™”ëœ í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            # API ìœ í˜•ì— ë”°ë¥¸ ë¶„ê¸°
            if self.api_type == 'openai':
                response = openai.ChatCompletion.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë³µì¡í•œ í…ìŠ¤íŠ¸ë¥¼ ë” ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.5,
                    max_tokens=2000
                )
                simplified_text = response['choices'][0]['message']['content']
                
            elif self.api_type == 'anthropic':
                message = self.client.messages.create(
                    model=self.model,
                    max_tokens=2000,
                    temperature=0.5,
                    system="ë‹¹ì‹ ì€ ë³µì¡í•œ í…ìŠ¤íŠ¸ë¥¼ ë” ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
                    messages=[{
                        "role": "user", 
                        "content": prompt
                    }]
                )
                simplified_text = message.content[0].text
                
            elif self.api_type == 'groq':
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë³µì¡í•œ í…ìŠ¤íŠ¸ë¥¼ ë” ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.5,
                    max_tokens=2000
                )
                simplified_text = response.choices[0].message.content
            
            return simplified_text
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}", exc_info=True)
            raise




# ---------


# views.pyì— ì¶”ê°€í•  ì´ë¯¸ì§€ ë¶„ì„ API ì½”ë“œ

# from rest_framework.views import APIView
# from rest_framework.response import Response
# from rest_framework import status
# from rest_framework.permissions import AllowAny
# from django.http import StreamingHttpResponse
# import logging
# import json
# import base64
# import time
# import os
# from PIL import Image
# from io import BytesIO

# logger = logging.getLogger(__name__)

# class ImageAnalysisView(APIView):
#     permission_classes = [AllowAny]
    
#     def post(self, request):
#         try:
#             logger.info("ì´ë¯¸ì§€ ë¶„ì„ ìš”ì²­ ìˆ˜ì‹ ")
            
#             # ìš”ì²­ ë°ì´í„° ì¶”ì¶œ
#             image_file = request.FILES.get('image')
#             prompt = request.data.get('prompt', 'ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.')
#             analysis_mode = request.data.get('analysisMode', 'describe')
#             request_id = request.data.get('requestId')
#             selected_models = json.loads(request.data.get('selectedModels', '["gpt", "claude", "mixtral"]'))
#             user_language = request.data.get('language', 'ko')
            
#             if not image_file:
#                 return Response({'error': 'ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.'}, status=status.HTTP_400_BAD_REQUEST)
            
#             # ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì¸ì½”ë”©
#             image = Image.open(image_file)
            
#             # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ë„ˆë¬´ í° ê²½ìš°)
#             max_size = 1024
#             if max(image.width, image.height) > max_size:
#                 if image.width > image.height:
#                     new_width = max_size
#                     new_height = int(image.height * max_size / image.width)
#                 else:
#                     new_height = max_size
#                     new_width = int(image.width * max_size / image.height)
#                 image = image.resize((new_width, new_height), Image.LANCZOS)
#              # íˆ¬ëª…ë„ ì±„ë„(RGBA)ì´ ìˆìœ¼ë©´ RGBë¡œ ë³€í™˜
#             if image.mode in ("RGBA", "LA") or (image.mode == "P" and "transparency" in image.info):
#                image = image.convert("RGB")
    
#             # API í† í° ë° ëª¨ë¸ ì´ˆê¸°í™”
#             OPENAI_API_KEY = "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"
#             ANTHROPIC_API_KEY = "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"
#             GROQ_API_KEY = "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"
            
#             from .views import ChatBot  # ChatBot í´ë˜ìŠ¤ ì„í¬íŠ¸
            
#             chatbots = {
#                 'gpt': ChatBot(OPENAI_API_KEY, 'gpt-3.5-turbo', 'openai'),
#                 'claude': ChatBot(ANTHROPIC_API_KEY, 'claude-3-5-haiku-20241022', 'anthropic'), 
#                 'mixtral': ChatBot(GROQ_API_KEY, 'llama3-8b-8192', 'groq'),
#             }
            
#             # ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë“œì— ë”°ë¥¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ êµ¬ì„±
#             system_message = {
#                 "role": "system",
#                 "content": f"""ì‚¬ìš©ìê°€ ì„ íƒí•œ ì–¸ì–´ëŠ” '{user_language}'ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
#                 ë‹¹ì‹ ì€ ì´ë¯¸ì§€ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ì´ë¯¸ì§€ë¥¼ ìƒì„¸íˆ ë¶„ì„í•˜ì„¸ìš”.
#                 ë¶„ì„ ëª¨ë“œ: {analysis_mode}
#                 - describe: ì´ë¯¸ì§€ì˜ ëª¨ë“  ì¤‘ìš” ìš”ì†Œë¥¼ ìì„¸íˆ ì„¤ëª…
#                 - ocr: ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° êµ¬ì¡°í™”
#                 - objects: ì´ë¯¸ì§€ì—ì„œ ëª¨ë“  ê°ì²´ë¥¼ ì¸ì‹í•˜ê³  ëª©ë¡í™”
#                 """
#             }
            
#             # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í•¨ìˆ˜
#             def stream_responses():
#                 try:
#                     responses = {}
                    
#                     # ì„ íƒëœ ëª¨ë¸ë“¤ë§Œ ëŒ€í™”ì— ì°¸ì—¬ì‹œí‚´
#                     selected_chatbots = {model: chatbots.get(model) for model in selected_models if model in chatbots}
                    
#                     # ê° ë´‡ì˜ ì‘ë‹µì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ì¦‰ì‹œ ì‘ë‹µ
#                     for bot_id, bot in selected_chatbots.items():
#                         if bot is None:
#                             logger.warning(f"Selected model {bot_id} not available in chatbots")
#                             yield json.dumps({
#                                 'type': 'bot_error',
#                                 'botId': bot_id,
#                                 'error': f"Model {bot_id} is not available"
#                             }) + '\n'
#                             continue
                            
#                         try:
#                             # ë§¤ë²ˆ ìƒˆë¡œìš´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì´ì „ ë‚´ìš© ì´ˆê¸°í™”)
#                             bot.conversation_history = [system_message]
                            
#                             # ì´ë¯¸ì§€ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
#                             response = bot.chat(
#                             prompt=prompt,
#                             image_file=image_file,           # â† JPEGë¡œ ë¦¬ì‚¬ì´ì¦ˆ/convert ëœ PIL íŒŒì¼
#                             analysis_mode=analysis_mode,
#                             user_language=user_language
#                         )
                            
#                             # ì´ë¯¸ì§€ ë¶„ì„ ìˆ˜í–‰
#                             responses[bot_id] = response
                            
#                             # ê° ë´‡ ì‘ë‹µì„ ì¦‰ì‹œ ì „ì†¡
#                             yield json.dumps({
#                                 'type': 'bot_response',
#                                 'botId': bot_id,
#                                 'response': response,
#                                 'requestId': request_id
#                             }) + '\n'
                            
#                         except Exception as e:
#                             logger.error(f"Error from {bot_id}: {str(e)}")
#                             responses[bot_id] = f"Error: {str(e)}"
                            
#                             # ì—ëŸ¬ë„ ì¦‰ì‹œ ì „ì†¡
#                             yield json.dumps({
#                                 'type': 'bot_error',
#                                 'botId': bot_id,
#                                 'error': str(e),
#                                 'requestId': request_id
#                             }) + '\n'
                    
#                     # ìœ ì‚¬ë„ ë¶„ì„ ê¸°ëŠ¥ - responsesì— 2ê°œ ì´ìƒì˜ ì‘ë‹µì´ ìˆì„ ë•Œ
#                     if len(responses) >= 2:
#                         try:
#                             from .views import SimilarityAnalyzer
#                             similarity_analyzer = SimilarityAnalyzer(threshold=0.85)
#                             similarity_result = similarity_analyzer.cluster_responses(responses)
                            
#                             # ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼ ì „ì†¡
#                             yield json.dumps({
#                                 'type': 'similarity_analysis',
#                                 'result': self.convert_to_serializable(similarity_result),
#                                 'requestId': request_id,
#                                 'timestamp': time.time(),
#                                 'userMessage': prompt
#                             }) + '\n'
                            
#                         except Exception as e:
#                             logger.error(f"ìœ ì‚¬ë„ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
#                             yield json.dumps({
#                                 'type': 'similarity_error',
#                                 'error': f"ìœ ì‚¬ë„ ë¶„ì„ ì˜¤ë¥˜: {str(e)}",
#                                 'requestId': request_id
#                             }) + '\n'
                    
#                     # ìµœì¢… ë¶„ì„ ìˆ˜í–‰
#                     if responses:
#                         # ë¶„ì„(ë¹„êµ)ì„ ìœ„í•´ ì„ íƒëœ ëª¨ë¸ ë˜ëŠ” ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
#                         analyzer_bot = chatbots.get(selected_models[0]) or chatbots.get('gpt')
                        
#                         # ë¶„ì„ìš© ë´‡ë„ ìƒˆë¡œìš´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
#                         analyzer_bot.conversation_history = [system_message]
                        
#                         # ë¶„ì„ ì‹¤í–‰
#                         analysis = analyzer_bot.analyze_responses(
#                             responses, 
#                             prompt, 
#                             user_language, 
#                             list(responses.keys())
#                         )
                        
#                         # ë¶„ì„ ê²°ê³¼ ì „ì†¡
#                         yield json.dumps({
#                             'type': 'analysis',
#                             'preferredModel': analyzer_bot.api_type.upper(),
#                             'best_response': analysis.get('best_response', ''),
#                             'analysis': analysis.get('analysis', {}),
#                             'reasoning': analysis.get('reasoning', ''),
#                             'language': user_language,
#                             'requestId': request_id,
#                             'timestamp': time.time(),
#                             'userMessage': prompt
#                         }) + '\n'
#                     else:
#                         logger.warning("No responses to analyze")
                    
#                 except Exception as e:
#                     logger.error(f"Stream processing error: {str(e)}", exc_info=True)
#                     yield json.dumps({
#                         'type': 'error',
#                         'error': f"Stream processing error: {str(e)}"
#                     }) + '\n'
            
#             # StreamingHttpResponse ë°˜í™˜
#             response = StreamingHttpResponse(
#                 streaming_content=stream_responses(),
#                 content_type='text/event-stream'
#             )
#             response['Cache-Control'] = 'no-cache'
#             response['X-Accel-Buffering'] = 'no'
#             return response
                
#         except Exception as e:
#             logger.error(f"ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
#             return Response({
#                 'error': str(e)
#             }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
#     def convert_to_serializable(self, obj):
#         """ëª¨ë“  ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
#         import numpy as np
        
#         if isinstance(obj, np.ndarray):
#             return obj.tolist()
#         elif isinstance(obj, np.integer):
#             return int(obj)
#         elif isinstance(obj, (np.float16, np.float32, np.float64)):
#             return float(obj)
#         elif isinstance(obj, (set, tuple)):
#             return list(obj)
#         elif hasattr(obj, 'isoformat'):
#             return obj.isoformat()
#         elif isinstance(obj, dict):
#             return {k: self.convert_to_serializable(v) for k, v in obj.items()}
#         elif isinstance(obj, list):
#             return [self.convert_to_serializable(i) for i in obj]
#         else:
#             try:
#                 return str(obj)
#             except:
#                 return repr(obj)
            

# import os
# import pytesseract
# from PIL import Image
# import requests
# import json
# import PyPDF2
# import tempfile
# from pdf2image import convert_from_path
# from django.conf import settings
# from rest_framework import status
# from rest_framework.response import Response
# from rest_framework.views import APIView
# from rest_framework.generics import RetrieveAPIView, ListAPIView
# from .models import OCRResult
# from .serializers import OCRResultSerializer

# from django.views.decorators.csrf import csrf_exempt
# from django.utils.decorators import method_decorator

# import logging
# @method_decorator(csrf_exempt, name='dispatch')
# class ProcessFileView(APIView):
#     permission_classes = [AllowAny]
    
#     def post(self, request):
#         try:
#             logger.info("ProcessFileView ìš”ì²­ ìˆ˜ì‹ : %s %s", request.method, request.path)
#             logger.info("ìš”ì²­ í—¤ë”: %s", request.headers)
#             logger.info("ìš”ì²­ ì¿ í‚¤: %s", request.COOKIES)
#             logger.info("ìš”ì²­ META: %s, %s", request.META.get('CSRF_COOKIE', 'ì—†ìŒ'), request.META.get('HTTP_X_CSRFTOKEN', 'ì—†ìŒ'))
            
#             # ê¶Œí•œ ì²´í¬ ë¡œê¹…
#             for permission in self.get_permissions():
#                 has_permission = permission.has_permission(request, self)
#                 logger.info("ê¶Œí•œ ì²´í¬ %s: %s", permission.__class__.__name__, has_permission)
            
#             # 1. íŒŒì¼ í™•ì¸
#             if 'file' not in request.FILES:
#                 logger.error("íŒŒì¼ì´ ì œê³µë˜ì§€ ì•ŠìŒ")
#                 return Response({'error': 'íŒŒì¼ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}, status=status.HTTP_400_BAD_REQUEST)
            
#             file_obj = request.FILES['file']
#             file_name = file_obj.name.lower()
#             logger.info("íŒŒì¼ ì—…ë¡œë“œ: %s, í¬ê¸°: %s bytes", file_name, file_obj.size)
            
#             # 2. íŒŒì¼ ìœ í˜• í™•ì¸
#             if file_name.endswith(('.pdf')):
#                 file_type = 'pdf'
#             elif file_name.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif')):
#                 file_type = 'image'
#             else:
#                 logger.error("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: %s", file_name)
#                 return Response({'error': 'ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ë‚˜ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.'},
#                               status=status.HTTP_400_BAD_REQUEST)
            
#             logger.info("íŒŒì¼ ìœ í˜•: %s", file_type)
            
#             # 3. OCR ê²°ê³¼ ëª¨ë¸ ìƒì„±
#             try:
#                 ocr_result = OCRResult.objects.create(file=file_obj, file_type=file_type)
#                 logger.info("OCRResult ê°ì²´ ìƒì„±: %s", ocr_result.id)
#             except Exception as e:
#                 logger.error("OCRResult ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: %s", str(e), exc_info=True)
#                 return Response({'error': f'OCRResult ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {str(e)}'}, 
#                                status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
#             # 4. OCR ì²˜ë¦¬
#             try:
#                 ocr_text = ""
                
#                 if file_type == 'image':
#                     logger.info("ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì‹œì‘: %s", ocr_result.file.path)
#                     img = Image.open(ocr_result.file.path)
#                     ocr_text = pytesseract.image_to_string(img, lang='kor+eng')
#                     logger.info("ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì™„ë£Œ, ì¶”ì¶œ í…ìŠ¤íŠ¸ ê¸¸ì´: %s", len(ocr_text))
                
#                 elif file_type == 'pdf':
#                     logger.info("PDF ì²˜ë¦¬ ì‹œì‘: %s", ocr_result.file.path)
#                     pdf_text = self.extract_text_from_pdf(ocr_result.file.path)
#                     logger.info("PDF ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ, í…ìŠ¤íŠ¸ ê¸¸ì´: %s", len(pdf_text))
                    
#                     if len(pdf_text.strip()) < 100:
#                         logger.info("PDF OCR ì²˜ë¦¬ ì‹œì‘ (í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŒ)")
#                         pdf_ocr_text = self.ocr_pdf(ocr_result.file.path)
#                         ocr_text = pdf_ocr_text if pdf_ocr_text else pdf_text
#                         logger.info("PDF OCR ì²˜ë¦¬ ì™„ë£Œ, ì¶”ì¶œ í…ìŠ¤íŠ¸ ê¸¸ì´: %s", len(ocr_text))
#                     else:
#                         ocr_text = pdf_text
                
#                 ocr_result.ocr_text = ocr_text
#                 logger.info("OCR í…ìŠ¤íŠ¸ ì €ì¥ (ì¼ë¶€): %s...", ocr_text[:100])
            
#             except Exception as e:
#                 logger.error("OCR ì²˜ë¦¬ ì‹¤íŒ¨: %s", str(e), exc_info=True)
#                 return Response({'error': f'OCR ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}'}, 
#                                status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
#             # 5. LLM ì²˜ë¦¬
#             try:
#                 logger.info("LLM ì²˜ë¦¬ ì‹œì‘")
                
#                 # OpenAI API í‚¤ ì„¤ì • í™•ì¸
#                 api_key = os.environ.get('OPENAI_API_KEY', "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")
#                 logger.info("API í‚¤ ì„¤ì •: %s", api_key[:10] + "..." if api_key else "API í‚¤ ì—†ìŒ")
                
#                 if not api_key:
#                     logger.error("LLM API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
#                     ocr_result.llm_response = "API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ LLM ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
#                 else:
#                     headers = {
#                         'Authorization': f'Bearer {api_key}',
#                         'Content-Type': 'application/json'
#                     }
                    
#                     data = {
#                         'model': 'gpt-3.5-turbo',
#                         'messages': [
#                             {'role': 'system', 'content': 'ë‹¹ì‹ ì€ OCR í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.'},
#                             {'role': 'user', 'content': f'ë‹¤ìŒ {"PDFì—ì„œ" if file_type == "pdf" else "ì´ë¯¸ì§€ì—ì„œ"} ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n{ocr_text}'}
#                         ],
#                         'max_tokens': 500
#                     }
                    
#                     logger.info("OpenAI API ìš”ì²­ ì‹œì‘")
#                     response = requests.post('https://api.openai.com/v1/chat/completions', 
#                                           headers=headers, data=json.dumps(data))
#                     logger.info("OpenAI API ì‘ë‹µ ìƒíƒœ ì½”ë“œ: %s", response.status_code)
                    
#                     if response.status_code == 200:
#                         response_data = response.json()
#                         logger.info("OpenAI API ì‘ë‹µ ë°ì´í„°: %s", response_data)
                        
#                         if 'choices' in response_data and len(response_data['choices']) > 0:
#                             llm_response = response_data['choices'][0]['message']['content']
#                             ocr_result.llm_response = llm_response
#                             logger.info("LLM ì‘ë‹µ (ì¼ë¶€): %s...", llm_response[:100])
#                         else:
#                             ocr_result.llm_response = "LLM ì²˜ë¦¬ê°€ ì˜ˆìƒëœ ì‘ë‹µì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
#                             logger.warning("LLM ì‘ë‹µì— 'choices' í•„ë“œê°€ ì—†ê±°ë‚˜ ë¹„ì–´ ìˆìŒ")
#                     else:
#                         error_msg = f"OpenAI API ì˜¤ë¥˜ (ìƒíƒœ ì½”ë“œ: {response.status_code}): {response.text}"
#                         ocr_result.llm_response = error_msg
#                         logger.error(error_msg)
            
#             except Exception as e:
#                 error_msg = f"LLM ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"
#                 ocr_result.llm_response = error_msg
#                 logger.error(error_msg, exc_info=True)
            
#             # 6. ìµœì¢… ê²°ê³¼ ì €ì¥
#             try:
#                 ocr_result.save()
#                 logger.info("OCRResult ì €ì¥ ì™„ë£Œ (ID: %s)", ocr_result.id)
#             except Exception as e:
#                 logger.error("OCRResult ì €ì¥ ì‹¤íŒ¨: %s", str(e), exc_info=True)
#                 return Response({'error': f'ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}'}, 
#                                status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
#             # 7. ì§ë ¬í™” ë° ì‘ë‹µ
#             try:
#                 serializer = OCRResultSerializer(ocr_result)
#                 logger.info("ì²˜ë¦¬ ì™„ë£Œ, ì‘ë‹µ ë°˜í™˜")
#                 return Response(serializer.data, status=status.HTTP_201_CREATED)
#             except Exception as e:
#                 logger.error("ì‘ë‹µ ì§ë ¬í™” ì‹¤íŒ¨: %s", str(e), exc_info=True)
#                 return Response({'error': f'ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}'}, 
#                                status=status.HTTP_500_INTERNAL_SERVER_ERROR)
                
#         except Exception as e:
#             logger.error("ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: %s", str(e), exc_info=True)
#             return Response({'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'}, 
#                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)

#     def extract_text_from_pdf(self, pdf_path):
#         """PyPDF2ë¥¼ ì‚¬ìš©í•˜ì—¬ PDFì—ì„œ ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
#         text = ""
#         with open(pdf_path, 'rb') as file:
#             reader = PyPDF2.PdfReader(file)
#             for page_num in range(len(reader.pages)):
#                 page = reader.pages[page_num]
#                 text += page.extract_text() + "\n"
#         return text
    
#     def ocr_pdf(self, pdf_path):
#         """PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ OCR ìˆ˜í–‰"""
#         all_text = ""
#         try:
#             # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
#             with tempfile.TemporaryDirectory() as path:
#                 images = convert_from_path(pdf_path, output_folder=path)
                
#                 # ê° í˜ì´ì§€ ì²˜ë¦¬
#                 for i, img in enumerate(images):
#                     text = pytesseract.image_to_string(img, lang='kor+eng')
#                     all_text += f"\n--- í˜ì´ì§€ {i+1} ---\n{text}\n"
                    
#             return all_text
#         except Exception as e:
#             print(f"PDF OCR ì˜¤ë¥˜: {e}")
#             return ""

# class OCRResultDetailView(RetrieveAPIView):
#     queryset = OCRResult.objects.all()
#     serializer_class = OCRResultSerializer

# class OCRResultListView(ListAPIView):
#     queryset = OCRResult.objects.all().order_by('-created_at')
#     serializer_class = OCRResultSerializer

# # chat/ollama_client.py
# import pytesseract
# import requests
# import json
# import logging
# import base64
# from io import BytesIO
# from PIL import Image
# from .models import OCRResult
# from .serializers import OCRResultSerializer  # OCRResultSerializer import ì¶”ê°€
# logger = logging.getLogger(__name__)

# class OllamaClient:
#     def __init__(self, base_url="http://localhost:11434"):
#         self.base_url = base_url
        
#     def analyze_image(self, image_path, prompt):
#         """ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ëŠ” Ollama API í˜¸ì¶œ"""
#         try:
#             # ì´ë¯¸ì§€ íŒŒì¼ ë¡œë“œ ë° base64 ì¸ì½”ë”©
#             with open(image_path, "rb") as image_file:
#                 image_data = image_file.read()
#                 base64_image = base64.b64encode(image_data).decode('utf-8')
            
#             # API ìš”ì²­ ë°ì´í„° ì¤€ë¹„
#             payload = {
#                 "model": "llava:latest",  # ë˜ëŠ” ë‹¤ë¥¸ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸
#                 "prompt": prompt,
#                 "images": [base64_image]
#             }
            
#             # API í˜¸ì¶œ
#             response = requests.post(
#                 f"{self.base_url}/api/generate",
#                 json=payload
#             )
            
#             if response.status_code != 200:
#                 logger.error(f"Ollama API ì˜¤ë¥˜: {response.status_code}, {response.text}")
#                 return f"Ollama API ì˜¤ë¥˜: {response.status_code}"
            
#             # ì‘ë‹µ ì²˜ë¦¬
#             response_data = response.json()
#             return response_data.get("response", "ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
#         except Exception as e:
#             logger.error(f"Ollama ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
#             return f"ì´ë¯¸ì§€ ë¶„ì„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
#     def analyze_text(self, text, prompt=None):
#         """í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ëŠ” Ollama API í˜¸ì¶œ"""
#         try:
#             # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì„¤ì •
#             if not prompt:
#                   prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ê³  í˜ì´ì§€ë³„ë¡œ ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

# {text}

# ë¶„ì„ ì§€ì¹¨:
# 1. ê° í˜ì´ì§€ë‚˜ ë¬¸ë‹¨ì„ êµ¬ë¶„í•´ì„œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
# 2. ë‚´ìš©ì„ ìš”ì•½í•˜ì§€ ë§ê³ , ê° í˜ì´ì§€/ì„¹ì…˜ì˜ í•µì‹¬ ì •ë³´ë¥¼ ì¶©ì‹¤í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
# 3. í˜ì´ì§€ ë²ˆí˜¸ë‚˜ ì„¹ì…˜ì´ ëª…í™•í•˜ê²Œ êµ¬ë¶„ë˜ì–´ ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ í‘œì‹œí•´ì£¼ì„¸ìš”.
# 4. ëª¨ë“  ì¤‘ìš”í•œ ì„¸ë¶€ ì •ë³´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
# 5. ë‚´ìš©ì„ ë‹¨ìˆœ ìš”ì•½í•˜ì§€ ë§ê³ , êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

# ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
# ===== í˜ì´ì§€ 1 (ë˜ëŠ” ì„¹ì…˜ 1) =====
# - ì£¼ìš” ë‚´ìš© ì •ë¦¬
# - ì¤‘ìš” ê°œë… ì„¤ëª…
# - í•µì‹¬ ì •ë³´ ë‚˜ì—´

# ===== í˜ì´ì§€ 2 (ë˜ëŠ” ì„¹ì…˜ 2) =====
# - ì£¼ìš” ë‚´ìš© ì •ë¦¬
# ...

# ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
#             else:
#                 prompt = f"{prompt}\n\n{text}"
            
#             # API ìš”ì²­ ë°ì´í„° ì¤€ë¹„
#             payload = {
#                 "model": "llama3:8b",  # í…ìŠ¤íŠ¸ ë¶„ì„ìš© ëª¨ë¸
#                 "prompt": prompt
#             }
            
#             # API í˜¸ì¶œ
#             response = requests.post(
#                 f"{self.base_url}/api/generate",
#                 json=payload
#             )
            
#             if response.status_code != 200:
#                 logger.error(f"Ollama API ì˜¤ë¥˜: {response.status_code}, {response.text}")
#                 return f"Ollama API ì˜¤ë¥˜: {response.status_code}"
            
#             # ì‘ë‹µ ì²˜ë¦¬
#             response_data = response.json()
#             return response_data.get("response", "ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
#         except Exception as e:
#             logger.error(f"Ollama í…ìŠ¤íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
#             return f"í…ìŠ¤íŠ¸ ë¶„ì„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# import logging
# import json
# import os
# from django.views.decorators.csrf import csrf_exempt
# from django.utils.decorators import method_decorator
# from rest_framework.views import APIView
# from rest_framework.response import Response
# from rest_framework import status
# from rest_framework.permissions import AllowAny
# from PIL import Image, ImageFilter, ImageEnhance
# import pytesseract
# from .models import OCRResult
# from .serializers import OCRResultSerializer

# import PyPDF2
# import tempfile
# from pdf2image import convert_from_path
# import re

# logger = logging.getLogger(__name__)

# # OllamaClient í´ë˜ìŠ¤ ê°€ì ¸ì˜¤ê¸°
# from .ollama_client import OllamaClient
# @method_decorator(csrf_exempt, name='dispatch')
# class ProcessFileView(APIView):
#     permission_classes = [AllowAny]
    
#     def post(self, request):
#         try:
#             logger.info("ProcessFileView ìš”ì²­ ìˆ˜ì‹ : %s %s", request.method, request.path)
            
#             # ìš”ì²­ ë°ì´í„° í™•ì¸
#             if 'file' not in request.FILES:
#                 logger.error("íŒŒì¼ì´ ì œê³µë˜ì§€ ì•ŠìŒ")
#                 return Response({'error': 'íŒŒì¼ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}, status=status.HTTP_400_BAD_REQUEST)
            
#             file_obj = request.FILES['file']
#             file_name = file_obj.name.lower()
#             logger.info("íŒŒì¼ ì—…ë¡œë“œ: %s, í¬ê¸°: %s bytes", file_name, file_obj.size)
            
#             # íŒŒì¼ ìœ í˜• í™•ì¸
#             if file_name.endswith(('.pdf')):
#                 file_type = 'pdf'
                
#                 # PDF í˜ì´ì§€ ë²”ìœ„ í™•ì¸
#                 start_page = int(request.data.get('start_page', 1))
#                 end_page = int(request.data.get('end_page', 0))  # 0ì€ ì „ì²´ í˜ì´ì§€ë¥¼ ì˜ë¯¸
                
#                 logger.info("PDF ì²˜ë¦¬ ë²”ìœ„: %s ~ %s í˜ì´ì§€", start_page, end_page if end_page > 0 else "ë")
                
#             elif file_name.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif')):
#                 file_type = 'image'
#             else:
#                 logger.error("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: %s", file_name)
#                 return Response({'error': 'ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ë‚˜ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.'},
#                               status=status.HTTP_400_BAD_REQUEST)
            
#             # OCR ê²°ê³¼ ê°ì²´ ìƒì„±
#             ocr_result = OCRResult.objects.create(file=file_obj, file_type=file_type)
#             logger.info("OCRResult ê°ì²´ ìƒì„±: %s", ocr_result.id)
            
#             # OCR ì²˜ë¦¬
#             try:
#                 ocr_text = ""
#                 page_texts = []  # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì €ì¥
#                 text_relevant = False  # í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ì—¬ë¶€ (ê¸°ë³¸ê°’: ê´€ë ¨ ì—†ìŒ)
                
#                 if file_type == 'image':
#                     # ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬ - ê°œì„ ëœ OCR ì ìš©
#                     img = Image.open(ocr_result.file.path)
#                     # ì´ë¯¸ì§€ ì •ë³´ ë¡œê¹…
#                     logger.info(f"ì´ë¯¸ì§€ ì •ë³´: í¬ê¸°={img.size}, ëª¨ë“œ={img.mode}, í¬ë§·={img.format}")
                    
#                     # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° OCR ìˆ˜í–‰
#                     preprocessed_img = self.preprocess_image_for_ocr(img)
#                     ocr_text = self.get_optimized_ocr_text(preprocessed_img, lang='kor+eng')
#                     page_texts.append({"page": 1, "text": ocr_text})
#                     logger.info("ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì™„ë£Œ, ì¶”ì¶œ í…ìŠ¤íŠ¸ ê¸¸ì´: %s", len(ocr_text))
#                     logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
                
#                 elif file_type == 'pdf':
#                     # PDF ì²˜ë¦¬ - ì§ì ‘ ì¶”ì¶œ í›„ í•„ìš”ì‹œ OCR
#                     logger.info("PDF ì²˜ë¦¬ ì‹œì‘: %s", ocr_result.file.path)
                    
#                     # ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„ (í˜ì´ì§€ë³„)
#                     direct_extract_success = False
#                     try:
#                         all_page_texts = self.extract_text_from_pdf_by_pages(ocr_result.file.path)
                        
#                         # í˜ì´ì§€ ë²”ìœ„ ì²˜ë¦¬
#                         if start_page > 1 or (end_page > 0 and end_page < len(all_page_texts)):
#                             if start_page <= len(all_page_texts):
#                                 if end_page > 0 and end_page >= start_page:
#                                     page_texts = all_page_texts[start_page-1:end_page]
#                                 else:
#                                     page_texts = all_page_texts[start_page-1:]
#                             else:
#                                 page_texts = []
#                         else:
#                             page_texts = all_page_texts
                        
#                         combined_text = "\n".join([page["text"] for page in page_texts])
                        
#                         # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì¶©ë¶„í•œì§€ í™•ì¸ (ë” ì—„ê²©í•œ ì¡°ê±´)
#                         if combined_text.strip() and len(combined_text.strip()) >= 50:
#                             meaningful_chars = sum(1 for c in combined_text if c.isalnum())
#                             if meaningful_chars > 30:  # ì˜ë¯¸ìˆëŠ” ê¸€ìê°€ 30ì ì´ìƒì´ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
#                                 ocr_text = combined_text
#                                 direct_extract_success = True
#                                 logger.info("PDF ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ, ì´ %s í˜ì´ì§€, í…ìŠ¤íŠ¸ ê¸¸ì´: %s", 
#                                           len(page_texts), len(ocr_text))
#                                 logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
#                     except Exception as e:
#                         logger.error(f"PDF ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    
#                     # ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì‹¤íŒ¨í•œ ê²½ìš°, OCR ì‹œë„
#                     if not direct_extract_success:
#                         logger.info("PDF OCR ì²˜ë¦¬ ì‹œì‘ (ì§ì ‘ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶ˆì¶©ë¶„)")
                        
#                         # í˜ì´ì§€ ë²”ìœ„ ì„¤ì •ìœ¼ë¡œ OCR
#                         all_page_texts = self.ocr_pdf_by_pages(ocr_result.file.path, start_page, end_page)
                        
#                         # í˜ì´ì§€ ë²”ìœ„ ì²˜ë¦¬ - ocr_pdf_by_pagesì—ì„œ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ì „ì²´ ì‚¬ìš©
#                         page_texts = all_page_texts
                        
#                         ocr_text = "\n".join([page["text"] for page in page_texts])
#                         logger.info("PDF OCR ì²˜ë¦¬ ì™„ë£Œ, ì´ %s í˜ì´ì§€, í…ìŠ¤íŠ¸ ê¸¸ì´: %s", 
#                                   len(page_texts), len(ocr_text))
#                         logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
                
#                 # í…ìŠ¤íŠ¸ ì •í™” - ê°œì„ ëœ í•¨ìˆ˜ ì‚¬ìš©
#                 ocr_result.ocr_text = self.clean_text(ocr_text)
                
#                 # PDF íŒŒì¼ì€ í•­ìƒ í…ìŠ¤íŠ¸ ê´€ë ¨ ìˆìŒìœ¼ë¡œ ì„¤ì •
#                 if file_type == 'pdf':
#                     text_relevant = True
                
#                 # ë¶„ì„ ìœ í˜• í™•ì¸ (ê¸°ë³¸ê°’: both)
#                 analysis_type = request.data.get('analysis_type', 'both')
#                 logger.info("ë¶„ì„ ìœ í˜•: %s", analysis_type)
                
#                 # ê²°ê³¼ ë³€ìˆ˜ ì´ˆê¸°í™”
#                 image_analysis = ""
#                 text_analysis = ""
#                 combined_analysis = ""
                
#                 # í˜ì´ì§€ ë¶„í•  ë¶„ì„ ì—¬ë¶€ í™•ì¸
#                 analyze_by_page = request.data.get('analyze_by_page', 'true').lower() == 'true'
                
#                 # ì„ íƒëœ ë¶„ì„ ìœ í˜•ì— ë”°ë¼ ì²˜ë¦¬
#                 if analysis_type in ['ollama', 'both']:
#                     # Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
#                     ollama_base_url = os.environ.get('OLLAMA_API_URL', 'http://localhost:11434')
#                     ollama_client = OllamaClient(base_url=ollama_base_url)
                    
#                     # ì´ë¯¸ì§€ íŒŒì¼ì¸ ê²½ìš° í…ìŠ¤íŠ¸ ê´€ë ¨ì„± í™•ì¸
#                     if file_type == 'image' and ocr_result.ocr_text.strip():
#                         # í…ìŠ¤íŠ¸ ê´€ë ¨ì„± í™•ì¸ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰
#                         try:
#                             relevance_result = ollama_client.check_text_relevance(ocr_result.file.path, ocr_result.ocr_text)
#                             text_relevant = relevance_result
#                             logger.info(f"í…ìŠ¤íŠ¸ ê´€ë ¨ì„± í™•ì¸ ê²°ê³¼: {text_relevant}")
                            
#                             # ê´€ë ¨ì„± ì •ë³´ ì €ì¥
#                             ocr_result.text_relevant = text_relevant
#                         except Exception as e:
#                             logger.error(f"í…ìŠ¤íŠ¸ ê´€ë ¨ì„± í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
#                             # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’(false) ìœ ì§€
                    
#                     # ì´ë¯¸ì§€ ë¶„ì„ (ì´ë¯¸ì§€ íŒŒì¼ì¸ ê²½ìš°)
#                     if file_type == 'image':
#                         # ê´€ë ¨ì„±ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ë¶„ê¸° - ê°œì„ ëœ í•¨ìˆ˜ í˜¸ì¶œ
#                         custom_prompt = None  # ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ì—¬ê¸°ì„œ ì²˜ë¦¬
                        
#                         image_analysis = ollama_client.analyze_image(
#                             ocr_result.file.path, 
#                             custom_prompt, 
#                             ocr_text=ocr_result.ocr_text, 
#                             text_relevant=text_relevant
#                         )
                        
#                         # OCR í…ìŠ¤íŠ¸ ë¶„ì„ (í…ìŠ¤íŠ¸ê°€ ìˆê³  both ëª¨ë“œì¸ ê²½ìš° & ê´€ë ¨ ìˆëŠ” ê²½ìš°ë§Œ)
#                         if ocr_result.ocr_text and analysis_type == 'both' and text_relevant:
#                             # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì •ë¦¬ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸
#                             text_prompt = f"""ë‹¤ìŒ OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ê³  ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

# {ocr_result.ocr_text}

# ë¶„ì„ ì§€ì¹¨:
# 1. í…ìŠ¤íŠ¸ì˜ ì£¼ìš” ë‚´ìš©ê³¼ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì—¬ ì •ë¦¬
# 2. ë‹¨ìˆœ ìš”ì•½ì´ ì•„ë‹Œ, í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ì •ë³´ë¥¼ ì¶©ì‹¤í•˜ê²Œ ì •ë¦¬
# 3. ì¤‘ìš”í•œ ì„¸ë¶€ ì •ë³´ë¥¼ í¬í•¨
# 4. ë‚´ìš©ì´ ì´ë¯¸ì§€ì™€ ê´€ë ¨ì´ ìˆìœ¼ë¯€ë¡œ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ì •ë¦¬

# ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
                            
#                             try:
#                                 text_analysis = ollama_client.analyze_text(ocr_result.ocr_text, text_prompt)
#                             except Exception as e:
#                                 logger.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
#                                 text_analysis = f"í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                            
#                             # ë‘ ë¶„ì„ ê²°ê³¼ ê²°í•©
#                             combined_analysis = f"ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼:\n{image_analysis}\n\ní…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼:\n{text_analysis}"
#                         else:
#                             # OCR ì—†ì´ ì´ë¯¸ì§€ ë¶„ì„ë§Œ ìˆ˜í–‰
#                             combined_analysis = image_analysis
                        
#                     else:  # PDF íŒŒì¼ì¸ ê²½ìš°
#                         if ocr_result.ocr_text:
#                             if analyze_by_page and len(page_texts) > 1:
#                                 # ê°œì„ ëœ í˜ì´ì§€ë³„ ë¶„ì„ ìˆ˜í–‰ - OllamaClientì˜ ë¶„ì„ ê¸°ëŠ¥ í™œìš©
#                                 try:
#                                     combined_analysis = ollama_client.analyze_text(ocr_result.ocr_text, None, page_texts)
#                                     logger.info("í˜ì´ì§€ë³„ ë¶„ì„ ì™„ë£Œ")
#                                 except Exception as e:
#                                     logger.error(f"í˜ì´ì§€ë³„ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
#                                     combined_analysis = f"í˜ì´ì§€ë³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
#                             else:
#                                 # ë¬¸ì„œ ì „ì²´ ë¶„ì„ - í˜ì´ì§€ë³„ êµ¬ì¡°í™” ìš”ì²­
#                                 text_prompt = f"""ë‹¤ìŒ PDFì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë³„ë¡œ ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

# {ocr_result.ocr_text}

# ë¶„ì„ ì§€ì¹¨:
# 1. í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë‚˜ ì„¹ì…˜ ë‹¨ìœ„ë¡œ êµ¬ë¶„í•˜ì—¬ ì •ë¦¬í•´ì£¼ì„¸ìš”.
# 2. ë‚´ìš©ì„ ìš”ì•½í•˜ì§€ ë§ê³ , ê° ì„¹ì…˜ì˜ í•µì‹¬ ì •ë³´ë¥¼ ì¶©ì‹¤í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
# 3. ëª¨ë“  ì¤‘ìš”í•œ ì„¸ë¶€ ì •ë³´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
# 4. ë‚´ìš©ì„ ë‹¨ìˆœ ìš”ì•½í•˜ì§€ ë§ê³ , êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

# ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
# ===== í˜ì´ì§€ 1 (ë˜ëŠ” ì„¹ì…˜ 1) =====
# - ì£¼ìš” ë‚´ìš© ì •ë¦¬
# - ì¤‘ìš” ê°œë… ì„¤ëª…
# - í•µì‹¬ ì •ë³´ ë‚˜ì—´

# ===== í˜ì´ì§€ 2 (ë˜ëŠ” ì„¹ì…˜ 2) =====
# - ì£¼ìš” ë‚´ìš© ì •ë¦¬
# ...

# ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
                                
#                                 try:
#                                     text_analysis = ollama_client.analyze_text(ocr_result.ocr_text, text_prompt)
#                                     combined_analysis = text_analysis
#                                 except Exception as e:
#                                     logger.error(f"ë¬¸ì„œ ì „ì²´ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
#                                     combined_analysis = f"ë¬¸ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\nOCR ê²°ê³¼: {ocr_result.ocr_text[:500]}..."
                    
#                     logger.info("ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë¶„ì„ ì™„ë£Œ")
                
#                 # MySQL ì €ì¥ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì •í™”
#                 ocr_result.llm_response = self.clean_text(combined_analysis)
                
#             except Exception as e:
#                 logger.error("ì²˜ë¦¬ ì‹¤íŒ¨: %s", str(e), exc_info=True)
#                 return Response({'error': f'ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}'}, 
#                                status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
#             # í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ì •ë³´ ì €ì¥
#             ocr_result.text_relevant = text_relevant
            
#             # ê²°ê³¼ ì €ì¥
#             ocr_result.save()
#             logger.info("OCRResult ì €ì¥ ì™„ë£Œ (ID: %s)", ocr_result.id)
            
#             # ì‘ë‹µ ë°˜í™˜
#             serializer = OCRResultSerializer(ocr_result)
#             return Response(serializer.data, status=status.HTTP_201_CREATED)
                
#         except Exception as e:
#             logger.error("ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: %s", str(e), exc_info=True)
#             return Response({'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'}, 
#                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)
                           


# import logging
# import json
# import os
# from django.views.decorators.csrf import csrf_exempt
# from django.utils.decorators import method_decorator
# from rest_framework.views import APIView
# from rest_framework.response import Response
# from rest_framework import status
# from rest_framework.permissions import AllowAny
# from PIL import Image, ImageFilter, ImageEnhance
# import pytesseract
# from .models import OCRResult
# from .serializers import OCRResultSerializer

# import PyPDF2
# import tempfile
# from pdf2image import convert_from_path
# import re

# logger = logging.getLogger(__name__)

# # OllamaClient í´ë˜ìŠ¤ ê°€ì ¸ì˜¤ê¸°
# from .ollama_client import OllamaClient

# @method_decorator(csrf_exempt, name='dispatch')
# class ProcessFileView(APIView):
#     permission_classes = [AllowAny]
    
#     def post(self, request):
#         try:
#             logger.info("ProcessFileView ìš”ì²­ ìˆ˜ì‹ : %s %s", request.method, request.path)
            
#             # ìš”ì²­ ë°ì´í„° í™•ì¸
#             if 'file' not in request.FILES:
#                 logger.error("íŒŒì¼ì´ ì œê³µë˜ì§€ ì•ŠìŒ")
#                 return Response({'error': 'íŒŒì¼ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}, status=status.HTTP_400_BAD_REQUEST)
            
#             file_obj = request.FILES['file']
#             file_name = file_obj.name.lower()
#             logger.info("íŒŒì¼ ì—…ë¡œë“œ: %s, í¬ê¸°: %s bytes", file_name, file_obj.size)
            
#             # íŒŒì¼ ìœ í˜• í™•ì¸
#             if file_name.endswith(('.pdf')):
#                 file_type = 'pdf'
                
#                 # PDF í˜ì´ì§€ ë²”ìœ„ í™•ì¸
#                 start_page = int(request.data.get('start_page', 1))
#                 end_page = int(request.data.get('end_page', 0))  # 0ì€ ì „ì²´ í˜ì´ì§€ë¥¼ ì˜ë¯¸
                
#                 logger.info("PDF ì²˜ë¦¬ ë²”ìœ„: %s ~ %s í˜ì´ì§€", start_page, end_page if end_page > 0 else "ë")
                
#             elif file_name.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif')):
#                 file_type = 'image'
#             else:
#                 logger.error("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: %s", file_name)
#                 return Response({'error': 'ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ë‚˜ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.'},
#                               status=status.HTTP_400_BAD_REQUEST)
            
#             # OCR ê²°ê³¼ ê°ì²´ ìƒì„±
#             ocr_result = OCRResult.objects.create(file=file_obj, file_type=file_type)
#             logger.info("OCRResult ê°ì²´ ìƒì„±: %s", ocr_result.id)
            
#             # Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” - ì—¬ê¸°ì„œ ê°ì²´ ìƒì„±
#             ollama_base_url = os.environ.get('OLLAMA_API_URL', 'http://localhost:11434')
#             ollama_client = OllamaClient(base_url=ollama_base_url)
            
#             # OCR ì²˜ë¦¬
#             try:
#                 ocr_text = ""
#                 page_texts = []  # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì €ì¥
#                 text_relevant = False  # í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ì—¬ë¶€ (ê¸°ë³¸ê°’: ê´€ë ¨ ì—†ìŒ)
                
#                 if file_type == 'image':
#                     # ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬ - ê°œì„ ëœ OCR ì ìš©
#                     img = Image.open(ocr_result.file.path)
#                     # ì´ë¯¸ì§€ ì •ë³´ ë¡œê¹…
#                     logger.info(f"ì´ë¯¸ì§€ ì •ë³´: í¬ê¸°={img.size}, ëª¨ë“œ={img.mode}, í¬ë§·={img.format}")
                    
#                     # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° OCR ìˆ˜í–‰ - OllamaClient ë©”ì„œë“œ ì‚¬ìš©
#                     preprocessed_img = ollama_client.preprocess_image_for_ocr(img)
#                     ocr_text = ollama_client.get_optimized_ocr_text(preprocessed_img, lang='kor+eng')
#                     page_texts.append({"page": 1, "text": ocr_text})
#                     logger.info("ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì™„ë£Œ, ì¶”ì¶œ í…ìŠ¤íŠ¸ ê¸¸ì´: %s", len(ocr_text))
#                     logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
                
#                 elif file_type == 'pdf':
#                     # PDF ì²˜ë¦¬ - ì§ì ‘ ì¶”ì¶œ í›„ í•„ìš”ì‹œ OCR
#                     logger.info("PDF ì²˜ë¦¬ ì‹œì‘: %s", ocr_result.file.path)
                    
#                     # ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„ (í˜ì´ì§€ë³„)
#                     direct_extract_success = False
#                     try:
#                         all_page_texts = self.extract_text_from_pdf_by_pages(ocr_result.file.path)
                        
#                         # í˜ì´ì§€ ë²”ìœ„ ì²˜ë¦¬
#                         if start_page > 1 or (end_page > 0 and end_page < len(all_page_texts)):
#                             if start_page <= len(all_page_texts):
#                                 if end_page > 0 and end_page >= start_page:
#                                     page_texts = all_page_texts[start_page-1:end_page]
#                                 else:
#                                     page_texts = all_page_texts[start_page-1:]
#                             else:
#                                 page_texts = []
#                         else:
#                             page_texts = all_page_texts
                        
#                         combined_text = "\n".join([page["text"] for page in page_texts])
                        
#                         # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì¶©ë¶„í•œì§€ í™•ì¸ (ë” ì—„ê²©í•œ ì¡°ê±´)
#                         if combined_text.strip() and len(combined_text.strip()) >= 50:
#                             meaningful_chars = sum(1 for c in combined_text if c.isalnum())
#                             if meaningful_chars > 30:  # ì˜ë¯¸ìˆëŠ” ê¸€ìê°€ 30ì ì´ìƒì´ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
#                                 ocr_text = combined_text
#                                 direct_extract_success = True
#                                 logger.info("PDF ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ, ì´ %s í˜ì´ì§€, í…ìŠ¤íŠ¸ ê¸¸ì´: %s", 
#                                           len(page_texts), len(ocr_text))
#                                 logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
#                     except Exception as e:
#                         logger.error(f"PDF ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    
#                     # ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì‹¤íŒ¨í•œ ê²½ìš°, OCR ì‹œë„
#                     if not direct_extract_success:
#                         logger.info("PDF OCR ì²˜ë¦¬ ì‹œì‘ (ì§ì ‘ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶ˆì¶©ë¶„)")
                        
#                         # í˜ì´ì§€ ë²”ìœ„ ì„¤ì •ìœ¼ë¡œ OCR - ì´ ë©”ì„œë“œë„ ì •ì˜í•´ì•¼ í•¨
#                         all_page_texts = self.ocr_pdf_by_pages(ocr_result.file.path, start_page, end_page)
                        
#                         # í˜ì´ì§€ ë²”ìœ„ ì²˜ë¦¬ - ocr_pdf_by_pagesì—ì„œ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ì „ì²´ ì‚¬ìš©
#                         page_texts = all_page_texts
                        
#                         ocr_text = "\n".join([page["text"] for page in page_texts])
#                         logger.info("PDF OCR ì²˜ë¦¬ ì™„ë£Œ, ì´ %s í˜ì´ì§€, í…ìŠ¤íŠ¸ ê¸¸ì´: %s", 
#                                   len(page_texts), len(ocr_text))
#                         logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
                
#                 # í…ìŠ¤íŠ¸ ì •í™” - ê°œì„ ëœ í•¨ìˆ˜ ì‚¬ìš©
#                 ocr_result.ocr_text = self.clean_text(ocr_text)
                
#                 # PDF íŒŒì¼ì€ í•­ìƒ í…ìŠ¤íŠ¸ ê´€ë ¨ ìˆìŒìœ¼ë¡œ ì„¤ì •
#                 if file_type == 'pdf':
#                     text_relevant = True
                
#                 # ë¶„ì„ ìœ í˜• í™•ì¸ (ê¸°ë³¸ê°’: both)
#                 analysis_type = request.data.get('analysis_type', 'both')
#                 logger.info("ë¶„ì„ ìœ í˜•: %s", analysis_type)
                
#                 # ê²°ê³¼ ë³€ìˆ˜ ì´ˆê¸°í™”
#                 image_analysis = ""
#                 text_analysis = ""
#                 combined_analysis = ""
                
#                 # í˜ì´ì§€ ë¶„í•  ë¶„ì„ ì—¬ë¶€ í™•ì¸
#                 analyze_by_page = request.data.get('analyze_by_page', 'true').lower() == 'true'
                
#                 # ì„ íƒëœ ë¶„ì„ ìœ í˜•ì— ë”°ë¼ ì²˜ë¦¬
#                 if analysis_type in ['ollama', 'both']:
#                     # ì´ë¯¸ì§€ íŒŒì¼ì¸ ê²½ìš° í…ìŠ¤íŠ¸ ê´€ë ¨ì„± í™•ì¸
#                     if file_type == 'image' and ocr_result.ocr_text.strip():
#                         # í…ìŠ¤íŠ¸ ê´€ë ¨ì„± í™•ì¸ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰
#                         try:
#                             relevance_result = ollama_client.check_text_relevance(ocr_result.file.path, ocr_result.ocr_text)
#                             text_relevant = relevance_result
#                             logger.info(f"í…ìŠ¤íŠ¸ ê´€ë ¨ì„± í™•ì¸ ê²°ê³¼: {text_relevant}")
                            
#                             # ê´€ë ¨ì„± ì •ë³´ ì €ì¥
#                             ocr_result.text_relevant = text_relevant
#                         except Exception as e:
#                             logger.error(f"í…ìŠ¤íŠ¸ ê´€ë ¨ì„± í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
#                             # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’(false) ìœ ì§€
                    
#                     # ì´ë¯¸ì§€ ë¶„ì„ (ì´ë¯¸ì§€ íŒŒì¼ì¸ ê²½ìš°)
#                     if file_type == 'image':
#                         # ê´€ë ¨ì„±ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ë¶„ê¸° - ê°œì„ ëœ í•¨ìˆ˜ í˜¸ì¶œ
#                         custom_prompt = None  # ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ì—¬ê¸°ì„œ ì²˜ë¦¬
                        
#                         image_analysis = ollama_client.analyze_image(
#                             ocr_result.file.path, 
#                             custom_prompt
#                         )
                        
#                         # OCR í…ìŠ¤íŠ¸ ë¶„ì„ (í…ìŠ¤íŠ¸ê°€ ìˆê³  both ëª¨ë“œì¸ ê²½ìš° & ê´€ë ¨ ìˆëŠ” ê²½ìš°ë§Œ)
#                         if ocr_result.ocr_text and analysis_type == 'both' and text_relevant:
#                             # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì •ë¦¬ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸
#                             text_prompt = f"""ë‹¤ìŒ OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ê³  ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

# {ocr_result.ocr_text}

# ë¶„ì„ ì§€ì¹¨:
# 1. í…ìŠ¤íŠ¸ì˜ ì£¼ìš” ë‚´ìš©ê³¼ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì—¬ ì •ë¦¬
# 2. ë‹¨ìˆœ ìš”ì•½ì´ ì•„ë‹Œ, í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ì •ë³´ë¥¼ ì¶©ì‹¤í•˜ê²Œ ì •ë¦¬
# 3. ì¤‘ìš”í•œ ì„¸ë¶€ ì •ë³´ë¥¼ í¬í•¨
# 4. ë‚´ìš©ì´ ì´ë¯¸ì§€ì™€ ê´€ë ¨ì´ ìˆìœ¼ë¯€ë¡œ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ì •ë¦¬

# ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
                            
#                             try:
#                                 text_analysis = ollama_client.analyze_text(ocr_result.ocr_text, text_prompt)
#                             except Exception as e:
#                                 logger.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
#                                 text_analysis = f"í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                            
#                             # ë‘ ë¶„ì„ ê²°ê³¼ ê²°í•©
#                             combined_analysis = f"ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼:\n{image_analysis}\n\ní…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼:\n{text_analysis}"
#                         else:
#                             # OCR ì—†ì´ ì´ë¯¸ì§€ ë¶„ì„ë§Œ ìˆ˜í–‰
#                             combined_analysis = image_analysis
                        
#                     else:  # PDF íŒŒì¼ì¸ ê²½ìš°
#                         if ocr_result.ocr_text:
#                             if analyze_by_page and len(page_texts) > 1:
#                                 # ê°œì„ ëœ í˜ì´ì§€ë³„ ë¶„ì„ ìˆ˜í–‰ - OllamaClientì˜ ë¶„ì„ ê¸°ëŠ¥ í™œìš©
#                                 try:
#                                     combined_analysis = ollama_client.analyze_text(ocr_result.ocr_text, None, page_texts)
#                                     logger.info("í˜ì´ì§€ë³„ ë¶„ì„ ì™„ë£Œ")
#                                 except Exception as e:
#                                     logger.error(f"í˜ì´ì§€ë³„ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
#                                     combined_analysis = f"í˜ì´ì§€ë³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
#                             else:
#                                 # ë¬¸ì„œ ì „ì²´ ë¶„ì„ - í˜ì´ì§€ë³„ êµ¬ì¡°í™” ìš”ì²­
#                                 text_prompt = f"""ë‹¤ìŒ PDFì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë³„ë¡œ ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

# {ocr_result.ocr_text}

# ë¶„ì„ ì§€ì¹¨:
# 1. í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë‚˜ ì„¹ì…˜ ë‹¨ìœ„ë¡œ êµ¬ë¶„í•˜ì—¬ ì •ë¦¬í•´ì£¼ì„¸ìš”.
# 2. ë‚´ìš©ì„ ìš”ì•½í•˜ì§€ ë§ê³ , ê° ì„¹ì…˜ì˜ í•µì‹¬ ì •ë³´ë¥¼ ì¶©ì‹¤í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
# 3. ëª¨ë“  ì¤‘ìš”í•œ ì„¸ë¶€ ì •ë³´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
# 4. ë‚´ìš©ì„ ë‹¨ìˆœ ìš”ì•½í•˜ì§€ ë§ê³ , êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

# ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
# ===== í˜ì´ì§€ 1 (ë˜ëŠ” ì„¹ì…˜ 1) =====
# - ì£¼ìš” ë‚´ìš© ì •ë¦¬
# - ì¤‘ìš” ê°œë… ì„¤ëª…
# - í•µì‹¬ ì •ë³´ ë‚˜ì—´

# ===== í˜ì´ì§€ 2 (ë˜ëŠ” ì„¹ì…˜ 2) =====
# - ì£¼ìš” ë‚´ìš© ì •ë¦¬
# ...

# ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
                                
#                                 try:
#                                     text_analysis = ollama_client.analyze_text(ocr_result.ocr_text, text_prompt)
#                                     combined_analysis = text_analysis
#                                 except Exception as e:
#                                     logger.error(f"ë¬¸ì„œ ì „ì²´ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
#                                     combined_analysis = f"ë¬¸ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\nOCR ê²°ê³¼: {ocr_result.ocr_text[:500]}..."
                    
#                     logger.info("ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë¶„ì„ ì™„ë£Œ")
                
#                 # MySQL ì €ì¥ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì •í™”
#                 ocr_result.llm_response = self.clean_text(combined_analysis)
                
#             except Exception as e:
#                 logger.error("ì²˜ë¦¬ ì‹¤íŒ¨: %s", str(e), exc_info=True)
#                 return Response({'error': f'ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}'}, 
#                                status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
#             # í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ì •ë³´ ì €ì¥
#             ocr_result.text_relevant = text_relevant
            
#             # ê²°ê³¼ ì €ì¥
#             ocr_result.save()
#             logger.info("OCRResult ì €ì¥ ì™„ë£Œ (ID: %s)", ocr_result.id)
            
#             # ì‘ë‹µ ë°˜í™˜
#             serializer = OCRResultSerializer(ocr_result)
#             return Response(serializer.data, status=status.HTTP_201_CREATED)
                
#         except Exception as e:
#             logger.error("ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: %s", str(e), exc_info=True)
#             return Response({'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'}, 
#                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
#     # í•„ìš”í•œ PDF ì²˜ë¦¬ ë©”ì„œë“œ ì¶”ê°€
#     def extract_text_from_pdf_by_pages(self, pdf_path):
#         """PDFì—ì„œ ì§ì ‘ í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë³„ë¡œ ì¶”ì¶œ"""
#         pages = []
#         try:
#             with open(pdf_path, 'rb') as file:
#                 reader = PyPDF2.PdfReader(file)
#                 total_pages = len(reader.pages)
                
#                 for i in range(total_pages):
#                     page = reader.pages[i]
#                     text = page.extract_text()
#                     pages.append({"page": i + 1, "text": text})
                    
#             return pages
#         except Exception as e:
#             logger.error(f"PDF í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
#             raise
    
#     def ocr_pdf_by_pages(self, pdf_path, start_page=1, end_page=0):
#         """PDFë¥¼ OCRë¡œ ì²˜ë¦¬í•˜ì—¬ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
#         pages = []
        
#         try:
#             # PDF2Imageë¡œ ì´ë¯¸ì§€ ë³€í™˜
#             with tempfile.TemporaryDirectory() as path:
#                 # í˜ì´ì§€ ë²ˆí˜¸ëŠ” 1ë¶€í„° ì‹œì‘í•˜ì§€ë§Œ, convert_from_pathëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ ì¡°ì •
#                 first_page = start_page
#                 last_page = None if end_page <= 0 else end_page
                
#                 images = convert_from_path(
#                     pdf_path, 
#                     dpi=300, 
#                     output_folder=path, 
#                     first_page=first_page,
#                     last_page=last_page
#                 )
                
#                 # Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
#                 ollama_base_url = os.environ.get('OLLAMA_API_URL', 'http://localhost:11434')
#                 ollama_client = OllamaClient(base_url=ollama_base_url)
                
#                 # ê° í˜ì´ì§€ ì´ë¯¸ì§€ OCR ì²˜ë¦¬
#                 for i, image in enumerate(images):
#                     # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
#                     preprocessed_img = ollama_client.preprocess_image_for_ocr(image)
#                     # OCR ìˆ˜í–‰
#                     text = ollama_client.get_optimized_ocr_text(preprocessed_img, lang='kor+eng')
                    
#                     # í˜ì´ì§€ ë²ˆí˜¸ ê³„ì‚° (ì‹œì‘ í˜ì´ì§€ ê³ ë ¤)
#                     page_num = start_page + i
#                     pages.append({"page": page_num, "text": text})
                    
#             return pages
#         except Exception as e:
#             logger.error(f"PDF OCR ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
#             raise
    
#     def clean_text(self, text):
#         """í…ìŠ¤íŠ¸ ì •í™” í•¨ìˆ˜"""
#         if not text:
#             return ""
            
#         # ì—°ì†ëœ ê³µë°± ì œê±°
#         text = re.sub(r'\s+', ' ', text)
#         # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì œê±°
#         text = re.sub(r'\n\s*\n', '\n\n', text)
#         # ì•ë’¤ ê³µë°± ì œê±°
#         text = text.strip()
        
#         return text

# import logging
# import json
# import os
# from django.views.decorators.csrf import csrf_exempt
# from django.utils.decorators import method_decorator
# from rest_framework.views import APIView
# from rest_framework.response import Response
# from rest_framework import status
# from rest_framework.permissions import AllowAny
# from PIL import Image, ImageFilter, ImageEnhance
# import pytesseract
# from .models import OCRResult
# from .serializers import OCRResultSerializer

# import PyPDF2
# import tempfile
# from pdf2image import convert_from_path
# import re

# logger = logging.getLogger(__name__)

# # OllamaClient í´ë˜ìŠ¤ ê°€ì ¸ì˜¤ê¸°
# from .ollama_client import OllamaClient

# @method_decorator(csrf_exempt, name='dispatch')
# class ProcessFileView(APIView):
#     permission_classes = [AllowAny]
    
#     def post(self, request):
#         try:
#             logger.info("ProcessFileView ìš”ì²­ ìˆ˜ì‹ : %s %s", request.method, request.path)
            
#             # ìš”ì²­ ë°ì´í„° í™•ì¸
#             if 'file' not in request.FILES:
#                 logger.error("íŒŒì¼ì´ ì œê³µë˜ì§€ ì•ŠìŒ")
#                 return Response({'error': 'íŒŒì¼ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}, status=status.HTTP_400_BAD_REQUEST)
            
#             file_obj = request.FILES['file']
#             file_name = file_obj.name.lower()
#             logger.info("íŒŒì¼ ì—…ë¡œë“œ: %s, í¬ê¸°: %s bytes", file_name, file_obj.size)
            
#             # Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” - ì—¬ê¸°ì„œ ê°ì²´ ìƒì„±
#             ollama_base_url = os.environ.get('OLLAMA_API_URL', 'http://localhost:11434')
#             ollama_client = OllamaClient(base_url=ollama_base_url)
            
#             # íŒŒì¼ ìœ í˜• í™•ì¸
#             if file_name.endswith(('.pdf')):
#                 file_type = 'pdf'
                
#                 # PDF í˜ì´ì§€ ë²”ìœ„ í™•ì¸
#                 start_page = int(request.data.get('start_page', 1))
#                 end_page = int(request.data.get('end_page', 0))  # 0ì€ ì „ì²´ í˜ì´ì§€ë¥¼ ì˜ë¯¸
                
#                 logger.info("PDF ì²˜ë¦¬ ë²”ìœ„: %s ~ %s í˜ì´ì§€", start_page, end_page if end_page > 0 else "ë")
                
#             elif file_name.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif')):
#                 file_type = 'image'
#             else:
#                 logger.error("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: %s", file_name)
#                 return Response({'error': 'ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ë‚˜ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.'},
#                               status=status.HTTP_400_BAD_REQUEST)
            
#             # OCR ê²°ê³¼ ê°ì²´ ìƒì„±
#             ocr_result = OCRResult.objects.create(file=file_obj, file_type=file_type)
#             logger.info("OCRResult ê°ì²´ ìƒì„±: %s", ocr_result.id)
            
#             # OCR ì²˜ë¦¬
#             try:
#                 ocr_text = ""
#                 page_texts = []  # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì €ì¥
#                 text_relevant = False  # í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ì—¬ë¶€ (ê¸°ë³¸ê°’: ê´€ë ¨ ì—†ìŒ)
                
#                 if file_type == 'image':
#                     # ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬ - ê°œì„ ëœ OCR ì ìš©
#                     img = Image.open(ocr_result.file.path)
#                     # ì´ë¯¸ì§€ ì •ë³´ ë¡œê¹…
#                     logger.info(f"ì´ë¯¸ì§€ ì •ë³´: í¬ê¸°={img.size}, ëª¨ë“œ={img.mode}, í¬ë§·={img.format}")
                    
#                     # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° OCR ìˆ˜í–‰ - OllamaClient ë©”ì„œë“œ ì‚¬ìš©
#                     preprocessed_img = ollama_client.preprocess_image_for_ocr(img)
#                     ocr_text = ollama_client.get_optimized_ocr_text(preprocessed_img, lang='kor+eng')
#                     page_texts.append({"page": 1, "text": ocr_text})
#                     logger.info("ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì™„ë£Œ, ì¶”ì¶œ í…ìŠ¤íŠ¸ ê¸¸ì´: %s", len(ocr_text))
#                     logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
                
#                 elif file_type == 'pdf':
#                     # PDF ì²˜ë¦¬ - ì§ì ‘ ì¶”ì¶œ í›„ í•„ìš”ì‹œ OCR
#                     logger.info("PDF ì²˜ë¦¬ ì‹œì‘: %s", ocr_result.file.path)
                    
#                     # ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„ (í˜ì´ì§€ë³„)
#                     direct_extract_success = False
#                     try:
#                         all_page_texts = self.extract_text_from_pdf_by_pages(ocr_result.file.path)
                        
#                         # í˜ì´ì§€ ë²”ìœ„ ì²˜ë¦¬
#                         if start_page > 1 or (end_page > 0 and end_page < len(all_page_texts)):
#                             if start_page <= len(all_page_texts):
#                                 if end_page > 0 and end_page >= start_page:
#                                     page_texts = all_page_texts[start_page-1:end_page]
#                                 else:
#                                     page_texts = all_page_texts[start_page-1:]
#                             else:
#                                 page_texts = []
#                         else:
#                             page_texts = all_page_texts
                        
#                         combined_text = "\n".join([page["text"] for page in page_texts])
                        
#                         # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì¶©ë¶„í•œì§€ í™•ì¸ (ë” ì—„ê²©í•œ ì¡°ê±´)
#                         if combined_text.strip() and len(combined_text.strip()) >= 50:
#                             meaningful_chars = sum(1 for c in combined_text if c.isalnum())
#                             if meaningful_chars > 30:  # ì˜ë¯¸ìˆëŠ” ê¸€ìê°€ 30ì ì´ìƒì´ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
#                                 ocr_text = combined_text
#                                 direct_extract_success = True
#                                 logger.info("PDF ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ, ì´ %s í˜ì´ì§€, í…ìŠ¤íŠ¸ ê¸¸ì´: %s", 
#                                           len(page_texts), len(ocr_text))
#                                 logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
#                     except Exception as e:
#                         logger.error(f"PDF ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    
#                     # ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì‹¤íŒ¨í•œ ê²½ìš°, OCR ì‹œë„
#                     if not direct_extract_success:
#                         logger.info("PDF OCR ì²˜ë¦¬ ì‹œì‘ (ì§ì ‘ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶ˆì¶©ë¶„)")
                        
#                         # í˜ì´ì§€ ë²”ìœ„ ì„¤ì •ìœ¼ë¡œ OCR
#                         all_page_texts = self.ocr_pdf_by_pages(ocr_result.file.path, ollama_client, start_page, end_page)
                        
#                         # í˜ì´ì§€ ë²”ìœ„ ì²˜ë¦¬ - ocr_pdf_by_pagesì—ì„œ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ì „ì²´ ì‚¬ìš©
#                         page_texts = all_page_texts
                        
#                         ocr_text = "\n".join([page["text"] for page in page_texts])
#                         logger.info("PDF OCR ì²˜ë¦¬ ì™„ë£Œ, ì´ %s í˜ì´ì§€, í…ìŠ¤íŠ¸ ê¸¸ì´: %s", 
#                                   len(page_texts), len(ocr_text))
#                         logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
                
#                 # í…ìŠ¤íŠ¸ ì •í™” - ê°œì„ ëœ í•¨ìˆ˜ ì‚¬ìš©
#                 ocr_result.ocr_text = self.clean_text(ocr_text)
                
#                 # PDF íŒŒì¼ì€ í•­ìƒ í…ìŠ¤íŠ¸ ê´€ë ¨ ìˆìŒìœ¼ë¡œ ì„¤ì •
#                 if file_type == 'pdf':
#                     text_relevant = True
                
#                 # ë¶„ì„ ìœ í˜• í™•ì¸ (ê¸°ë³¸ê°’: both)
#                 analysis_type = request.data.get('analysis_type', 'both')
#                 logger.info("ë¶„ì„ ìœ í˜•: %s", analysis_type)
                
#                 # ê²°ê³¼ ë³€ìˆ˜ ì´ˆê¸°í™”
#                 image_analysis = ""
#                 text_analysis = ""
#                 combined_analysis = ""
                
#                 # í˜ì´ì§€ ë¶„í•  ë¶„ì„ ì—¬ë¶€ í™•ì¸
#                 analyze_by_page = request.data.get('analyze_by_page', 'true').lower() == 'true'
                
#                 # ì„ íƒëœ ë¶„ì„ ìœ í˜•ì— ë”°ë¼ ì²˜ë¦¬
#                 if analysis_type in ['ollama', 'both']:
#                     # ì´ë¯¸ì§€ íŒŒì¼ì¸ ê²½ìš°
#                     if file_type == 'image':
#                         # OCRì„ ë¨¼ì € ìˆ˜í–‰í•˜ê³  ì´ë¯¸ì§€ ë¶„ì„ì— OCR ê²°ê³¼ë¥¼ í¬í•¨
#                         custom_prompt = None  # ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ì—¬ê¸°ì„œ ì²˜ë¦¬
                        
#                         # í…ìŠ¤íŠ¸ ê´€ë ¨ì„± í™•ì¸ì€ í•˜ì§€ ì•Šê³  OCR í…ìŠ¤íŠ¸ë¥¼ í•­ìƒ ì „ë‹¬
#                         # analyze_image í•¨ìˆ˜ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ ê´€ë ¨ì„±ì„ ê²°ì •
#                         image_analysis = ollama_client.analyze_image(
#                             ocr_result.file.path, 
#                             custom_prompt,
#                             ocr_text=ocr_result.ocr_text
#                         )
                        
#                         # OCR í…ìŠ¤íŠ¸ ë¶„ì„ (í…ìŠ¤íŠ¸ê°€ ìˆê³  both ëª¨ë“œì¸ ê²½ìš°)
#                         if ocr_result.ocr_text and analysis_type == 'both':
#                             # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì •ë¦¬ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸
#                             text_prompt = f"""ë‹¤ìŒ OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ê³  ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

# {ocr_result.ocr_text}

# ë¶„ì„ ì§€ì¹¨:
# 1. í…ìŠ¤íŠ¸ì˜ ì£¼ìš” ë‚´ìš©ê³¼ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì—¬ ì •ë¦¬
# 2. ë‹¨ìˆœ ìš”ì•½ì´ ì•„ë‹Œ, í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ì •ë³´ë¥¼ ì¶©ì‹¤í•˜ê²Œ ì •ë¦¬
# 3. ì¤‘ìš”í•œ ì„¸ë¶€ ì •ë³´ë¥¼ í¬í•¨
# 4. ë‚´ìš©ì´ ì´ë¯¸ì§€ì™€ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ì •ë¦¬

# ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
                            
#                             try:
#                                 text_analysis = ollama_client.analyze_text(ocr_result.ocr_text, text_prompt)
#                             except Exception as e:
#                                 logger.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
#                                 text_analysis = f"í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                            
#                             # ë‘ ë¶„ì„ ê²°ê³¼ ê²°í•©
#                             combined_analysis = f"ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼:\n{image_analysis}\n\ní…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼:\n{text_analysis}"
#                         else:
#                             # OCR ì—†ì´ ì´ë¯¸ì§€ ë¶„ì„ë§Œ ìˆ˜í–‰
#                             combined_analysis = image_analysis
                        
#                     else:  # PDF íŒŒì¼ì¸ ê²½ìš°
#                         if ocr_result.ocr_text:
#                             if analyze_by_page and len(page_texts) > 1:
#                                 # ê°œì„ ëœ í˜ì´ì§€ë³„ ë¶„ì„ ìˆ˜í–‰ - OllamaClientì˜ ë¶„ì„ ê¸°ëŠ¥ í™œìš©
#                                 try:
#                                     combined_analysis = ollama_client.analyze_text(ocr_result.ocr_text, None, page_texts)
#                                     logger.info("í˜ì´ì§€ë³„ ë¶„ì„ ì™„ë£Œ")
#                                 except Exception as e:
#                                     logger.error(f"í˜ì´ì§€ë³„ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
#                                     combined_analysis = f"í˜ì´ì§€ë³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
#                             else:
#                                 # ë¬¸ì„œ ì „ì²´ ë¶„ì„ - í˜ì´ì§€ë³„ êµ¬ì¡°í™” ìš”ì²­
#                                 text_prompt = f"""ë‹¤ìŒ PDFì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë³„ë¡œ ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

# {ocr_result.ocr_text}

# ë¶„ì„ ì§€ì¹¨:
# 1. í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë‚˜ ì„¹ì…˜ ë‹¨ìœ„ë¡œ êµ¬ë¶„í•˜ì—¬ ì •ë¦¬í•´ì£¼ì„¸ìš”.
# 2. ë‚´ìš©ì„ ìš”ì•½í•˜ì§€ ë§ê³ , ê° ì„¹ì…˜ì˜ í•µì‹¬ ì •ë³´ë¥¼ ì¶©ì‹¤í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
# 3. ëª¨ë“  ì¤‘ìš”í•œ ì„¸ë¶€ ì •ë³´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
# 4. ë‚´ìš©ì„ ë‹¨ìˆœ ìš”ì•½í•˜ì§€ ë§ê³ , êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

# ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
# ===== í˜ì´ì§€ 1 (ë˜ëŠ” ì„¹ì…˜ 1) =====
# - ì£¼ìš” ë‚´ìš© ì •ë¦¬
# - ì¤‘ìš” ê°œë… ì„¤ëª…
# - í•µì‹¬ ì •ë³´ ë‚˜ì—´

# ===== í˜ì´ì§€ 2 (ë˜ëŠ” ì„¹ì…˜ 2) =====
# - ì£¼ìš” ë‚´ìš© ì •ë¦¬
# ...

# ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
                                
#                                 try:
#                                     text_analysis = ollama_client.analyze_text(ocr_result.ocr_text, text_prompt)
#                                     combined_analysis = text_analysis
#                                 except Exception as e:
#                                     logger.error(f"ë¬¸ì„œ ì „ì²´ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
#                                     combined_analysis = f"ë¬¸ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\nOCR ê²°ê³¼: {ocr_result.ocr_text[:500]}..."
                    
#                     logger.info("ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë¶„ì„ ì™„ë£Œ")
                
#                 # MySQL ì €ì¥ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì •í™”
#                 ocr_result.llm_response = self.clean_text(combined_analysis)
#                 ocr_result.text_relevant = text_relevant
                
#             except Exception as e:
#                 logger.error("ì²˜ë¦¬ ì‹¤íŒ¨: %s", str(e), exc_info=True)
#                 return Response({'error': f'ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}'}, 
#                                status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
#             # ê²°ê³¼ ì €ì¥
#             ocr_result.save()
#             logger.info("OCRResult ì €ì¥ ì™„ë£Œ (ID: %s)", ocr_result.id)
            
#             # ì‘ë‹µ ë°˜í™˜
#             serializer = OCRResultSerializer(ocr_result)
#             return Response(serializer.data, status=status.HTTP_201_CREATED)
                
#         except Exception as e:
#             logger.error("ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: %s", str(e), exc_info=True)
#             return Response({'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'}, 
#                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
#     def extract_text_from_pdf_by_pages(self, pdf_path):
#         """PDFì—ì„œ ì§ì ‘ í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë³„ë¡œ ì¶”ì¶œ"""
#         pages = []
#         try:
#             with open(pdf_path, 'rb') as file:
#                 reader = PyPDF2.PdfReader(file)
#                 total_pages = len(reader.pages)
                
#                 for i in range(total_pages):
#                     page = reader.pages[i]
#                     text = page.extract_text()
#                     pages.append({"page": i + 1, "text": text})
                    
#             return pages
#         except Exception as e:
#             logger.error(f"PDF í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
#             raise
    
#     def ocr_pdf_by_pages(self, pdf_path, ollama_client, start_page=1, end_page=0):
#         """PDFë¥¼ OCRë¡œ ì²˜ë¦¬í•˜ì—¬ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
#         pages = []
        
#         try:
#             # PDF2Imageë¡œ ì´ë¯¸ì§€ ë³€í™˜
#             with tempfile.TemporaryDirectory() as path:
#                 # í˜ì´ì§€ ë²ˆí˜¸ëŠ” 1ë¶€í„° ì‹œì‘í•˜ì§€ë§Œ, convert_from_pathëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ ì¡°ì •
#                 first_page = start_page
#                 last_page = None if end_page <= 0 else end_page
                
#                 images = convert_from_path(
#                     pdf_path, 
#                     dpi=300, 
#                     output_folder=path, 
#                     first_page=first_page,
#                     last_page=last_page
#                 )
                
#                 # ê° í˜ì´ì§€ ì´ë¯¸ì§€ OCR ì²˜ë¦¬
#                 for i, image in enumerate(images):
#                     # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
#                     preprocessed_img = ollama_client.preprocess_image_for_ocr(image)
#                     # OCR ìˆ˜í–‰
#                     text = ollama_client.get_optimized_ocr_text(preprocessed_img, lang='kor+eng')
                    
#                     # í˜ì´ì§€ ë²ˆí˜¸ ê³„ì‚° (ì‹œì‘ í˜ì´ì§€ ê³ ë ¤)
#                     page_num = start_page + i
#                     pages.append({"page": page_num, "text": text})
                    
#             return pages
#         except Exception as e:
#             logger.error(f"PDF OCR ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
#             raise
    
#     def clean_text(self, text):
#         """í…ìŠ¤íŠ¸ ì •í™” í•¨ìˆ˜"""
#         if not text:
#             return ""
            
#         # ì—°ì†ëœ ê³µë°± ì œê±°
#         text = re.sub(r'\s+', ' ', text)
#         # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì œê±°
#         text = re.sub(r'\n\s*\n', '\n\n', text)
#         # ì•ë’¤ ê³µë°± ì œê±°
#         text = text.strip()
        
#         return text

# import logging
# import json
# import os
# from django.views.decorators.csrf import csrf_exempt
# from django.utils.decorators import method_decorator
# from rest_framework.views import APIView
# from rest_framework.response import Response
# from rest_framework import status
# from rest_framework.permissions import AllowAny
# from PIL import Image, ImageFilter, ImageEnhance
# import pytesseract
# from .models import OCRResult
# from .serializers import OCRResultSerializer

# import PyPDF2
# import tempfile
# from pdf2image import convert_from_path
# import re

# logger = logging.getLogger(__name__)

# # OllamaClient í´ë˜ìŠ¤ ê°€ì ¸ì˜¤ê¸°
# from .ollama_client import OllamaClient

# @method_decorator(csrf_exempt, name='dispatch')
# class ProcessFileView(APIView):
#     permission_classes = [AllowAny]
    
#     def post(self, request):
#         try:
#             logger.info("ProcessFileView ìš”ì²­ ìˆ˜ì‹ : %s %s", request.method, request.path)
            
#             # ìš”ì²­ ë°ì´í„° í™•ì¸
#             if 'file' not in request.FILES:
#                 logger.error("íŒŒì¼ì´ ì œê³µë˜ì§€ ì•ŠìŒ")
#                 return Response({'error': 'íŒŒì¼ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}, status=status.HTTP_400_BAD_REQUEST)
            
#             file_obj = request.FILES['file']
#             file_name = file_obj.name.lower()
#             logger.info("íŒŒì¼ ì—…ë¡œë“œ: %s, í¬ê¸°: %s bytes", file_name, file_obj.size)
            
#             # Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” - ì—¬ê¸°ì„œ ê°ì²´ ìƒì„±
#             ollama_base_url = os.environ.get('OLLAMA_API_URL', 'http://localhost:11434')
#             ollama_client = OllamaClient(base_url=ollama_base_url)
            
#             # íŒŒì¼ ìœ í˜• í™•ì¸
#             if file_name.endswith(('.pdf')):
#                 file_type = 'pdf'
                
#                 # PDF í˜ì´ì§€ ë²”ìœ„ í™•ì¸
#                 start_page = int(request.data.get('start_page', 1))
#                 end_page = int(request.data.get('end_page', 0))  # 0ì€ ì „ì²´ í˜ì´ì§€ë¥¼ ì˜ë¯¸
                
#                 logger.info("PDF ì²˜ë¦¬ ë²”ìœ„: %s ~ %s í˜ì´ì§€", start_page, end_page if end_page > 0 else "ë")
                
#             elif file_name.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif')):
#                 file_type = 'image'
#             else:
#                 logger.error("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: %s", file_name)
#                 return Response({'error': 'ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ë‚˜ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.'},
#                               status=status.HTTP_400_BAD_REQUEST)
            
#             # OCR ê²°ê³¼ ê°ì²´ ìƒì„±
#             ocr_result = OCRResult.objects.create(file=file_obj, file_type=file_type)
#             logger.info("OCRResult ê°ì²´ ìƒì„±: %s", ocr_result.id)
            
#             # OCR ì²˜ë¦¬
#             try:
#                 ocr_text = ""
#                 page_texts = []  # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì €ì¥
                
#                 if file_type == 'image':
#                     # ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬ - ê°œì„ ëœ OCR ì ìš©
#                     img = Image.open(ocr_result.file.path)
#                     # ì´ë¯¸ì§€ ì •ë³´ ë¡œê¹…
#                     logger.info(f"ì´ë¯¸ì§€ ì •ë³´: í¬ê¸°={img.size}, ëª¨ë“œ={img.mode}, í¬ë§·={img.format}")
                    
#                     # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° OCR ìˆ˜í–‰ - OllamaClient ë©”ì„œë“œ ì‚¬ìš©
#                     preprocessed_img = ollama_client.preprocess_image_for_ocr(img)
#                     ocr_text = ollama_client.get_optimized_ocr_text(preprocessed_img, lang='kor+eng')
#                     page_texts.append({"page": 1, "text": ocr_text})
#                     logger.info("ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì™„ë£Œ, ì¶”ì¶œ í…ìŠ¤íŠ¸ ê¸¸ì´: %s", len(ocr_text))
#                     logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
                
#                 elif file_type == 'pdf':
#                     # PDF ì²˜ë¦¬ - ì§ì ‘ ì¶”ì¶œ í›„ í•„ìš”ì‹œ OCR
#                     logger.info("PDF ì²˜ë¦¬ ì‹œì‘: %s", ocr_result.file.path)
                    
#                     # ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„ (í˜ì´ì§€ë³„)
#                     direct_extract_success = False
#                     try:
#                         all_page_texts = self.extract_text_from_pdf_by_pages(ocr_result.file.path)
                        
#                         # í˜ì´ì§€ ë²”ìœ„ ì²˜ë¦¬
#                         if start_page > 1 or (end_page > 0 and end_page < len(all_page_texts)):
#                             if start_page <= len(all_page_texts):
#                                 if end_page > 0 and end_page >= start_page:
#                                     page_texts = all_page_texts[start_page-1:end_page]
#                                 else:
#                                     page_texts = all_page_texts[start_page-1:]
#                             else:
#                                 page_texts = []
#                         else:
#                             page_texts = all_page_texts
                        
#                         combined_text = "\n".join([page["text"] for page in page_texts])
                        
#                         # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì¶©ë¶„í•œì§€ í™•ì¸ (ë” ì—„ê²©í•œ ì¡°ê±´)
#                         if combined_text.strip() and len(combined_text.strip()) >= 50:
#                             meaningful_chars = sum(1 for c in combined_text if c.isalnum())
#                             if meaningful_chars > 30:  # ì˜ë¯¸ìˆëŠ” ê¸€ìê°€ 30ì ì´ìƒì´ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
#                                 ocr_text = combined_text
#                                 direct_extract_success = True
#                                 logger.info("PDF ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ, ì´ %s í˜ì´ì§€, í…ìŠ¤íŠ¸ ê¸¸ì´: %s", 
#                                           len(page_texts), len(ocr_text))
#                                 logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
#                     except Exception as e:
#                         logger.error(f"PDF ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    
#                     # ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì‹¤íŒ¨í•œ ê²½ìš°, OCR ì‹œë„
#                     if not direct_extract_success:
#                         logger.info("PDF OCR ì²˜ë¦¬ ì‹œì‘ (ì§ì ‘ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶ˆì¶©ë¶„)")
                        
#                         # í˜ì´ì§€ ë²”ìœ„ ì„¤ì •ìœ¼ë¡œ OCR
#                         all_page_texts = self.ocr_pdf_by_pages(ocr_result.file.path, ollama_client, start_page, end_page)
                        
#                         # í˜ì´ì§€ ë²”ìœ„ ì²˜ë¦¬ - ocr_pdf_by_pagesì—ì„œ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ì „ì²´ ì‚¬ìš©
#                         page_texts = all_page_texts
                        
#                         ocr_text = "\n".join([page["text"] for page in page_texts])
#                         logger.info("PDF OCR ì²˜ë¦¬ ì™„ë£Œ, ì´ %s í˜ì´ì§€, í…ìŠ¤íŠ¸ ê¸¸ì´: %s", 
#                                   len(page_texts), len(ocr_text))
#                         logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
                
#                 # í…ìŠ¤íŠ¸ ì •í™” - ê°œì„ ëœ í•¨ìˆ˜ ì‚¬ìš©
#                 ocr_result.ocr_text = self.clean_text(ocr_text)
                
#                 # PDF íŒŒì¼ì€ í•­ìƒ í…ìŠ¤íŠ¸ ê´€ë ¨ ìˆìŒìœ¼ë¡œ ì„¤ì •
#                 if file_type == 'pdf':
#                     text_relevant = True
                
#                 # ë¶„ì„ ìœ í˜• í™•ì¸ (ê¸°ë³¸ê°’: both)
#                 analysis_type = request.data.get('analysis_type', 'both')
#                 logger.info("ë¶„ì„ ìœ í˜•: %s", analysis_type)
                
#                 # ê²°ê³¼ ë³€ìˆ˜ ì´ˆê¸°í™”
#                 image_analysis = ""
#                 text_analysis = ""
#                 combined_analysis = ""
                
#                 # í˜ì´ì§€ ë¶„í•  ë¶„ì„ ì—¬ë¶€ í™•ì¸
#                 analyze_by_page = request.data.get('analyze_by_page', 'true').lower() == 'true'
                
#                 # ì„ íƒëœ ë¶„ì„ ìœ í˜•ì— ë”°ë¼ ì²˜ë¦¬
#                 if analysis_type in ['ollama', 'both']:
#                     # ì´ë¯¸ì§€ íŒŒì¼ì¸ ê²½ìš°
#                     if file_type == 'image':
#                         # ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ ì„¤ì • (ìš”ì•½ëœ ê°„ê²°í•œ ì„¤ëª…ì„ ìœ„í•´)
#                         custom_prompt = f"""ì´ë¯¸ì§€ë¥¼ ê°ê´€ì ìœ¼ë¡œ ê´€ì°°í•˜ê³  ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ ì‘ë‹µí•˜ì„¸ìš”:

# í•„ìˆ˜ í¬í•¨ ì‚¬í•­:
# - ì´ë¯¸ì§€ì— ì‹¤ì œë¡œ ë³´ì´ëŠ” ì‚¬ëŒ, ë™ë¬¼, ë¬¼ì²´ë§Œ ì–¸ê¸‰ (ì—†ìœ¼ë©´ ì–¸ê¸‰í•˜ì§€ ì•ŠìŒ)
# - ë§Œì•½ ë™ë¬¼ì´ë¼ë©´, ì–´ë–¤ ì¢…ì˜ ë™ë¬¼ì¸ì§€ë„ ì¶œë ¥
# - í™•ì‹¤íˆ ë³´ì´ëŠ” ìƒ‰ìƒë§Œ ì–¸ê¸‰ (ë°°ê²½ìƒ‰, ì˜· ìƒ‰ìƒ ë“±)
# - ëª…í™•íˆ ë³´ì´ëŠ” ìì„¸ë‚˜ ìœ„ì¹˜ ê´€ê³„ (ì •ë©´, ì¸¡ë©´ ë“±)

# ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ:
# - ì¶”ì¸¡ì´ë‚˜ í•´ì„ ("~ë¡œ ë³´ì…ë‹ˆë‹¤", "~ê°™ìŠµë‹ˆë‹¤" í‘œí˜„ ê¸ˆì§€)
# - ë³´ì´ì§€ ì•ŠëŠ” ë¶€ë¶„ì— ëŒ€í•œ ì–¸ê¸‰ ("ë³´ì´ì§€ ì•ŠëŠ”ë‹¤", "ì—†ë‹¤" ë“±ì˜ í‘œí˜„ ê¸ˆì§€)
# - ë°˜ë³µì ì¸ ì„¤ëª…
# - ê°ì •ì´ë‚˜ ë¶„ìœ„ê¸° ë¬˜ì‚¬

# í˜•ì‹:
# - 1-2ë¬¸ì¥ìœ¼ë¡œ ë§¤ìš° ê°„ê²°í•˜ê²Œ ì‘ì„±
# - ë‹¨ìˆœ ì‚¬ì‹¤ ë‚˜ì—´ í˜•ì‹ (ì˜ˆ: "ì´ë¯¸ì§€ì—ëŠ” ê²€ì€ ë¨¸ë¦¬ ì—¬ì„±ì´ ìˆê³ , ë°°ê²½ì€ í°ìƒ‰ì´ë‹¤.")

# OCR í…ìŠ¤íŠ¸ (ì°¸ê³ ìš©, ì‹¤ì œ ì´ë¯¸ì§€ì— ë³´ì´ëŠ” ê²½ìš°ë§Œ ì–¸ê¸‰): {ocr_result.ocr_text}

# ì˜ì–´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

                        
#                         # OCR í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬ (analyze_image ë‚´ë¶€ì—ì„œ ê´€ë ¨ì„± íŒë‹¨)
#                         image_analysis = ollama_client.analyze_image(
#                             ocr_result.file.path, 
#                             custom_prompt,
#                             ocr_text=ocr_result.ocr_text
#                         )
                        
#                         # OCR í…ìŠ¤íŠ¸ ë¶„ì„ (í…ìŠ¤íŠ¸ê°€ ìˆê³  both ëª¨ë“œì¸ ê²½ìš°)
#                         if ocr_result.ocr_text and analysis_type == 'both':
#                             # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì •ë¦¬ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸
#                             text_prompt = f"""ë‹¤ìŒ OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ê³  ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

# {ocr_result.ocr_text}

# ë¶„ì„ ì§€ì¹¨:
# 1. í…ìŠ¤íŠ¸ì˜ ì£¼ìš” ë‚´ìš©ê³¼ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì—¬ ì •ë¦¬
# 2. ë‹¨ìˆœ ìš”ì•½ì´ ì•„ë‹Œ, í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ì •ë³´ë¥¼ ì¶©ì‹¤í•˜ê²Œ ì •ë¦¬
# 3. ì¤‘ìš”í•œ ì„¸ë¶€ ì •ë³´ë¥¼ í¬í•¨
# 4. ë‚´ìš©ì´ ì´ë¯¸ì§€ì™€ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ì •ë¦¬

# ë°˜ë“œì‹œ "ì˜ì–´(En)"ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
                            
#                             try:
#                                 text_analysis = ollama_client.analyze_text(ocr_result.ocr_text, text_prompt)
#                             except Exception as e:
#                                 logger.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
#                                 text_analysis = f"í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                            
#                             # ë‘ ë¶„ì„ ê²°ê³¼ ê²°í•©
#                             combined_analysis = f"ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼:\n{image_analysis}\n\ní…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼:\n{text_analysis}"
#                         else:
#                             # OCR ì—†ì´ ì´ë¯¸ì§€ ë¶„ì„ë§Œ ìˆ˜í–‰
#                             combined_analysis = image_analysis
                        
#                     else:  # PDF íŒŒì¼ì¸ ê²½ìš°
#                         if ocr_result.ocr_text:
#                             if analyze_by_page and len(page_texts) > 1:
#                                 # ê°œì„ ëœ í˜ì´ì§€ë³„ ë¶„ì„ ìˆ˜í–‰ - OllamaClientì˜ ë¶„ì„ ê¸°ëŠ¥ í™œìš©
#                                 try:
#                                     combined_analysis = ollama_client.analyze_text(ocr_result.ocr_text, None, page_texts)
#                                     logger.info("í˜ì´ì§€ë³„ ë¶„ì„ ì™„ë£Œ")
#                                 except Exception as e:
#                                     logger.error(f"í˜ì´ì§€ë³„ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
#                                     combined_analysis = f"í˜ì´ì§€ë³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
#                             else:
#                                 # ë¬¸ì„œ ì „ì²´ ë¶„ì„ - í˜ì´ì§€ë³„ êµ¬ì¡°í™” ìš”ì²­
#                                 text_prompt = f"""ë‹¤ìŒ PDFì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë³„ë¡œ ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

# {ocr_result.ocr_text}

# ë¶„ì„ ì§€ì¹¨:
# 1. í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë‚˜ ì„¹ì…˜ ë‹¨ìœ„ë¡œ êµ¬ë¶„í•˜ì—¬ ì •ë¦¬í•´ì£¼ì„¸ìš”.
# 2. ë‚´ìš©ì„ ìš”ì•½í•˜ì§€ ë§ê³ , ê° ì„¹ì…˜ì˜ í•µì‹¬ ì •ë³´ë¥¼ ì¶©ì‹¤í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
# 3. ëª¨ë“  ì¤‘ìš”í•œ ì„¸ë¶€ ì •ë³´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
# 4. ë‚´ìš©ì„ ë‹¨ìˆœ ìš”ì•½í•˜ì§€ ë§ê³ , êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

# ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
# ===== í˜ì´ì§€ 1 (ë˜ëŠ” ì„¹ì…˜ 1) =====
# - ì£¼ìš” ë‚´ìš© ì •ë¦¬
# - ì¤‘ìš” ê°œë… ì„¤ëª…
# - í•µì‹¬ ì •ë³´ ë‚˜ì—´

# ===== í˜ì´ì§€ 2 (ë˜ëŠ” ì„¹ì…˜ 2) =====
# - ì£¼ìš” ë‚´ìš© ì •ë¦¬
# ...

# ë°˜ë“œì‹œ "ì˜ì–´(En)"ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
                                
#                                 try:
#                                     text_analysis = ollama_client.analyze_text(ocr_result.ocr_text, text_prompt)
#                                     combined_analysis = text_analysis
#                                 except Exception as e:
#                                     logger.error(f"ë¬¸ì„œ ì „ì²´ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
#                                     combined_analysis = f"ë¬¸ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\nOCR ê²°ê³¼: {ocr_result.ocr_text[:500]}..."
                    
#                     logger.info("ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë¶„ì„ ì™„ë£Œ")
                
#                 # MySQL ì €ì¥ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì •í™”
#                 ocr_result.llm_response = self.clean_text(combined_analysis)
                
#                 # í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ì •ë³´ ì €ì¥ - PDFëŠ” í•­ìƒ True, ì´ë¯¸ì§€ëŠ” ë¶„ì„ ê³¼ì •ì—ì„œ ê²°ì •
#                 if file_type == 'pdf':
#                     ocr_result.text_relevant = True
                
#             except Exception as e:
#                 logger.error("ì²˜ë¦¬ ì‹¤íŒ¨: %s", str(e), exc_info=True)
#                 return Response({'error': f'ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}'}, 
#                                status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
#             # ê²°ê³¼ ì €ì¥
#             ocr_result.save()
#             logger.info("OCRResult ì €ì¥ ì™„ë£Œ (ID: %s)", ocr_result.id)
            
#             # ì‘ë‹µ ë°˜í™˜
#             serializer = OCRResultSerializer(ocr_result)
#             return Response(serializer.data, status=status.HTTP_201_CREATED)
                
#         except Exception as e:
#             logger.error("ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: %s", str(e), exc_info=True)
#             return Response({'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'}, 
#                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
#     def extract_text_from_pdf_by_pages(self, pdf_path):
#         """PDFì—ì„œ ì§ì ‘ í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë³„ë¡œ ì¶”ì¶œ"""
#         pages = []
#         try:
#             with open(pdf_path, 'rb') as file:
#                 reader = PyPDF2.PdfReader(file)
#                 total_pages = len(reader.pages)
                
#                 for i in range(total_pages):
#                     page = reader.pages[i]
#                     text = page.extract_text()
#                     pages.append({"page": i + 1, "text": text})
                    
#             return pages
#         except Exception as e:
#             logger.error(f"PDF í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
#             raise
    
#     def ocr_pdf_by_pages(self, pdf_path, ollama_client, start_page=1, end_page=0):
#         """PDFë¥¼ OCRë¡œ ì²˜ë¦¬í•˜ì—¬ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
#         pages = []
        
#         try:
#             # PDF2Imageë¡œ ì´ë¯¸ì§€ ë³€í™˜
#             with tempfile.TemporaryDirectory() as path:
#                 # í˜ì´ì§€ ë²ˆí˜¸ëŠ” 1ë¶€í„° ì‹œì‘í•˜ì§€ë§Œ, convert_from_pathëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ ì¡°ì •
#                 first_page = start_page
#                 last_page = None if end_page <= 0 else end_page
                
#                 images = convert_from_path(
#                     pdf_path, 
#                     dpi=300, 
#                     output_folder=path, 
#                     first_page=first_page,
#                     last_page=last_page
#                 )
                
#                 # ê° í˜ì´ì§€ ì´ë¯¸ì§€ OCR ì²˜ë¦¬
#                 for i, image in enumerate(images):
#                     # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
#                     preprocessed_img = ollama_client.preprocess_image_for_ocr(image)
#                     # OCR ìˆ˜í–‰
#                     text = ollama_client.get_optimized_ocr_text(preprocessed_img, lang='kor+eng')
                    
#                     # í˜ì´ì§€ ë²ˆí˜¸ ê³„ì‚° (ì‹œì‘ í˜ì´ì§€ ê³ ë ¤)
#                     page_num = start_page + i
#                     pages.append({"page": page_num, "text": text})
                    
#             return pages
#         except Exception as e:
#             logger.error(f"PDF OCR ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
#             raise
    
#     def clean_text(self, text):
#         """í…ìŠ¤íŠ¸ ì •í™” í•¨ìˆ˜"""
#         if not text:
#             return ""
            
#         # ì—°ì†ëœ ê³µë°± ì œê±°
#         text = re.sub(r'\s+', ' ', text)
#         # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì œê±°
#         text = re.sub(r'\n\s*\n', '\n\n', text)
#         # ì•ë’¤ ê³µë°± ì œê±°
#         text = text.strip()
        
#         return text
    
import logging
import json
import os
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
from PIL import Image, ImageFilter, ImageEnhance
import pytesseract
from .models import OCRResult
from .serializers import OCRResultSerializer

import PyPDF2
import tempfile
from pdf2image import convert_from_path
import re

logger = logging.getLogger(__name__)

# OllamaClientì™€ GPTTranslator í´ë˜ìŠ¤ ê°€ì ¸ì˜¤ê¸°
from .ollama_client import OllamaClient
from .gpt_translator import GPTTranslator 

@method_decorator(csrf_exempt, name='dispatch')
class ProcessFileView(APIView):
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            logger.info("ProcessFileView ìš”ì²­ ìˆ˜ì‹ : %s %s", request.method, request.path)
            
            # ìš”ì²­ ë°ì´í„° í™•ì¸
            if 'file' not in request.FILES:
                logger.error("íŒŒì¼ì´ ì œê³µë˜ì§€ ì•ŠìŒ")
                return Response({'error': 'íŒŒì¼ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}, status=status.HTTP_400_BAD_REQUEST)
            
            file_obj = request.FILES['file']
            file_name = file_obj.name.lower()
            logger.info("íŒŒì¼ ì—…ë¡œë“œ: %s, í¬ê¸°: %s bytes", file_name, file_obj.size)
            
            # Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            ollama_base_url = os.environ.get('OLLAMA_API_URL', 'http://localhost:11434')
            ollama_client = OllamaClient(base_url=ollama_base_url)
            
            # GPT ë²ˆì—­ê¸° ì´ˆê¸°í™”
            gpt_translator = GPTTranslator()
            
            # ë²ˆì—­ ì˜µì…˜ í™•ì¸ (ê¸°ë³¸ê°’: True)
            enable_translation = request.data.get('enable_translation', 'true').lower() == 'true'
            
            # íŒŒì¼ ìœ í˜• í™•ì¸
            if file_name.endswith(('.pdf')):
                file_type = 'pdf'
                
                # PDF í˜ì´ì§€ ë²”ìœ„ í™•ì¸
                start_page = int(request.data.get('start_page', 1))
                end_page = int(request.data.get('end_page', 0))  # 0ì€ ì „ì²´ í˜ì´ì§€ë¥¼ ì˜ë¯¸
                
                logger.info("PDF ì²˜ë¦¬ ë²”ìœ„: %s ~ %s í˜ì´ì§€", start_page, end_page if end_page > 0 else "ë")
                
            elif file_name.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif')):
                file_type = 'image'
            else:
                logger.error("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: %s", file_name)
                return Response({'error': 'ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ë‚˜ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.'},
                              status=status.HTTP_400_BAD_REQUEST)
            
            # OCR ê²°ê³¼ ê°ì²´ ìƒì„±
            ocr_result = OCRResult.objects.create(file=file_obj, file_type=file_type)
            logger.info("OCRResult ê°ì²´ ìƒì„±: %s", ocr_result.id)
            
            # OCR ì²˜ë¦¬
            try:
                ocr_text = ""
                page_texts = []  # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì €ì¥
                
                if file_type == 'image':
                    # ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬ - ê°œì„ ëœ OCR ì ìš©
                    img = Image.open(ocr_result.file.path)
                    # ì´ë¯¸ì§€ ì •ë³´ ë¡œê¹…
                    logger.info(f"ì´ë¯¸ì§€ ì •ë³´: í¬ê¸°={img.size}, ëª¨ë“œ={img.mode}, í¬ë§·={img.format}")
                    
                    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° OCR ìˆ˜í–‰ - OllamaClient ë©”ì„œë“œ ì‚¬ìš©
                    preprocessed_img = ollama_client.preprocess_image_for_ocr(img)
                    ocr_text = ollama_client.get_optimized_ocr_text(preprocessed_img, lang='kor+eng')
                    page_texts.append({"page": 1, "text": ocr_text})
                    logger.info("ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì™„ë£Œ, ì¶”ì¶œ í…ìŠ¤íŠ¸ ê¸¸ì´: %s", len(ocr_text))
                    logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
                
                elif file_type == 'pdf':
                    # PDF ì²˜ë¦¬ - ì§ì ‘ ì¶”ì¶œ í›„ í•„ìš”ì‹œ OCR
                    logger.info("PDF ì²˜ë¦¬ ì‹œì‘: %s", ocr_result.file.path)
                    
                    # ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„ (í˜ì´ì§€ë³„)
                    direct_extract_success = False
                    try:
                        all_page_texts = self.extract_text_from_pdf_by_pages(ocr_result.file.path)
                        
                        # í˜ì´ì§€ ë²”ìœ„ ì²˜ë¦¬
                        if start_page > 1 or (end_page > 0 and end_page < len(all_page_texts)):
                            if start_page <= len(all_page_texts):
                                if end_page > 0 and end_page >= start_page:
                                    page_texts = all_page_texts[start_page-1:end_page]
                                else:
                                    page_texts = all_page_texts[start_page-1:]
                            else:
                                page_texts = []
                        else:
                            page_texts = all_page_texts
                        
                        combined_text = "\n".join([page["text"] for page in page_texts])
                        
                        # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì¶©ë¶„í•œì§€ í™•ì¸ (ë” ì—„ê²©í•œ ì¡°ê±´)
                        if combined_text.strip() and len(combined_text.strip()) >= 50:
                            meaningful_chars = sum(1 for c in combined_text if c.isalnum())
                            if meaningful_chars > 30:  # ì˜ë¯¸ìˆëŠ” ê¸€ìê°€ 30ì ì´ìƒì´ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
                                ocr_text = combined_text
                                direct_extract_success = True
                                logger.info("PDF ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ, ì´ %s í˜ì´ì§€, í…ìŠ¤íŠ¸ ê¸¸ì´: %s", 
                                          len(page_texts), len(ocr_text))
                                logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
                    except Exception as e:
                        logger.error(f"PDF ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    
                    # ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì‹¤íŒ¨í•œ ê²½ìš°, OCR ì‹œë„
                    if not direct_extract_success:
                        logger.info("PDF OCR ì²˜ë¦¬ ì‹œì‘ (ì§ì ‘ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶ˆì¶©ë¶„)")
                        
                        # í˜ì´ì§€ ë²”ìœ„ ì„¤ì •ìœ¼ë¡œ OCR
                        all_page_texts = self.ocr_pdf_by_pages(ocr_result.file.path, ollama_client, start_page, end_page)
                        
                        # í˜ì´ì§€ ë²”ìœ„ ì²˜ë¦¬ - ocr_pdf_by_pagesì—ì„œ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ì „ì²´ ì‚¬ìš©
                        page_texts = all_page_texts
                        
                        ocr_text = "\n".join([page["text"] for page in page_texts])
                        logger.info("PDF OCR ì²˜ë¦¬ ì™„ë£Œ, ì´ %s í˜ì´ì§€, í…ìŠ¤íŠ¸ ê¸¸ì´: %s", 
                                  len(page_texts), len(ocr_text))
                        logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
                
                # í…ìŠ¤íŠ¸ ì •í™” - ê°œì„ ëœ í•¨ìˆ˜ ì‚¬ìš©
                ocr_result.ocr_text = self.clean_text(ocr_text)
                
                # PDF íŒŒì¼ì€ í•­ìƒ í…ìŠ¤íŠ¸ ê´€ë ¨ ìˆìŒìœ¼ë¡œ ì„¤ì •
                if file_type == 'pdf':
                    text_relevant = True
                
                # ë¶„ì„ ìœ í˜• í™•ì¸ (ê¸°ë³¸ê°’: both)
                analysis_type = request.data.get('analysis_type', 'both')
                logger.info("ë¶„ì„ ìœ í˜•: %s", analysis_type)
                
                # ê²°ê³¼ ë³€ìˆ˜ ì´ˆê¸°í™”
                image_analysis = ""
                text_analysis = ""
                combined_analysis = ""
                
                # ë²ˆì—­ ê²°ê³¼ ë³€ìˆ˜ ì´ˆê¸°í™”
                translated_analysis = ""
                translation_success = False
                translation_error = ""
                
                # í˜ì´ì§€ ë¶„í•  ë¶„ì„ ì—¬ë¶€ í™•ì¸
                analyze_by_page = request.data.get('analyze_by_page', 'true').lower() == 'true'
                
                # ì„ íƒëœ ë¶„ì„ ìœ í˜•ì— ë”°ë¼ ì²˜ë¦¬
                if analysis_type in ['ollama', 'both']:
                    # ì´ë¯¸ì§€ íŒŒì¼ì¸ ê²½ìš°
                    if file_type == 'image':
                        # ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ ì„¤ì • (ìš”ì•½ëœ ê°„ê²°í•œ ì„¤ëª…ì„ ìœ„í•´)
                        custom_prompt = f"""ì´ë¯¸ì§€ë¥¼ ê°ê´€ì ìœ¼ë¡œ ê´€ì°°í•˜ê³  ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ ì‘ë‹µí•˜ì„¸ìš”:

í•„ìˆ˜ í¬í•¨ ì‚¬í•­:
- ì´ë¯¸ì§€ì— ì‹¤ì œë¡œ ë³´ì´ëŠ” ì‚¬ëŒ, ë™ë¬¼, ë¬¼ì²´ë§Œ ì–¸ê¸‰ (ì—†ìœ¼ë©´ ì–¸ê¸‰í•˜ì§€ ì•ŠìŒ)
- ë§Œì•½ ë™ë¬¼ì´ë¼ë©´, ì–´ë–¤ ì¢…ì˜ ë™ë¬¼ì¸ì§€ë„ ì¶œë ¥
- í™•ì‹¤íˆ ë³´ì´ëŠ” ìƒ‰ìƒë§Œ ì–¸ê¸‰ (ë°°ê²½ìƒ‰, ì˜· ìƒ‰ìƒ ë“±)
- ëª…í™•íˆ ë³´ì´ëŠ” ìì„¸ë‚˜ ìœ„ì¹˜ ê´€ê³„ (ì •ë©´, ì¸¡ë©´ ë“±)

ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ:
- ì¶”ì¸¡ì´ë‚˜ í•´ì„ ("~ë¡œ ë³´ì…ë‹ˆë‹¤", "~ê°™ìŠµë‹ˆë‹¤" í‘œí˜„ ê¸ˆì§€)
- ë³´ì´ì§€ ì•ŠëŠ” ë¶€ë¶„ì— ëŒ€í•œ ì–¸ê¸‰ ("ë³´ì´ì§€ ì•ŠëŠ”ë‹¤", "ì—†ë‹¤" ë“±ì˜ í‘œí˜„ ê¸ˆì§€)
- ë°˜ë³µì ì¸ ì„¤ëª…
- ê°ì •ì´ë‚˜ ë¶„ìœ„ê¸° ë¬˜ì‚¬

í˜•ì‹:
- 1-2ë¬¸ì¥ìœ¼ë¡œ ë§¤ìš° ê°„ê²°í•˜ê²Œ ì‘ì„±
- ë‹¨ìˆœ ì‚¬ì‹¤ ë‚˜ì—´ í˜•ì‹ (ì˜ˆ: "ì´ë¯¸ì§€ì—ëŠ” ê²€ì€ ë¨¸ë¦¬ ì—¬ì„±ì´ ìˆê³ , ë°°ê²½ì€ í°ìƒ‰ì´ë‹¤.")

OCR í…ìŠ¤íŠ¸ (ì°¸ê³ ìš©, ì‹¤ì œ ì´ë¯¸ì§€ì— ë³´ì´ëŠ” ê²½ìš°ë§Œ ì–¸ê¸‰): {ocr_result.ocr_text}

ì˜ì–´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

                        
                        # OCR í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬ (analyze_image ë‚´ë¶€ì—ì„œ ê´€ë ¨ì„± íŒë‹¨)
                        image_analysis = ollama_client.analyze_image(
                            ocr_result.file.path, 
                            custom_prompt,
                            ocr_text=ocr_result.ocr_text
                        )
                        
                        # OCR í…ìŠ¤íŠ¸ ë¶„ì„ (í…ìŠ¤íŠ¸ê°€ ìˆê³  both ëª¨ë“œì¸ ê²½ìš°)
                        if ocr_result.ocr_text and analysis_type == 'both':
                            # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì •ë¦¬ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸
                            text_prompt = f"""ë‹¤ìŒ OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ê³  ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

{ocr_result.ocr_text}

ë¶„ì„ ì§€ì¹¨:
1. í…ìŠ¤íŠ¸ì˜ ì£¼ìš” ë‚´ìš©ê³¼ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì—¬ ì •ë¦¬
2. ë‹¨ìˆœ ìš”ì•½ì´ ì•„ë‹Œ, í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ì •ë³´ë¥¼ ì¶©ì‹¤í•˜ê²Œ ì •ë¦¬
3. ì¤‘ìš”í•œ ì„¸ë¶€ ì •ë³´ë¥¼ í¬í•¨
4. ë‚´ìš©ì´ ì´ë¯¸ì§€ì™€ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ì •ë¦¬

ë°˜ë“œì‹œ "ì˜ì–´(En)"ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
                            
                            try:
                                text_analysis = ollama_client.analyze_text(ocr_result.ocr_text, text_prompt)
                            except Exception as e:
                                logger.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                                text_analysis = f"í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                            
                            # ë‘ ë¶„ì„ ê²°ê³¼ ê²°í•©
                            combined_analysis = f"ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼:\n{image_analysis}\n\ní…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼:\n{text_analysis}"
                        else:
                            # OCR ì—†ì´ ì´ë¯¸ì§€ ë¶„ì„ë§Œ ìˆ˜í–‰
                            combined_analysis = image_analysis
                        
                    else:  # PDF íŒŒì¼ì¸ ê²½ìš°
                        if ocr_result.ocr_text:
                            if analyze_by_page and len(page_texts) > 1:
                                # ê°œì„ ëœ í˜ì´ì§€ë³„ ë¶„ì„ ìˆ˜í–‰ - OllamaClientì˜ ë¶„ì„ ê¸°ëŠ¥ í™œìš©
                                try:
                                    combined_analysis = ollama_client.analyze_text(ocr_result.ocr_text, None, page_texts)
                                    logger.info("í˜ì´ì§€ë³„ ë¶„ì„ ì™„ë£Œ")
                                except Exception as e:
                                    logger.error(f"í˜ì´ì§€ë³„ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                                    combined_analysis = f"í˜ì´ì§€ë³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                            else:
                                # ë¬¸ì„œ ì „ì²´ ë¶„ì„ - í˜ì´ì§€ë³„ êµ¬ì¡°í™” ìš”ì²­
                                text_prompt = f"""ë‹¤ìŒ PDFì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë³„ë¡œ ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

{ocr_result.ocr_text}

ë¶„ì„ ì§€ì¹¨:
1. í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë‚˜ ì„¹ì…˜ ë‹¨ìœ„ë¡œ êµ¬ë¶„í•˜ì—¬ ì •ë¦¬í•´ì£¼ì„¸ìš”.
2. ë‚´ìš©ì„ ìš”ì•½í•˜ì§€ ë§ê³ , ê° ì„¹ì…˜ì˜ í•µì‹¬ ì •ë³´ë¥¼ ì¶©ì‹¤í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
3. ëª¨ë“  ì¤‘ìš”í•œ ì„¸ë¶€ ì •ë³´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
4. ë‚´ìš©ì„ ë‹¨ìˆœ ìš”ì•½í•˜ì§€ ë§ê³ , êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
===== í˜ì´ì§€ 1 (ë˜ëŠ” ì„¹ì…˜ 1) =====
- ì£¼ìš” ë‚´ìš© ì •ë¦¬
- ì¤‘ìš” ê°œë… ì„¤ëª…
- í•µì‹¬ ì •ë³´ ë‚˜ì—´

===== í˜ì´ì§€ 2 (ë˜ëŠ” ì„¹ì…˜ 2) =====
- ì£¼ìš” ë‚´ìš© ì •ë¦¬
...

ë°˜ë“œì‹œ "ì˜ì–´ë¡œ" ì‘ë‹µí•´ì£¼ì„¸ìš”."""
                                
                                try:
                                    text_analysis = ollama_client.analyze_text(ocr_result.ocr_text, text_prompt)
                                    combined_analysis = text_analysis
                                except Exception as e:
                                    logger.error(f"ë¬¸ì„œ ì „ì²´ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                                    combined_analysis = f"ë¬¸ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\nOCR ê²°ê³¼: {ocr_result.ocr_text[:500]}..."
                    
                    logger.info("ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë¶„ì„ ì™„ë£Œ")
                
                # GPT ë²ˆì—­ ìˆ˜í–‰ (ë²ˆì—­ì´ í™œì„±í™”ëœ ê²½ìš°)
                if enable_translation and combined_analysis and gpt_translator.is_available:
                    logger.info("GPT ë²ˆì—­ ì‹œì‘")
                    try:
                        # ë¶„ì„ ìœ í˜•ì— ë”°ë¥¸ ë²ˆì—­
                        if file_type == 'pdf' and analyze_by_page and len(page_texts) > 1:
                            # í˜ì´ì§€ë³„ ë¶„ì„ ê²°ê³¼ ë²ˆì—­
                            translation_result = gpt_translator.translate_paged_analysis(combined_analysis)
                        else:
                            # ì¼ë°˜ ë¶„ì„ ê²°ê³¼ ë²ˆì—­
                            translation_result = gpt_translator.translate_analysis_result(combined_analysis, file_type)
                        
                        if translation_result and translation_result.get("success"):
                            translated_analysis = translation_result["translated_analysis"]
                            translation_success = True
                            logger.info("GPT ë²ˆì—­ ì„±ê³µ")
                        else:
                            error_msg = translation_result.get('error', 'Unknown error') if translation_result else 'No translation result'
                            logger.error(f"GPT ë²ˆì—­ ì‹¤íŒ¨: {error_msg}")
                            translated_analysis = f"ë²ˆì—­ ì‹¤íŒ¨: {error_msg}"
                            translation_error = error_msg
                            
                    except Exception as e:
                        logger.error(f"GPT ë²ˆì—­ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                        translated_analysis = f"ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                        translation_error = str(e)
                
                # ë²ˆì—­ ê´€ë ¨ ë©”íƒ€ë°ì´í„° ì €ì¥
                ocr_result.translation_enabled = enable_translation
                ocr_result.translation_success = translation_success
                ocr_result.analysis_type = analysis_type
                ocr_result.analyze_by_page = analyze_by_page
                
                # MySQL ì €ì¥ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì •í™”
                ocr_result.llm_response = self.clean_text(combined_analysis)
                
                # ë²ˆì—­ ê²°ê³¼ë„ ì €ì¥
                if enable_translation and translated_analysis:
                    if translation_success:
                        # ì„±ê³µí•œ ë²ˆì—­ ê²°ê³¼ ì €ì¥
                        ocr_result.llm_response_korean = self.clean_text(translated_analysis)
                        ocr_result.translation_model = gpt_translator.model if gpt_translator else "unknown"
                    else:
                        # ì‹¤íŒ¨í•œ ê²½ìš° ì˜¤ë¥˜ ë©”ì‹œì§€ ì €ì¥ (ë””ë²„ê¹…ìš©)
                        ocr_result.llm_response_korean = f"ë²ˆì—­ ì‹¤íŒ¨: {translation_error}"
                
                # í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ì •ë³´ ì €ì¥ - PDFëŠ” í•­ìƒ True, ì´ë¯¸ì§€ëŠ” ë¶„ì„ ê³¼ì •ì—ì„œ ê²°ì •
                if file_type == 'pdf':
                    ocr_result.text_relevant = True
                
            except Exception as e:
                logger.error("ì²˜ë¦¬ ì‹¤íŒ¨: %s", str(e), exc_info=True)
                return Response({'error': f'ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}'}, 
                               status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # ê²°ê³¼ ì €ì¥
            try:
                ocr_result.save()
                logger.info("OCRResult ì €ì¥ ì™„ë£Œ (ID: %s)", ocr_result.id)
            except Exception as e:
                logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                return Response({'error': f'ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}'}, 
                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # ì‘ë‹µ ë°ì´í„° êµ¬ì„± - ëª…ì‹œì ìœ¼ë¡œ í•„ë“œ ì§€ì •
            try:
                # ê¸°ë³¸ ì‹œë¦¬ì–¼ë¼ì´ì € ë°ì´í„°
                response_data = OCRResultSerializer(ocr_result).data
                
                # ë²ˆì—­ ê´€ë ¨ ì •ë³´ ëª…ì‹œì  ì¶”ê°€
                response_data['translation_enabled'] = enable_translation
                response_data['translation_success'] = translation_success
                
                # ì˜ì–´ ì›ë¬¸ê³¼ í•œêµ­ì–´ ë²ˆì—­ì„ ëª…í™•íˆ êµ¬ë¶„
                response_data['llm_response'] = ocr_result.llm_response  # ì˜ì–´ ì›ë¬¸
                
                if enable_translation and translation_success:
                    # ë²ˆì—­ ì„±ê³µ ì‹œ í•œêµ­ì–´ ë²ˆì—­ ì¶”ê°€
                    response_data['llm_response_korean'] = ocr_result.llm_response_korean
                    logger.info("ì‘ë‹µì— í•œêµ­ì–´ ë²ˆì—­ í¬í•¨")
                elif enable_translation and not translation_success:
                    # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ì •ë³´ ì¶”ê°€
                    response_data['llm_response_korean'] = None
                    response_data['translation_error'] = translation_error if translation_error else "ë²ˆì—­ ì‹¤íŒ¨"
                    logger.info("ë²ˆì—­ ì‹¤íŒ¨ - ì˜ì–´ ì›ë¬¸ë§Œ í¬í•¨")
                else:
                    # ë²ˆì—­ ë¹„í™œì„±í™” ì‹œ
                    response_data['llm_response_korean'] = None
                    logger.info("ë²ˆì—­ ë¹„í™œì„±í™” - ì˜ì–´ ì›ë¬¸ë§Œ í¬í•¨")
                
                # ë””ë²„ê¹…ìš© ë¡œê·¸
                logger.info(f"ì‘ë‹µ ë°ì´í„° êµ¬ì„± ì™„ë£Œ:")
                logger.info(f"  - ì˜ì–´ ì›ë¬¸ ê¸¸ì´: {len(response_data.get('llm_response', ''))}")
                logger.info(f"  - í•œêµ­ì–´ ë²ˆì—­ ê¸¸ì´: {len(response_data.get('llm_response_korean', '') or '')}")
                logger.info(f"  - ë²ˆì—­ ì„±ê³µ: {response_data.get('translation_success', False)}")
                
            except Exception as e:
                logger.error(f"ì‘ë‹µ ë°ì´í„° êµ¬ì„± ì‹¤íŒ¨: {str(e)}")
                return Response({'error': f'ì‘ë‹µ êµ¬ì„± ì‹¤íŒ¨: {str(e)}'}, 
                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # ì‘ë‹µ ë°˜í™˜
            return Response(response_data, status=status.HTTP_201_CREATED)
                
        except Exception as e:
            logger.error("ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: %s", str(e), exc_info=True)
            return Response({'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'}, 
                           status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def extract_text_from_pdf_by_pages(self, pdf_path):
        """PDFì—ì„œ ì§ì ‘ í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë³„ë¡œ ì¶”ì¶œ"""
        pages = []
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                total_pages = len(reader.pages)
                
                for i in range(total_pages):
                    page = reader.pages[i]
                    text = page.extract_text()
                    pages.append({"page": i + 1, "text": text})
                    
            return pages
        except Exception as e:
            logger.error(f"PDF í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def ocr_pdf_by_pages(self, pdf_path, ollama_client, start_page=1, end_page=0):
        """PDFë¥¼ OCRë¡œ ì²˜ë¦¬í•˜ì—¬ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        pages = []
        
        try:
            # PDF2Imageë¡œ ì´ë¯¸ì§€ ë³€í™˜
            with tempfile.TemporaryDirectory() as path:
                # í˜ì´ì§€ ë²ˆí˜¸ëŠ” 1ë¶€í„° ì‹œì‘í•˜ì§€ë§Œ, convert_from_pathëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ ì¡°ì •
                first_page = start_page
                last_page = None if end_page <= 0 else end_page
                
                images = convert_from_path(
                    pdf_path, 
                    dpi=300, 
                    output_folder=path, 
                    first_page=first_page,
                    last_page=last_page
                )
                
                # ê° í˜ì´ì§€ ì´ë¯¸ì§€ OCR ì²˜ë¦¬
                for i, image in enumerate(images):
                    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                    preprocessed_img = ollama_client.preprocess_image_for_ocr(image)
                    # OCR ìˆ˜í–‰
                    text = ollama_client.get_optimized_ocr_text(preprocessed_img, lang='kor+eng')
                    
                    # í˜ì´ì§€ ë²ˆí˜¸ ê³„ì‚° (ì‹œì‘ í˜ì´ì§€ ê³ ë ¤)
                    page_num = start_page + i
                    pages.append({"page": page_num, "text": text})
                    
            return pages
        except Exception as e:
            logger.error(f"PDF OCR ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •í™” í•¨ìˆ˜"""
        if not text:
            return ""
            
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì œê±°
        text = re.sub(r'\n\s*\n', '\n\n', text)
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text