from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
import logging
import json
import openai
import anthropic
from groq import Groq
from django.conf import settings
from bs4 import BeautifulSoup
logger = logging.getLogger(__name__)
import re
import json
import logging
import os
from dotenv import load_dotenv

def fetch_and_clean_url(url, timeout=10):
    """
    Ï£ºÏñ¥ÏßÑ URLÏùò HTMLÏùÑ ÏöîÏ≤≠Ìï¥, Ïä§ÌÅ¨Î¶ΩÌä∏¬∑Ïä§ÌÉÄÏùº Ï†úÍ±∞ ÌõÑ ÌÖçÏä§Ìä∏Îßå Î∞òÌôòÌï©ÎãàÎã§.
    """
    resp = requests.get(url, timeout=timeout)
    resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "html.parser")
    # Ïä§ÌÅ¨Î¶ΩÌä∏¬∑Ïä§ÌÉÄÏùº¬∑ÎÑ§ÎπÑÍ≤åÏù¥ÏÖò ÌÉúÍ∑∏ Ï†úÍ±∞
    for tag in soup(["script", "style", "header", "footer", "nav"]):
        tag.decompose()

    text = soup.get_text(separator="\n")
    # Îπà Ï§Ñ Ï†úÍ±∞
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    return "\n".join(lines)

# Add this function to your ChatBot class or as a standalone function
def sanitize_and_parse_json(text, selected_models, responses):
    """
    Sanitize and parse the JSON response from AI models.
    Handles various edge cases and formatting issues.
    """
    import re
    import json
    import logging
    
    logger = logging.getLogger(__name__)
    
    try:
        # Step 1: Basic cleanup
        text = text.strip()
        
        # Step 2: Handle code blocks
        if text.startswith('```json') and '```' in text:
            text = re.sub(r'```json(.*?)```', r'\1', text, flags=re.DOTALL).strip()
        elif text.startswith('```') and text.endswith('```'):
            text = text[3:-3].strip()
            
        # Step 3: Extract JSON object if embedded in other text
        json_pattern = r'({[\s\S]*})'
        json_matches = re.findall(json_pattern, text)
        if json_matches:
            text = json_matches[0]
            
        # Step 4: Handle escaped backslashes in the text
        # First identify all occurrences of escaped backslashes followed by characters like "_"
        text = re.sub(r'\\([_"])', r'\1', text)
        
        # Step 5: Attempt to parse the JSON
        result = json.loads(text)
        
        # Ensure the required fields exist
        required_fields = ["preferredModel", "best_response", "analysis", "reasoning"]
        for field in required_fields:
            if field not in result:
                if field == "best_response" and "bestResponse" in result:
                    result["best_response"] = result["bestResponse"]
                else:
                    result[field] = "" if field != "analysis" else {}
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå JSON ÌååÏã± Ïã§Ìå®: {str(e)}")
        logger.error(f"ÏõêÎ≥∏ ÌÖçÏä§Ìä∏: {text[:200]}..." if len(text) > 200 else text)
        
        # Advanced recovery attempt for malformed JSON
        try:
            # Remove problematic escaped characters
            fixed_text = text.replace("\\_", "_").replace('\\"', '"')
            
            # Try to fix common issues with JSON (missing quotes, commas, etc.)
            fixed_text = re.sub(r'([{,])\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', fixed_text)
            fixed_text = re.sub(r':\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*([,}])', r':"\1"\2', fixed_text)
            
            # Handle unclosed strings
            for match in re.finditer(r':\s*"([^"\\]*(\\.[^"\\]*)*)', fixed_text):
                if not re.search(r':\s*"([^"\\]*(\\.[^"\\]*)*)"', match.group(0)):
                    pos = match.end()
                    fixed_text = fixed_text[:pos] + '"' + fixed_text[pos:]
            
            result = json.loads(fixed_text)
            logger.info("‚úÖ Recovered JSON after fixing format issues")
            return result
        except:
            # Last resort: construct a sensible fallback response
            error_analysis = {}
            for model in selected_models:
                model_lower = model.lower()
                error_analysis[model_lower] = {"Ïû•Ï†ê": "Î∂ÑÏÑù Ïã§Ìå®", "Îã®Ï†ê": "Î∂ÑÏÑù Ïã§Ìå®"}
            
            # Find the largest response to use as best_response
            best_response = ""
            if responses:
                best_response = max(responses.values(), key=len) 
            
            return {
                "preferredModel": "FALLBACK",
                "best_response": best_response,
                "analysis": error_analysis,
                "reasoning": "ÏùëÎãµ Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌïòÏó¨ ÏµúÏ†ÅÏùò ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±ÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§."
            }

# src/chatbot_backend/chat/bot.py

# import logging
# import openai
# import anthropic
# import base64
# import imghdr
# from groq import Groq
# from io import BytesIO

# logger = logging.getLogger(__name__)

# class ChatBot:
#     def __init__(self, api_key, model, api_type):
#         self.conversation_history = []
#         self.api_type = api_type
#         self.api_key = api_key

#         # Anthropic Î©ÄÌã∞Î™®Îã¨ÏùÄ Opus Î™®Îç∏ Í∂åÏû•
#         if api_type == 'anthropic' and not model.startswith('claude-3-opus-20240229'):
#             logger.info(f"Overriding Anthropic model '{model}' to 'claude-3-opus-20240229' for image support")
#             self.model = 'claude-3-opus-20240229'
#         else:
#             self.model = model

#         if api_type == 'openai':
#             openai.api_key = api_key
#         elif api_type == 'anthropic':
#             # Anthropic Python SDK Ï¥àÍ∏∞Ìôî
#             self.client = anthropic.Client(api_key=api_key)
#         elif api_type == 'groq':
#             self.client = Groq(api_key=api_key)
#         else:
#             raise ValueError(f"Unsupported api_type: {api_type}")

#     def chat(self, prompt=None, user_input=None, image_file=None, analysis_mode=None, user_language=None):
#         """
#         prompt       : ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ (ÌÇ§ÏõåÎìú)
#         user_input   : ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ (ÏúÑÏπò Ïù∏Ïûê)
#         image_file   : ÌååÏùº Í∞ùÏ≤¥ (BytesIO, InMemoryUploadedFile Îì±)
#         analysis_mode: 'describe'|'ocr'|'objects'
#         user_language: 'ko','en'
#         """
#         text = prompt if prompt is not None else user_input
#         try:
#             logger.info(f"[{self.api_type}] Received input: {text}")

#             # Î™®Îç∏Î≥Ñ Ìò∏Ï∂ú
#             if self.api_type == 'openai':
#                 # GPT-4 Vision ÏßÄÏõê
#                 params = {
#                     'model': self.model,
#                     'messages': self.conversation_history + [{"role": "user", "content": text}],
#                     'temperature': 0.7,
#                     'max_tokens': 1024
#                 }
#                 if image_file:
#                     params['files'] = [("image", image_file)]
#                 resp = openai.ChatCompletion.create(**params)
#                 assistant_response = resp.choices[0].message.content

#             elif self.api_type == 'anthropic':
#                 # Claude 3 Opus: Ïù¥ÎØ∏ÏßÄ+ÌÖçÏä§Ìä∏ ÏßÄÏõê via Messages API
#                 messages = []
#                 # ÌÜ†ÌÅ∞ Ïàò ÏÑ§Ï†ï
#                 max_tokens = 1024 if image_file else 4096
#                 if image_file:
#                     # Ïù¥ÎØ∏ÏßÄ Î∞îÏù¥ÎÑàÎ¶¨ ÏùΩÍ∏∞ Î∞è ÎØ∏ÎîîÏñ¥ ÌÉÄÏûÖ ÏûêÎèô Í∞êÏßÄ
#                     image_file.seek(0)
#                     data_bytes = image_file.read()
#                     ext = imghdr.what(None, h=data_bytes) or 'jpeg'
#                     mime_map = {
#                         'jpeg': 'image/jpeg', 'jpg': 'image/jpeg',
#                         'png': 'image/png', 'gif': 'image/gif',
#                         'bmp': 'image/bmp', 'webp': 'image/webp'
#                     }
#                     media_type = mime_map.get(ext, 'image/jpeg')
#                     b64 = base64.b64encode(data_bytes).decode('utf-8')

#                     # Ïù¥ÎØ∏ÏßÄ Î∏îÎ°ùÍ≥º ÌÖçÏä§Ìä∏ Î∏îÎ°ùÏùÑ Î¶¨Ïä§Ìä∏Î°ú Íµ¨ÏÑ±
#                     image_block = {
#                         'type': 'image',
#                         'source': {'type': 'base64', 'media_type': media_type, 'data': b64}
#                     }
#                     text_block = {'type': 'text', 'text': text}
#                     content_blocks = [image_block, text_block]

#                     # Îã®Ïùº Î©îÏãúÏßÄÏóê Î∏îÎ°ù Î¶¨Ïä§Ìä∏ Ï†ÑÎã¨
#                     messages.append({'role': 'user', 'content': content_blocks})
#                 else:
#                     # ÌÖçÏä§Ìä∏ Ï†ÑÏö© Î©îÏãúÏßÄ
#                     messages.append({'role': 'user', 'content': [{'type': 'text', 'text': text}]})

#                 # Messages API Ìò∏Ï∂ú
#                 resp = self.client.messages.create(
#                     model=self.model,
#                     messages=messages,
#                     max_tokens=max_tokens
#                 )
#                 # ÏùëÎãµ Î∏îÎ°ùÏóêÏÑú ÌÖçÏä§Ìä∏Îßå Ìï©ÏπòÍ∏∞
#                 assistant_response = ' '.join(getattr(block, 'text', '') for block in resp.content)

#             elif self.api_type == 'groq':
#                 # Groq Chat API
#                 resp = self.client.chat.completions.create(
#                     model=self.model,
#                     messages=self.conversation_history + [{"role": "user", "content": text}],
#                     temperature=0.7,
#                     max_tokens=1024
#                 )
#                 assistant_response = resp.choices[0].message.content

#             else:
#                 raise ValueError(f"Unsupported api_type: {self.api_type}")

#             # ÏùëÎãµ Í∏∞Î°ù Î∞è Î∞òÌôò
#             self.conversation_history.append({"role": "assistant", "content": assistant_response})
#             logger.info(f"[{self.api_type}] Response: {assistant_response[:100]}...")
#             return assistant_response

#         except Exception as e:
#             logger.error(f"Error in chat method ({self.api_type}): {e}", exc_info=True)
#             raise


# paste-2.txt ÏàòÏ†ïÎêú ÎÇ¥Ïö©

# chatbot.py - OpenAI v1.0+ Ìò∏Ìôò Î≤ÑÏ†Ñ
import openai
import anthropic
from groq import Groq
import logging
import json
import asyncio
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)

# sanitize_and_parse_json Ìï®Ïàò (Í∏∞Ï°¥ Ìï®Ïàò Ìè¨Ìï®)
def sanitize_and_parse_json(text, selected_models=None, responses=None):
    """JSON ÏùëÎãµÏùÑ Ï†ïÎ¶¨ÌïòÍ≥† ÌååÏã±ÌïòÎäî Ìï®Ïàò"""
    import re
    try:
        text = text.strip()
        
        # ÏΩîÎìú Î∏îÎ°ù Ï†úÍ±∞
        if text.startswith('```json') and '```' in text:
            text = re.sub(r'```json(.*?)```', r'\1', text, flags=re.DOTALL).strip()
        elif text.startswith('```') and text.endswith('```'):
            text = text[3:-3].strip()
        
        # JSON Ìå®ÌÑ¥ Ï∂îÏ∂ú
        json_pattern = r'({[\s\S]*})'
        json_matches = re.findall(json_pattern, text)
        if json_matches:
            text = json_matches[0]
        
        # Ïù¥Ïä§ÏºÄÏù¥ÌîÑ Î¨∏Ïûê Ï≤òÎ¶¨
        text = re.sub(r'\\([_"])', r'\1', text)
        
        # JSON ÌååÏã±
        result = json.loads(text)
        
        # ÌïÑÏàò ÌïÑÎìú ÌôïÏù∏ Î∞è Î≥¥Ï†ï
        required_fields = ["preferredModel", "best_response", "analysis", "reasoning"]
        for field in required_fields:
            if field not in result:
                if field == "best_response" and "bestResponse" in result:
                    result["best_response"] = result["bestResponse"]
                else:
                    result[field] = "" if field != "analysis" else {}
        
        return result
        
    except Exception as e:
        logger.error(f"JSON ÌååÏã± Ïã§Ìå®: {e}")
        # Ìè¥Î∞± ÏùëÎãµ ÏÉùÏÑ±
        error_analysis = {}
        if selected_models:
            for model in selected_models:
                model_lower = model.lower()
                error_analysis[model_lower] = {"Ïû•Ï†ê": "Î∂ÑÏÑù Ïã§Ìå®", "Îã®Ï†ê": "Î∂ÑÏÑù Ïã§Ìå®"}
        
        return {
            "preferredModel": "ERROR",
            "best_response": max(responses.values(), key=len) if responses else "Î∂ÑÏÑù Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.",
            "analysis": error_analysis,
            "reasoning": "ÏùëÎãµ Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."
        }
import openai
import os 
import anthropic
from groq import Groq
import logging
from .langchain_config import LangChainManager

logger = logging.getLogger(__name__)

class ChatBot:
    def __init__(self, api_key, model, api_type, langchain_manager=None):
        self.conversation_history = []
        self.model = model
        self.api_type = api_type
        self.api_key = api_key
        self.langchain_manager = langchain_manager
        
        # LangChain ÏÇ¨Ïö© Ïó¨Î∂Ä Í≤∞Ï†ï
        self.use_langchain = langchain_manager is not None
        
        if not self.use_langchain:
            # Í∏∞Ï°¥ Î∞©Ïãù Ï¥àÍ∏∞Ìôî
            if api_type == 'openai':
                openai.api_key = api_key
            elif api_type == 'anthropic':
                self.client = anthropic.Anthropic(api_key=api_key)
            elif api_type == 'groq':
                self.client = Groq(api_key=api_key)
        else:
            # LangChain Ï≤¥Ïù∏ ÏÉùÏÑ±
            try:
                if api_type in ['gpt', 'claude']:
                    self.chat_chain = langchain_manager.create_chat_chain(api_type)
                elif api_type == 'groq' or api_type == 'mixtral':
                    # GroqÎäî Î≥ÑÎèÑ Ï≤òÎ¶¨
                    self.groq_llm = langchain_manager.groq_llm if hasattr(langchain_manager, 'groq_llm') else None
                logger.info(f"LangChain Ï≤¥Ïù∏ ÏÉùÏÑ± ÏôÑÎ£å: {api_type}")
            except Exception as e:
                logger.warning(f"LangChain Ï≤¥Ïù∏ ÏÉùÏÑ± Ïã§Ìå®, Í∏∞Ï°¥ Î∞©Ïãù ÏÇ¨Ïö©: {e}")
                self.use_langchain = False
   
    async def chat_async(self, user_input, image_file=None, analysis_mode=None, user_language=None):
        """ÎπÑÎèôÍ∏∞ Ï±ÑÌåÖ Î©îÏÑúÎìú (LangChain Ïö©)"""
        if self.use_langchain:
            return await self._chat_with_langchain(user_input, user_language)
        else:
            return self.chat(user_input, image_file, analysis_mode, user_language)
    
    async def _chat_with_langchain(self, user_input, user_language='ko'):
        """LangChainÏùÑ ÏÇ¨Ïö©Ìïú Ï±ÑÌåÖ"""
        try:
            if self.api_type in ['gpt', 'claude']:
                result = await self.chat_chain.arun(
                    user_input=user_input,
                    user_language=user_language
                )
                return result
            elif self.api_type == 'groq' or self.api_type == 'mixtral':
                if self.groq_llm:
                    prompt = f"ÏÇ¨Ïö©ÏûêÍ∞Ä ÏÑ†ÌÉùÌïú Ïñ∏Ïñ¥Îäî '{user_language}'ÏûÖÎãàÎã§. Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú ÏùëÎãµÌïòÏÑ∏Ïöî.\n\n{user_input}"
                    result = self.groq_llm(prompt)
                    return result
                else:
                    # Ìè¥Î∞±: Í∏∞Ï°¥ Î∞©Ïãù
                    return self.chat(user_input, user_language=user_language)
            else:
                raise ValueError(f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî API ÌÉÄÏûÖ: {self.api_type}")
                
        except Exception as e:
            logger.error(f"LangChain Ï±ÑÌåÖ ÏóêÎü¨: {e}")
            # Ìè¥Î∞±: Í∏∞Ï°¥ Î∞©Ïãù
            return self.chat(user_input, user_language=user_language)

    def chat(self, user_input, image_file=None, analysis_mode=None, user_language=None):
        """Í∏∞Ï°¥ ÎèôÍ∏∞ Ï±ÑÌåÖ Î©îÏÑúÎìú (Ìò∏ÌôòÏÑ± Ïú†ÏßÄ)"""
        try:
            logger.info(f"Processing chat request for {self.api_type}")
            logger.info(f"User input: {user_input}")
            
            # ÎåÄÌôî Í∏∞Î°ùÏóê ÏÇ¨Ïö©Ïûê ÏûÖÎ†• Ï∂îÍ∞Ä
            if image_file:
                self.conversation_history = [{
                    "role": "system",
                    "content": f"Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Î™®Îìú: {analysis_mode}, ÏùëÎãµ Ïñ∏Ïñ¥: {user_language}"
                }]
                messages = [
                    {"role": "user", "content": user_input}
                ]
            else:
                self.conversation_history.append({"role": "user", "content": user_input})
                messages = self.conversation_history

            try:
                if self.api_type == 'openai':
                    response = openai.ChatCompletion.create(
                        model=self.model,
                        messages=self.conversation_history,
                        temperature=0.7,
                        max_tokens=1024
                    )
                    assistant_response = response['choices'][0]['message']['content']
                    
                elif self.api_type == 'anthropic':
                    try:
                        # ÏãúÏä§ÌÖú Î©îÏãúÏßÄ Ï∞æÍ∏∞
                        system_message = next((msg['content'] for msg in self.conversation_history 
                                            if msg['role'] == 'system'), '')
                        
                        # ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄ Ï∞æÍ∏∞
                        user_content = next((msg['content'] for msg in self.conversation_history 
                                        if msg['role'] == 'user'), '')

                        message = self.client.messages.create(
                            model=self.model,
                            max_tokens=4096,
                            temperature=0,
                            system=system_message,
                            messages=[{
                                "role": "user",
                                "content": user_content
                            }]
                        )
                        assistant_response = message.content[0].text
                        logger.info(f"Anthropic response with system message: {system_message[:100]}")
                        
                    except Exception as e:
                        logger.error(f"Anthropic API error: {str(e)}")
                        raise
               
                elif self.api_type == 'groq':
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=self.conversation_history,
                        temperature=0.7,
                        max_tokens=1024
                    )
                    assistant_response = response.choices[0].message.content
               
                # ÏùëÎãµ Í∏∞Î°ù
                self.conversation_history.append({
                    "role": "assistant",
                    "content": assistant_response
                })
               
                logger.info(f"Generated response: {assistant_response[:100]}...")
                return assistant_response
               
            except Exception as e:
                logger.error(f"API error in {self.api_type}: {str(e)}", exc_info=True)
                raise
               
        except Exception as e:
            logger.error(f"Error in chat method: {str(e)}", exc_info=True)
            raise

    async def analyze_responses_async(self, responses, query, user_language, selected_models):
        """ÎπÑÎèôÍ∏∞ ÏùëÎãµ Î∂ÑÏÑù (LangChain Ïö©)"""
        if self.use_langchain and self.langchain_manager:
            return await self._analyze_with_langchain(responses, query, user_language, selected_models)
        else:
            return self.analyze_responses(responses, query, user_language, selected_models)
    
    async def _analyze_with_langchain(self, responses, query, user_language, selected_models):
        """LangChainÏùÑ ÏÇ¨Ïö©Ìïú ÏùëÎãµ Î∂ÑÏÑù"""
        try:
            logger.info("\n" + "="*100)
            logger.info("üìä LangChain Î∂ÑÏÑù ÏãúÏûë")
            logger.info(f"ü§ñ Î∂ÑÏÑù ÏàòÌñâ AI: {self.api_type.upper()}")
            logger.info(f"üîç ÏÑ†ÌÉùÎêú Î™®Îç∏Îì§: {', '.join(selected_models)}")
            logger.info("="*100)
            
            # Î∂ÑÏÑù Ï≤¥Ïù∏ ÏÉùÏÑ±
            analysis_chain = self.langchain_manager.create_analysis_chain(self.api_type)
            
            # ÏùëÎãµ Ìè¨Îß∑ÌåÖ
            formatted = self.langchain_manager.format_responses_for_analysis(
                responses, selected_models
            )
            
            # Î∂ÑÏÑù Ïã§Ìñâ
            analysis_result = await analysis_chain.arun(
                query=query,
                user_language=user_language,
                selected_models=selected_models,
                **formatted
            )
            
            # preferredModel ÏÑ§Ï†ï
            analysis_result['preferredModel'] = self.api_type.upper()
            
            logger.info("‚úÖ LangChain Î∂ÑÏÑù ÏôÑÎ£å\n")
            return analysis_result
            
        except Exception as e:
            logger.error(f"‚ùå LangChain Î∂ÑÏÑù ÏóêÎü¨: {str(e)}")
            # Ìè¥Î∞±: Í∏∞Ï°¥ Î∞©Ïãù
            return self.analyze_responses(responses, query, user_language, selected_models)

    def analyze_responses(self, responses, query, user_language, selected_models):
        """Í∏∞Ï°¥ ÎèôÍ∏∞ ÏùëÎãµ Î∂ÑÏÑù Î©îÏÑúÎìú (Ìò∏ÌôòÏÑ± Ïú†ÏßÄ)"""
        try:
            logger.info("\n" + "="*100)
            logger.info("üìä Î∂ÑÏÑù ÏãúÏûë")
            logger.info(f"ü§ñ Î∂ÑÏÑù ÏàòÌñâ AI: {self.api_type.upper()}")
            logger.info(f"üîç ÏÑ†ÌÉùÎêú Î™®Îç∏Îì§: {', '.join(selected_models)}")
            logger.info("="*100)

            # ÏÑ†ÌÉùÎêú Î™®Îç∏Îì§Îßå Î∂ÑÏÑùÏóê Ìè¨Ìï®
            responses_section = ""
            analysis_section = ""
            
            for model in selected_models:
                model_lower = model.lower()
                responses_section += f"\n{model.upper()} ÏùëÎãµ: Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú ÏûëÏÑ± {responses.get(model_lower, 'ÏùëÎãµ ÏóÜÏùå')}"
                
                analysis_section += f"""
                        "{model_lower}": {{
                            "Ïû•Ï†ê": "Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú ÏûëÏÑ± {model.upper()} ÎãµÎ≥ÄÏùò Ïû•Ï†ê",
                            "Îã®Ï†ê": "Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú ÏûëÏÑ± {model.upper()} ÎãµÎ≥ÄÏùò Îã®Ï†ê"
                        }}{"," if model_lower != selected_models[-1].lower() else ""}"""

            # Í∏∞Ï°¥ Î∂ÑÏÑù ÌîÑÎ°¨ÌîÑÌä∏ (Î≥ÄÍ≤Ω ÏóÜÏùå)
            analysis_prompt = f"""Îã§ÏùåÏùÄ ÎèôÏùºÌïú ÏßàÎ¨∏Ïóê ÎåÄÌïú {len(selected_models)}Í∞ÄÏßÄ AIÏùò ÏùëÎãµÏùÑ Î∂ÑÏÑùÌïòÎäî Í≤ÉÏûÖÎãàÎã§.
                    ÏÇ¨Ïö©ÏûêÍ∞Ä ÏÑ†ÌÉùÌïú Ïñ∏Ïñ¥Îäî '{user_language}'ÏûÖÎãàÎã§.
                    Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú ÏµúÏ†ÅÏùò ÎãµÏùÑ ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
                    Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú Ïû•Ï†êÏùÑ ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
                    Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú Îã®Ï†êÏùÑ ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
                    Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú Î∂ÑÏÑù Í∑ºÍ±∞Î•º ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.

                    ÏßàÎ¨∏: {query}
                    {responses_section}

                     [ÏµúÏ†ÅÏùò ÏùëÎãµÏùÑ ÎßåÎì§ Îïå Í≥†Î†§Ìï† ÏÇ¨Ìï≠]
                    - Î™®Îì† AIÏùò ÎãµÎ≥ÄÎì§ÏùÑ Ï¢ÖÌï©ÌïòÏó¨ ÏµúÏ†ÅÏùò ÎãµÎ≥ÄÏúºÎ°ú Î∞òÎìúÏãú Ïû¨Íµ¨ÏÑ±Ìï©ÎãàÎã§
                    - Í∏∞Ï°¥ AIÏùò ÎãµÎ≥ÄÏùÑ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©ÌïòÎ©¥ ÏïàÎê©ÎãàÎã§
                    - Ï¶â, Í∏∞Ï°¥ AIÏùò ÎãµÎ≥ÄÍ≥º ÏµúÏ†ÅÏùò ÎãµÎ≥ÄÏù¥ ÎèôÏùºÌïòÎ©¥ ÏïàÎê©ÎãàÎã§.
                    - Îã§ÏàòÏùò AIÍ∞Ä Í≥µÌÜµÏúºÎ°ú Ï†úÍ≥µÌïú Ï†ïÎ≥¥Îäî Í∞ÄÏû• Ïã†Î¢∞Ìï† Ïàò ÏûàÎäî Ïò¨Î∞îÎ•∏ Ï†ïÎ≥¥Î°ú Í∞ÑÏ£ºÌï©ÎãàÎã§
                    - ÏΩîÎìúÎ•º Î¨ªÎäî ÏßàÎ¨∏ÏùºÎïåÎäî, AIÏùò ÎãµÎ≥Ä Ï§ë Ï†úÏùº Ï¢ãÏùÄ ÎãµÎ≥ÄÏùÑ ÏÑ†ÌÉùÌï¥ÏÑú Ïû¨Íµ¨ÏÑ±Ìï¥Ï§ò
                    - Î∞òÎìúÏãú JSON ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî
                    [Ï∂úÎ†• ÌòïÏãù]
                    {{
                        "preferredModel": "{self.api_type.upper()}",
                        "best_response": "ÏµúÏ†ÅÏùò ÎãµÎ≥Ä ({user_language}Î°ú ÏûëÏÑ±)",
                        "analysis": {{
                            {analysis_section}
                        }},
                        "reasoning": "Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú ÏûëÏÑ± ÏµúÏ†ÅÏùò ÏùëÎãµÏùÑ ÏÑ†ÌÉùÌïú Ïù¥Ïú†"
                    }}"""

            # Í∏∞Ï°¥ API Ìò∏Ï∂ú Î°úÏßÅ (Î≥ÄÍ≤Ω ÏóÜÏùå)
            if self.api_type == 'openai':
                response = openai.ChatCompletion.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant. Always respond with valid JSON ONLY, no additional text or explanations."},
                        {"role": "user", "content": analysis_prompt}
                    ],
                    temperature=0,
                    max_tokens=4096
                )
                analysis_text = response['choices'][0]['message']['content']
                
            elif self.api_type == 'anthropic':
                system_message = next((msg['content'] for msg in self.conversation_history 
                                    if msg['role'] == 'system'), '')
                
                message = self.client.messages.create(
                    model=self.model,
                    max_tokens=4096,
                    temperature=0,
                    system=f"{system_message}\nYou must respond with valid JSON only in the specified language. No other text or formatting.",
                    messages=[{
                        "role": "user", 
                        "content": analysis_prompt
                    }]
                )
                analysis_text = message.content[0].text.strip()
            
            elif self.api_type == 'groq':
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant. Always respond with valid JSON ONLY, no additional text or explanations."},
                        {"role": "user", "content": analysis_prompt}
                    ],
                    temperature=0,
                    max_tokens=4096
                )
                analysis_text = response.choices[0].message.content

            logger.info("‚úÖ Î∂ÑÏÑù ÏôÑÎ£å\n")
            
            # JSON ÌååÏã± (Í∏∞Ï°¥ Ìï®Ïàò ÏÇ¨Ïö©)
            from paste_3 import sanitize_and_parse_json  # Í∏∞Ï°¥ Ìï®Ïàò import
            analysis_result = sanitize_and_parse_json(analysis_text, selected_models, responses)
            analysis_result['preferredModel'] = self.api_type.upper()
            
            return analysis_result
        
        except Exception as e:
            logger.error(f"‚ùå Analysis error: {str(e)}")
            # Í∏∞Ï°¥ Ìè¥Î∞± Î°úÏßÅ
            error_analysis = {}
            for model in selected_models:
                model_lower = model.lower()
                error_analysis[model_lower] = {"Ïû•Ï†ê": "Î∂ÑÏÑù Ïã§Ìå®", "Îã®Ï†ê": "Î∂ÑÏÑù Ïã§Ìå®"}
            
            return {
                "preferredModel": self.api_type.upper(),
                "best_response": max(responses.values(), key=len) if responses else "",
                "analysis": error_analysis,
                "reasoning": "ÏùëÎãµ Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌïòÏó¨ ÏµúÏ†ÅÏùò ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±ÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§."
            }
# class ChatView(APIView):
#     permission_classes = [AllowAny]

#     def post(self, request, preferredModel):
#         try:
#             logger.info(f"Received chat request for {preferredModel}")
            
#             data = request.data
#             user_message = data.get('message')
#             compare_responses = data.get('compare', True)
            
#             # ÏÉàÎ°úÏö¥ ÌååÎùºÎØ∏ÌÑ∞: ÏÑ†ÌÉùÎêú Î™®Îç∏Îì§
#             selected_models = data.get('selectedModels', ['gpt', 'claude', 'mixtral'])
            
#             # ÏÑ†ÌÉùÎêú Î™®Îç∏ Î°úÍ∑∏
#             logger.info(f"Selected models: {selected_models}")
            
#             # ÌÜ†ÌÅ∞ Ïú†Î¨¥Ïóê Îî∞Î•∏ Ïñ∏Ïñ¥ Î∞è ÏÑ†Ìò∏ Î™®Îç∏ Ï≤òÎ¶¨
#             token = request.headers.get('Authorization')
#             if not token:
#                 # ÎπÑÎ°úÍ∑∏Ïù∏: Í∏∞Î≥∏ Ïñ∏Ïñ¥Îäî ko, ÏÑ†Ìò∏ Î™®Îç∏ÏùÄ GPTÎ°ú Í≥†Ï†ï
#                 user_language = 'ko'
#                 preferredModel = 'gpt'
#             else:
#                 # Î°úÍ∑∏Ïù∏: ÏöîÏ≤≠ Îç∞Ïù¥ÌÑ∞Ïùò Ïñ∏Ïñ¥ ÏÇ¨Ïö© (ÌòπÏùÄ ÏÇ¨Ïö©ÏûêÏùò ÏÑ§Ï†ïÏùÑ Îî∞Î¶Ñ)
#                 user_language = data.get('language', 'ko')
#                 # URLÏóê Ï†ÑÎã¨Îêú preferredModelÏùÑ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö© (ÌîÑÎ°†Ìä∏ÏóîÎìúÏóêÏÑú ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ï Î∞òÏòÅ)

#             logger.info(f"Received language setting: {user_language}")

#             if not user_message:
#                 return Response({'error': 'No message provided'}, 
#                                 status=status.HTTP_400_BAD_REQUEST)

#             # ÎπÑÎèôÍ∏∞ ÏùëÎãµÏùÑ ÏúÑÌïú StreamingHttpResponse ÏÇ¨Ïö©
#             from django.http import StreamingHttpResponse
#             import json
#             import time

#             def stream_responses():
#                 try:
#                     system_message = {
#                         "role": "system",
#                         "content": f"ÏÇ¨Ïö©ÏûêÍ∞Ä ÏÑ†ÌÉùÌïú Ïñ∏Ïñ¥Îäî '{user_language}'ÏûÖÎãàÎã§. Î∞òÎìúÏãú Î™®Îì† ÏùëÎãµÏùÑ Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú Ï†úÍ≥µÌï¥Ï£ºÏÑ∏Ïöî."
#                     }
                    
#                     responses = {}
                    
#                     # ÌòÑÏû¨ ÏöîÏ≤≠Ïóê ÎåÄÌïú Í≥†Ïú† ÏãùÎ≥ÑÏûê ÏÉùÏÑ± (ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ ÌôúÏö©)
#                     request_id = str(time.time())
                    
#                     # ÏÑ†ÌÉùÎêú Î™®Îç∏Îì§Îßå ÎåÄÌôîÏóê Ï∞∏Ïó¨ÏãúÌÇ¥
#                     selected_chatbots = {model: chatbots.get(model) for model in selected_models if model in chatbots}
                    
#                     # Í∞Å Î¥áÏùò ÏùëÎãµÏùÑ Í∞úÎ≥ÑÏ†ÅÏúºÎ°ú Ï≤òÎ¶¨ÌïòÍ≥† Ï¶âÏãú ÏùëÎãµ
#                     for bot_id, bot in selected_chatbots.items():
#                         if bot is None:
#                             logger.warning(f"Selected model {bot_id} not available in chatbots")
#                             yield json.dumps({
#                                 'type': 'bot_error',
#                                 'botId': bot_id,
#                                 'error': f"Model {bot_id} is not available"
#                             }) + '\n'
#                             continue
                            
#                         try:
#                             # Îß§Î≤à ÏÉàÎ°úÏö¥ ÎåÄÌôî Ïª®ÌÖçÏä§Ìä∏ ÏÉùÏÑ± (Ïù¥Ï†Ñ ÎÇ¥Ïö© Ï¥àÍ∏∞Ìôî)
#                             bot.conversation_history = [system_message]
#                             response = bot.chat(user_message)
#                             responses[bot_id] = response
                            
#                             # Í∞Å Î¥á ÏùëÎãµÏùÑ Ï¶âÏãú Ï†ÑÏÜ°
#                             yield json.dumps({
#                                 'type': 'bot_response',
#                                 'botId': bot_id,
#                                 'response': response,
#                                 'requestId': request_id  # ÏöîÏ≤≠ ID Ï∂îÍ∞Ä
#                             }) + '\n'
                            
#                         except Exception as e:
#                             logger.error(f"Error from {bot_id}: {str(e)}")
#                             responses[bot_id] = f"Error: {str(e)}"
                            
#                             # ÏóêÎü¨ÎèÑ Ï¶âÏãú Ï†ÑÏÜ°
#                             yield json.dumps({
#                                 'type': 'bot_error',
#                                 'botId': bot_id,
#                                 'error': str(e),
#                                 'requestId': request_id  # ÏöîÏ≤≠ ID Ï∂îÍ∞Ä
#                             }) + '\n'
                    
#                     # ÏÑ†ÌÉùÎêú Î™®Îç∏Ïù¥ ÏûàÍ≥† ÏùëÎãµÏù¥ ÏûàÏùÑ ÎïåÎßå Î∂ÑÏÑù ÏàòÌñâ
#                     if selected_models and responses:
#                         # Î∂ÑÏÑù(ÎπÑÍµê)ÏùÄ Î°úÍ∑∏Ïù∏ Ïãú ÏÇ¨Ïö©ÏûêÏùò ÏÑ†Ìò∏ Î™®Îç∏ÏùÑ, ÎπÑÎ°úÍ∑∏Ïù∏ Ïãú GPTÎ•º ÏÇ¨Ïö©
#                         if token:
#                             analyzer_bot = chatbots.get(preferredModel) or chatbots.get('gpt')
#                         else:
#                             analyzer_bot = chatbots.get('gpt')
                        
#                         # Î∂ÑÏÑùÏö© Î¥áÎèÑ ÏÉàÎ°úÏö¥ ÎåÄÌôî Ïª®ÌÖçÏä§Ìä∏Î°ú Ï¥àÍ∏∞Ìôî
#                         analyzer_bot.conversation_history = [system_message]
                        
#                         # Î∂ÑÏÑù Ïã§Ìñâ (Ìï≠ÏÉÅ ÏÉàÎ°≠Í≤å Ïã§Ìñâ)
#                         analysis = analyzer_bot.analyze_responses(responses, user_message, user_language, selected_models)
                        
#                         # Î∂ÑÏÑù Í≤∞Í≥º Ï†ÑÏÜ°
#                         yield json.dumps({
#                             'type': 'analysis',
#                             'preferredModel': analyzer_bot and analyzer_bot.api_type.upper(),
#                             'best_response': analysis.get('best_response', ''),
#                             'analysis': analysis.get('analysis', {}),
#                             'reasoning': analysis.get('reasoning', ''),
#                             'language': user_language,
#                             'requestId': request_id,  # ÏöîÏ≤≠ ID Ï∂îÍ∞Ä
#                             'timestamp': time.time()  # ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ Ï∂îÍ∞Ä
#                         }) + '\n'
#                     else:
#                         logger.warning("No selected models or responses to analyze")
                    
#                 except Exception as e:
#                     logger.error(f"Stream processing error: {str(e)}", exc_info=True)
#                     yield json.dumps({
#                         'type': 'error',
#                         'error': f"Stream processing error: {str(e)}"
#                     }) + '\n'

#             # StreamingHttpResponse Î∞òÌôò
#             response = StreamingHttpResponse(
#                 streaming_content=stream_responses(),
#                 content_type='text/event-stream'
#             )
#             response['Cache-Control'] = 'no-cache'
#             response['X-Accel-Buffering'] = 'no'
#             return response
                
#         except Exception as e:
#             logger.error(f"Unexpected error: {str(e)}", exc_info=True)
#             return Response({
#                 'error': str(e)
#             }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
from django.http import StreamingHttpResponse
import logging
import json
import openai
import anthropic
from groq import Groq
from django.conf import settings
from bs4 import BeautifulSoup
import re
import time
import asyncio
from asgiref.sync import sync_to_async

# ÏÉàÎ°ú Ï∂îÍ∞ÄÎêú import
from .langchain_config import LangChainManager
from .langgraph_workflow import AIComparisonWorkflow

logger = logging.getLogger(__name__)

def convert_to_serializable(obj):
    """Í∞ùÏ≤¥Î•º ÏßÅÎ†¨Ìôî Í∞ÄÎä•Ìïú ÌòïÌÉúÎ°ú Î≥ÄÌôò"""
    if hasattr(obj, '__dict__'):
        return {k: convert_to_serializable(v) for k, v in obj.__dict__.items()}
    elif isinstance(obj, dict):
        return {k: convert_to_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_serializable(item) for item in obj]
    else:
        return obj

class ChatView(APIView):
    permission_classes = [AllowAny]

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # Í∏∞Ï°¥ Ïú†ÏÇ¨ÎèÑ Î∂ÑÏÑùÍ∏∞
        from .similarity_analyzer import SimilarityAnalyzer  # Ïã§Ï†ú import Í≤ΩÎ°úÎ°ú Î≥ÄÍ≤Ω ÌïÑÏöî
        self.similarity_analyzer = SimilarityAnalyzer(threshold=0.85)
        
        # LangChain Í¥ÄÎ¶¨Ïûê Ï¥àÍ∏∞Ìôî
        self.langchain_manager = LangChainManager(
            openai_key=OPENAI_API_KEY,
            anthropic_key=ANTHROPIC_API_KEY,
            groq_key=GROQ_API_KEY
        )
        
        # LangGraph ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ï¥àÍ∏∞Ìôî
        self.workflow = AIComparisonWorkflow(
            langchain_manager=self.langchain_manager,
            similarity_analyzer=self.similarity_analyzer
        )
        
        # Í∏∞Ï°¥ ChatBot Ïù∏Ïä§ÌÑ¥Ïä§Îì§ÎèÑ LangChain ÏÇ¨Ïö©ÌïòÎèÑÎ°ù ÏóÖÎç∞Ïù¥Ìä∏
        self.update_chatbots_with_langchain()

    def update_chatbots_with_langchain(self):
        """Í∏∞Ï°¥ ChatBotÎì§ÏùÑ LangChainÏùÑ ÏÇ¨Ïö©ÌïòÎèÑÎ°ù ÏóÖÎç∞Ïù¥Ìä∏"""
        global chatbots
        
        # Í∏∞Ï°¥ ChatBotÎì§Ïóê LangChain Îß§ÎãàÏ†Ä Ï∂îÍ∞Ä
        for bot_id, bot in chatbots.items():
            bot.langchain_manager = self.langchain_manager
            bot.use_langchain = True
            
            # LangChain Ï≤¥Ïù∏ ÏÉùÏÑ± ÏãúÎèÑ
            try:
                if bot_id == 'gpt':
                    bot.chat_chain = self.langchain_manager.create_chat_chain('gpt')
                elif bot_id == 'claude':
                    bot.chat_chain = self.langchain_manager.create_chat_chain('claude')
                elif bot_id == 'mixtral':
                    bot.groq_llm = self.langchain_manager.groq_llm if hasattr(self.langchain_manager, 'groq_llm') else None
                logger.info(f"LangChain Ï≤¥Ïù∏ ÏÉùÏÑ± ÏôÑÎ£å: {bot_id}")
            except Exception as e:
                logger.warning(f"LangChain Ï≤¥Ïù∏ ÏÉùÏÑ± Ïã§Ìå® ({bot_id}), Í∏∞Ï°¥ Î∞©Ïãù ÏÇ¨Ïö©: {e}")
                bot.use_langchain = False

    def post(self, request, preferredModel):
        try:
            logger.info(f"Received chat request for {preferredModel}")
            data = request.data
            user_message = data.get('message')
            selected_models = data.get('selectedModels', ['gpt', 'claude', 'mixtral'])
            token = request.headers.get('Authorization')
            user_language = 'ko' if not token else data.get('language', 'ko')
            use_workflow = data.get('useWorkflow', True)  # ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏÇ¨Ïö© Ïó¨Î∂Ä
            
            if not user_message:
                return Response({'error': 'No message provided'}, status=status.HTTP_400_BAD_REQUEST)

            # URL Ï≤òÎ¶¨ Î°úÏßÅ (Í∏∞Ï°¥Í≥º ÎèôÏùº)
            url_pattern = r'^(https?://\S+)$'
            match = re.match(url_pattern, user_message.strip())
            if match:
                url = match.group(1)
                try:
                    page_text = fetch_and_clean_url(url)
                    if len(page_text) > 10000:
                        page_text = page_text[:5000] + "\n\n‚Ä¶(Ï§ëÎûµ)‚Ä¶\n\n" + page_text[-5000:]
                    user_message = (
                        f"Îã§Ïùå ÏõπÌéòÏù¥ÏßÄÏùò ÎÇ¥Ïö©ÏùÑ Î∂ÑÏÑùÌï¥ Ï£ºÏÑ∏Ïöî:\n"
                        f"URL: {url}\n\n"
                        f"{page_text}"
                    )
                except Exception as e:
                    logger.error(f"URL fetch error: {e}")
                    return Response({'error': f"URLÏùÑ Í∞ÄÏ†∏Ïò§ÏßÄ Î™ªÌñàÏäµÎãàÎã§: {e}"}, status=status.HTTP_400_BAD_REQUEST)

            # ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏÇ¨Ïö© Ïó¨Î∂ÄÏóê Îî∞Î•∏ Î∂ÑÍ∏∞
            if use_workflow:
                return self.handle_with_workflow(user_message, selected_models, user_language, preferredModel)
            else:
                return self.handle_with_legacy(user_message, selected_models, user_language, preferredModel)

        except Exception as e:
            logger.error(f"Unexpected error: {e}", exc_info=True)
            return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def handle_with_workflow(self, user_message, selected_models, user_language, preferred_model):
        """LangGraph ÏõåÌÅ¨ÌîåÎ°úÏö∞Î•º ÏÇ¨Ïö©Ìïú Ï≤òÎ¶¨"""
        def stream_workflow_responses():
            try:
                request_id = str(time.time())
                
                # ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ïã§ÌñâÏùÑ ÏúÑÌïú async ÎûòÌçº
                async def run_workflow_async():
                    return await self.workflow.run_workflow(
                        user_message=user_message,
                        selected_models=selected_models,
                        user_language=user_language,
                        request_id=request_id
                    )
                
                # asyncio Ïù¥Î≤§Ìä∏ Î£®ÌîÑÏóêÏÑú Ïã§Ìñâ
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                try:
                    workflow_result = loop.run_until_complete(run_workflow_async())
                finally:
                    loop.close()
                
                # Í∞úÎ≥Ñ ÏùëÎãµ Ïä§Ìä∏Î¶¨Î∞ç
                for bot_id, response in workflow_result["individual_responses"].items():
                    yield json.dumps({
                        'type': 'bot_response',
                        'botId': bot_id,
                        'response': response,
                        'requestId': request_id
                    }) + '\n'
                
                # Ïú†ÏÇ¨ÎèÑ Î∂ÑÏÑù Í≤∞Í≥º
                if workflow_result["similarity_analysis"]:
                    yield json.dumps({
                        'type': 'similarity_analysis',
                        'result': workflow_result["similarity_analysis"],
                        'requestId': request_id,
                        'timestamp': time.time(),
                        'userMessage': user_message
                    }) + '\n'
                
                # ÏµúÏ¢Ö Î∂ÑÏÑù Í≤∞Í≥º
                final_analysis = workflow_result["final_analysis"]
                yield json.dumps({
                    'type': 'analysis',
                    'preferredModel': final_analysis.get('preferredModel', preferred_model.upper()),
                    'best_response': final_analysis.get('best_response', ''),
                    'analysis': final_analysis.get('analysis', {}),
                    'reasoning': final_analysis.get('reasoning', ''),
                    'language': user_language,
                    'requestId': request_id,
                    'timestamp': time.time(),
                    'userMessage': user_message,
                    'workflowUsed': True,
                    'errors': workflow_result.get("errors", [])
                }) + '\n'
                
            except Exception as e:
                logger.error(f"ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ïä§Ìä∏Î¶¨Î∞ç ÏóêÎü¨: {e}")
                yield json.dumps({
                    'type': 'error',
                    'error': f"Workflow error: {e}",
                    'fallbackToLegacy': True
                }) + '\n'

        return StreamingHttpResponse(stream_workflow_responses(), content_type='text/event-stream')

    def handle_with_legacy(self, user_message, selected_models, user_language, preferred_model):
        """Í∏∞Ï°¥ Î∞©ÏãùÏúºÎ°ú Ï≤òÎ¶¨ (Ìò∏ÌôòÏÑ± Ïú†ÏßÄ)"""
        def stream_responses():
            try:
                system_message = {
                    'role': 'system',
                    'content': f"ÏÇ¨Ïö©ÏûêÍ∞Ä ÏÑ†ÌÉùÌïú Ïñ∏Ïñ¥Îäî '{user_language}'ÏûÖÎãàÎã§. Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú ÏùëÎãµÌïòÏÑ∏Ïöî."
                }
                responses = {}
                request_id = str(time.time())
                
                # Í∞Å Î™®Îç∏Î≥Ñ Ï±óÎ¥á Ïù∏Ïä§ÌÑ¥Ïä§ Í∞ÄÏ†∏Ïò§Í∏∞
                selected_chatbots = {m: chatbots.get(m) for m in selected_models if chatbots.get(m)}

                # Î™®Îç∏ ÏùëÎãµ ÏàòÏßë (ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨ ÏãúÎèÑ)
                async def collect_responses_async():
                    responses = {}
                    tasks = []
                    
                    for bot_id, bot in selected_chatbots.items():
                        if hasattr(bot, 'chat_async') and bot.use_langchain:
                            # LangChain ÎπÑÎèôÍ∏∞ ÏÇ¨Ïö©
                            task = bot.chat_async(user_message, user_language=user_language)
                        else:
                            # Í∏∞Ï°¥ ÎèôÍ∏∞ Î∞©ÏãùÏùÑ ÎπÑÎèôÍ∏∞Î°ú ÎûòÌïë
                            task = sync_to_async(self.sync_chat)(bot, user_message, system_message)
                        tasks.append((bot_id, task))
                    
                    for bot_id, task in tasks:
                        try:
                            response = await task
                            responses[bot_id] = response
                            logger.info(f"‚úÖ {bot_id} ÏùëÎãµ ÏôÑÎ£å")
                        except Exception as e:
                            logger.error(f"‚ùå {bot_id} ÏùëÎãµ Ïã§Ìå®: {e}")
                    
                    return responses

                # ÎπÑÎèôÍ∏∞ ÏùëÎãµ ÏàòÏßë
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                try:
                    responses = loop.run_until_complete(collect_responses_async())
                finally:
                    loop.close()

                # Í∞úÎ≥Ñ ÏùëÎãµ Ïä§Ìä∏Î¶¨Î∞ç
                for bot_id, resp_text in responses.items():
                    yield json.dumps({
                        'type': 'bot_response',
                        'botId': bot_id,
                        'response': resp_text,
                        'requestId': request_id
                    }) + '\n'

                # Ïú†ÏÇ¨ÎèÑ Î∂ÑÏÑù
                if len(responses) >= 2:
                    sim_res = self.similarity_analyzer.cluster_responses(responses)
                    serial = convert_to_serializable(sim_res)
                    yield json.dumps({
                        'type': 'similarity_analysis',
                        'result': serial,
                        'requestId': request_id,
                        'timestamp': time.time(),
                        'userMessage': user_message
                    }) + '\n'

                # ÏµúÏ¢Ö ÎπÑÍµê Î∞è Î∂ÑÏÑù
                analyzer_bot = chatbots.get(preferred_model) or chatbots.get('gpt')
                analyzer_bot.conversation_history = [system_message]
                
                # LangChain ÎπÑÎèôÍ∏∞ Î∂ÑÏÑù ÏãúÎèÑ
                if hasattr(analyzer_bot, 'analyze_responses_async') and analyzer_bot.use_langchain:
                    async def analyze_async():
                        return await analyzer_bot.analyze_responses_async(
                            responses, user_message, user_language, list(responses.keys())
                        )
                    
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    
                    try:
                        analysis = loop.run_until_complete(analyze_async())
                    finally:
                        loop.close()
                else:
                    # Í∏∞Ï°¥ ÎèôÍ∏∞ Î∞©Ïãù
                    analysis = analyzer_bot.analyze_responses(
                        responses, user_message, user_language, list(responses.keys())
                    )
                
                yield json.dumps({
                    'type': 'analysis',
                    'preferredModel': analyzer_bot.api_type.upper(),
                    'best_response': analysis.get('best_response', ''),
                    'analysis': analysis.get('analysis', {}),
                    'reasoning': analysis.get('reasoning', ''),
                    'language': user_language,
                    'requestId': request_id,
                    'timestamp': time.time(),
                    'userMessage': user_message,
                    'workflowUsed': False
                }) + '\n'
                
            except Exception as e:
                yield json.dumps({
                    'type': 'error',
                    'error': f"Stream error: {e}"
                }) + '\n'

        return StreamingHttpResponse(stream_responses(), content_type='text/event-stream')

    def sync_chat(self, bot, user_message, system_message):
        """ÎèôÍ∏∞ Ï±ÑÌåÖÏùÑ ÏúÑÌïú Ìó¨Ìçº Î©îÏÑúÎìú"""
        bot.conversation_history = [system_message]
        return bot.chat(user_message)
from dotenv import load_dotenv
load_dotenv()
# API ÌÇ§ ÏÑ§Ï†ï (Í∏∞Ï°¥Í≥º ÎèôÏùº)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
# ChatBot import (ÏàòÏ†ïÎêú Î≤ÑÏ†Ñ)

chatbots = {
    'gpt': ChatBot(OPENAI_API_KEY, 'gpt-3.5-turbo', 'openai'),
    'claude': ChatBot(ANTHROPIC_API_KEY, 'claude-3-5-haiku-20241022', 'anthropic'), 
    'mixtral': ChatBot(GROQ_API_KEY, 'llama-3.1-8b-instant', 'groq'),
}
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import IsAuthenticated
from .models import UserProfile, UserSettings
from .serializers import UserSerializer

class UserSettingsView(APIView):
    permission_classes = [IsAuthenticated]

    def get(self, request):
        try:
            user_settings, created = UserSettings.objects.get_or_create(
                user=request.user,
                defaults={
                    'language': 'ko',
                    'analyzer_bot': 'claude'
                }
            )
            serializer = UserSerializer(user_settings)
            return Response(serializer.data)
        except Exception as e:
            return Response({
                'language': 'ko', 
                'analyzer_bot': 'claude'
            }, status=status.HTTP_200_OK)

    def post(self, request):
        try:
            user_settings, created = UserSettings.objects.get_or_create(
                user=request.user,
                defaults={
                    'language': request.data.get('language', 'ko'),
                    'analyzer_bot': request.data.get('analyzer_bot', 'claude')
                }
            )

            # Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÎäî Í≤ΩÏö∞ ÏóÖÎç∞Ïù¥Ìä∏
            if not created:
                user_settings.language = request.data.get('language', user_settings.language)
                user_settings.analyzer_bot = request.data.get('analyzer_bot', user_settings.analyzer_bot)
                user_settings.save()

            serializer = UserSettingsSerializer(user_settings)
            return Response(serializer.data, status=status.HTTP_200_OK)
        except Exception as e:
            return Response({
                'error': str(e)
            }, status=status.HTTP_400_BAD_REQUEST)
from rest_framework import status
from rest_framework.decorators import api_view, permission_classes
from rest_framework.permissions import AllowAny
from rest_framework.response import Response
import requests
import logging
from django.contrib.auth import get_user_model
from .models import SocialAccount

import uuid

logger = logging.getLogger(__name__)
User = get_user_model()

# def generate_unique_username(email, name=None):
#     """Í≥†Ïú†Ìïú username ÏÉùÏÑ±"""
#     base = name or email.split('@')[0]
#     username = base
#     suffix = 1
    
#     # usernameÏù¥ Í≥†Ïú†Ìï† ÎïåÍπåÏßÄ Ïà´Ïûê Ï∂îÍ∞Ä
#     while User.objects.filter(username=username).exists():
#         username = f"{base}_{suffix}"
#         suffix += 1
    
#     return username

def generate_unique_username(email, name=None):
    """username ÏÉùÏÑ± - Ïù¥Î©îÏùº ÏïûÎ∂ÄÎ∂Ñ ÎòêÎäî Ïù¥Î¶Ñ ÏÇ¨Ïö©"""
    if name:
        return name  # Ïù¥Î¶ÑÏù¥ ÏûàÏúºÎ©¥ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©
    return email.split('@')[0]  # Ïù¥Î¶ÑÏù¥ ÏóÜÏúºÎ©¥ Ïù¥Î©îÏùº ÏïûÎ∂ÄÎ∂Ñ ÏÇ¨Ïö©
from rest_framework.authentication import TokenAuthentication
from rest_framework.permissions import AllowAny
from rest_framework.authtoken.models import Token
from rest_framework import status
from rest_framework.decorators import api_view, authentication_classes, permission_classes
# views.py
from django.db import transaction, IntegrityError

# @api_view(['GET'])
# @authentication_classes([TokenAuthentication])
# @permission_classes([AllowAny])
# @api_view(['GET'])
# @permission_classes([AllowAny])
# def google_callback(request):
#     logger.info("Starting Google callback process")  # Î°úÍπÖ Ï∂îÍ∞Ä
#     try:
#         with transaction.atomic():
#             # 1. ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
#             auth_header = request.headers.get('Authorization', '')
#             access_token = auth_header.split(' ')[1]
            
#             user_info_response = requests.get(
#                 'https://www.googleapis.com/oauth2/v3/userinfo',
#                 headers={'Authorization': f'Bearer {access_token}'}
#             )
            
#             user_info = user_info_response.json()
#             email = user_info.get('email')
#             name = user_info.get('name')

#             logger.info(f"Processing user: {email}")  # Î°úÍπÖ Ï∂îÍ∞Ä

#             # 2. User Í∞ùÏ≤¥ Í∞ÄÏ†∏Ïò§Í∏∞ ÎòêÎäî ÏÉùÏÑ±
#             user = User.objects.filter(email=email).first()
#             if not user:
#                 user = User.objects.create(
#                     username=email,
#                     email=email,
#                     first_name=name or '',
#                     is_active=True
#                 )
#                 logger.info(f"Created new user: {user.id}")
#             else:
#                 logger.info(f"Found existing user: {user.id}")

#             # 3. Í∏∞Ï°¥ UserSettings ÏÇ≠Ï†ú (ÏûàÎã§Î©¥)
#             UserSettings.objects.filter(user=user).delete()
#             logger.info("Deleted any existing settings")

#             # 4. ÏÉàÎ°úÏö¥ UserSettings ÏÉùÏÑ±
#             settings = UserSettings.objects.create(
#                 user=user,
#                 language='ko',
#                 preferred_model='default'
#             )
#             logger.info(f"Created new settings for user: {user.id}")

#             # 5. ÌÜ†ÌÅ∞ ÏÉùÏÑ±
#             token, _ = Token.objects.get_or_create(user=user)
            
#             return Response({
#                 'user': {
#                     'id': user.id,
#                     'email': user.email,
#                     'username': user.username,
#                     'first_name': user.first_name,
#                     'settings': {
#                         'language': settings.language,
#                         'preferred_model': settings.preferred_model
#                     }
#                 },
#                 'access_token': token.key
#             })
#     except Exception as e:
#         logger.error(f"Error in google_callback: {str(e)}")
#         return Response(
#             {'error': str(e)},
#             status=status.HTTP_400_BAD_REQUEST
        # )
@api_view(['GET'])
@authentication_classes([TokenAuthentication])
@permission_classes([AllowAny])
def google_callback(request):
    try:
        # Ïï°ÏÑ∏Ïä§ ÌÜ†ÌÅ∞ Ï∂îÏ∂ú
        auth_header = request.headers.get('Authorization', '')
        if not auth_header.startswith('Bearer '):
            return Response(
                {'error': 'ÏûòÎ™ªÎêú Ïù∏Ï¶ù Ìó§Îçî'}, 
                status=status.HTTP_401_UNAUTHORIZED
            )
        
        access_token = auth_header.split(' ')[1]

        # Google APIÎ°ú ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥ ÏöîÏ≤≠
        user_info_response = requests.get(
            'https://www.googleapis.com/oauth2/v3/userinfo',
            headers={'Authorization': f'Bearer {access_token}'}
        )

        if user_info_response.status_code != 200:
            return Response(
                {'error': 'GoogleÏóêÏÑú ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Ïò§ÎäîÎç∞ Ïã§Ìå®ÌñàÏäµÎãàÎã§'}, 
                status=status.HTTP_400_BAD_REQUEST
            )

        user_info = user_info_response.json()
        email = user_info.get('email')
        name = user_info.get('name')
        
        if not email:
            return Response(
                {'error': 'Ïù¥Î©îÏùºÏù¥ Ï†úÍ≥µÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§'}, 
                status=status.HTTP_400_BAD_REQUEST
            )

        try:
            # Í∏∞Ï°¥ ÏÇ¨Ïö©Ïûê Í≤ÄÏÉâ
            user = User.objects.get(email=email)
        except User.DoesNotExist:
            # ÏÉàÎ°úÏö¥ ÏÇ¨Ïö©Ïûê ÏÉùÏÑ±
            username = generate_unique_username(email, name)
            user = User.objects.create(
                username=username,
                email=email,
                is_active=True
            )
            
            # Í∏∞Î≥∏ ÎπÑÎ∞ÄÎ≤àÌò∏ ÏÑ§Ï†ï (ÏÑ†ÌÉùÏ†Å)
            random_password = uuid.uuid4().hex
            user.set_password(random_password)
            user.save()

        # ÏÜåÏÖú Í≥ÑÏ†ï Ï†ïÎ≥¥ ÏÉùÏÑ± ÎòêÎäî ÏóÖÎç∞Ïù¥Ìä∏
        social_account, created = SocialAccount.objects.get_or_create(
            email=email,
            provider='google',
            defaults={'user': user}
        )

        if not created and social_account.user != user:
            social_account.user = user
            social_account.save()

        # ÌÜ†ÌÅ∞ ÏÉùÏÑ± ÎòêÎäî Í∞ÄÏ†∏Ïò§Í∏∞
        token, created = Token.objects.get_or_create(user=user)
        logger.info(f"GOOGLE Token created: {created}")
        logger.info(f"Token key: {token.key}")
        logger.info(f"Token user: {token.user.username}")


        # ÏÇ¨Ïö©Ïûê Îç∞Ïù¥ÌÑ∞ Î∞òÌôò
        serializer = UserSerializer(user)
        return Response({
            'user': serializer.data,
            'access_token': token.key,  # Django REST Framework Token Î∞òÌôò
            'token_created': created,
            'google_access_token': access_token,  # Google OAuth Ïï°ÏÑ∏Ïä§ ÌÜ†ÌÅ∞

        })

    except Exception as e:
        logger.error(f"Error in google_callback: {str(e)}")
        return Response(
            {'error': str(e)}, 
            status=status.HTTP_500_INTERNAL_SERVER_ERROR
        )
import requests
import json
import requests
from django.conf import settings
from django.http import JsonResponse
from django.shortcuts import redirect
from .models import User  # User Î™®Îç∏ÏùÑ ÏûÑÌè¨Ìä∏


@api_view(['GET'])
@permission_classes([AllowAny])
def kakao_callback(request):
    try:
        auth_code = request.GET.get('code')
        logger.info(f"Received Kakao auth code: {auth_code}")
        
        # Ïπ¥Ïπ¥Ïò§ ÌÜ†ÌÅ∞ Î∞õÍ∏∞
        token_url = "https://kauth.kakao.com/oauth/token"
        data = {
            "grant_type": "authorization_code",
            "client_id": settings.KAKAO_CLIENT_ID,
            "redirect_uri": settings.KAKAO_REDIRECT_URI,
            "code": auth_code,
        }
        
        token_response = requests.post(token_url, data=data)
        
        if not token_response.ok:
            return Response({
                'error': 'Ïπ¥Ïπ¥Ïò§ ÌÜ†ÌÅ∞ Î∞õÍ∏∞ Ïã§Ìå®',
                'details': token_response.text
            }, status=status.HTTP_400_BAD_REQUEST)
        
        token_data = token_response.json()
        access_token = token_data.get('access_token')
        
        if not access_token:
            return Response({
                'error': 'Ïï°ÏÑ∏Ïä§ ÌÜ†ÌÅ∞ ÏóÜÏùå',
                'details': token_data
            }, status=status.HTTP_400_BAD_REQUEST)

        # Ïπ¥Ïπ¥Ïò§ ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥ Î∞õÍ∏∞
        user_info_url = "https://kapi.kakao.com/v2/user/me"
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-type": "application/x-www-form-urlencoded;charset=utf-8"
        }
        
        user_info_response = requests.get(
            user_info_url,
            headers=headers,
            params={
                'property_keys': json.dumps([
                    "kakao_account.email",
                    "kakao_account.profile",
                    "kakao_account.name"
                ])
            }
        )
        
        if not user_info_response.ok:
            return Response({
                'error': 'ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥ Î∞õÍ∏∞ Ïã§Ìå®',
                'details': user_info_response.text
            }, status=status.HTTP_400_BAD_REQUEST)
        
        user_info = user_info_response.json()
        kakao_account = user_info.get('kakao_account', {})
        email = kakao_account.get('email')
        profile = kakao_account.get('profile', {})
        nickname = profile.get('nickname')
        
        logger.info(f"Kakao user info - email: {email}, nickname: {nickname}")
        
        if not email:
            return Response({
                'error': 'Ïù¥Î©îÏùº Ï†ïÎ≥¥ ÏóÜÏùå',
                'details': 'Ïπ¥Ïπ¥Ïò§ Í≥ÑÏ†ïÏùò Ïù¥Î©îÏùº Ï†ïÎ≥¥Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.'
            }, status=status.HTTP_400_BAD_REQUEST)

        # ÏÇ¨Ïö©Ïûê ÏÉùÏÑ± ÎòêÎäî ÏóÖÎç∞Ïù¥Ìä∏
        try:
            user = User.objects.get(email=email)
            logger.info(f"Updated existing user with nickname: {nickname}")
        except User.DoesNotExist:
            unique_username = generate_unique_username(email, nickname)
            user = User.objects.create(
                email=email,
                username=unique_username,
                is_active=True
            )            
            logger.info(f"Created new user with nickname: {nickname}")

        # ÏÜåÏÖú Í≥ÑÏ†ï ÏÉùÏÑ± ÎòêÎäî ÏóÖÎç∞Ïù¥Ìä∏
        social_account, created = SocialAccount.objects.update_or_create(
            email=email,
            provider='kakao',
            defaults={
                'user': user,
                'nickname': nickname
            }
        )
        logger.info(f"Social account updated - email: {email}, nickname: {nickname}")

        if not created and social_account.user != user:
            social_account.user = user
            social_account.save()

        # ÌÜ†ÌÅ∞ ÏÉùÏÑ± ÎòêÎäî Í∞ÄÏ†∏Ïò§Í∏∞
        token, token_created = Token.objects.get_or_create(user=user)
        logger.info(f"KAKAO Token created: {token_created}")
        logger.info(f"Token key: {token.key}")
        logger.info(f"Token user: {token.user.username}")

        serializer = UserSerializer(user)
        return Response({
            'user': serializer.data,
            'access_token': token.key,  # Django REST Framework Token Î∞òÌôò
            'token_created': created,
            'kakao_access_token': access_token,  # Google OAuth Ïï°ÏÑ∏Ïä§ ÌÜ†ÌÅ∞

        })


        
    except Exception as e:
        logger.exception("Unexpected error in kakao_callback")
        return Response({
            'error': str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


from rest_framework.authtoken.models import Token  # Token Î™®Îç∏ Ï∂îÍ∞Ä

@api_view(['GET'])
@permission_classes([AllowAny])
def naver_callback(request):
    try:
        code = request.GET.get('code')
        state = request.GET.get('state')
        logger.info(f"Received Naver auth code: {code}")

        # ÎÑ§Ïù¥Î≤Ñ ÌÜ†ÌÅ∞ Î∞õÍ∏∞
        token_url = "https://nid.naver.com/oauth2.0/token"
        token_params = {
            "grant_type": "authorization_code",
            "client_id": settings.NAVER_CLIENT_ID,
            "client_secret": settings.NAVER_CLIENT_SECRET,
            "code": code,
            "state": state
        }

        token_response = requests.get(token_url, params=token_params)

        if not token_response.ok:
            return Response({
                'error': 'ÎÑ§Ïù¥Î≤Ñ ÌÜ†ÌÅ∞ Î∞õÍ∏∞ Ïã§Ìå®',
                'details': token_response.text
            }, status=status.HTTP_400_BAD_REQUEST)

        token_data = token_response.json()
        access_token = token_data.get('access_token')

        if not access_token:
            return Response({
                'error': 'Ïï°ÏÑ∏Ïä§ ÌÜ†ÌÅ∞ ÏóÜÏùå',
                'details': token_data
            }, status=status.HTTP_400_BAD_REQUEST)

        # ÎÑ§Ïù¥Î≤Ñ ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥ Î∞õÍ∏∞
        user_info_url = "https://openapi.naver.com/v1/nid/me"
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-type": "application/x-www-form-urlencoded;charset=utf-8"
        }

        user_info_response = requests.get(user_info_url, headers=headers)

        if not user_info_response.ok:
            return Response({
                'error': 'ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥ Î∞õÍ∏∞ Ïã§Ìå®',
                'details': user_info_response.text
            }, status=status.HTTP_400_BAD_REQUEST)

        user_info = user_info_response.json()
        response = user_info.get('response', {})
        email = response.get('email')
        nickname = response.get('nickname')
        username = email.split('@')[0]

        if not email:
            return Response({
                'error': 'Ïù¥Î©îÏùº Ï†ïÎ≥¥ ÏóÜÏùå',
                'details': 'ÎÑ§Ïù¥Î≤Ñ Í≥ÑÏ†ïÏùò Ïù¥Î©îÏùº Ï†ïÎ≥¥Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.'
            }, status=status.HTTP_400_BAD_REQUEST)

        # ÏÇ¨Ïö©Ïûê ÏÉùÏÑ± ÎòêÎäî Ï°∞Ìöå
        user, created = User.objects.get_or_create(
            email=email,
            defaults={'username': generate_unique_username(email, username), 'is_active': True}
        )

        # ÏÜåÏÖú Í≥ÑÏ†ï Ï°∞Ìöå Î∞è ÏóÖÎç∞Ïù¥Ìä∏
        social_account, social_created = SocialAccount.objects.update_or_create(
            provider='naver',
            email=email,
            defaults={'user': user, 'nickname': nickname}
        )

        logger.info(f"Social account updated - email: {email}, nickname: {nickname}")

        # ‚úÖ Django REST Framework Token ÏÉùÏÑ±
        token, token_created = Token.objects.get_or_create(user=user)
        logger.info(f"Naver Token created: {token_created}")
        logger.info(f"Token key: {token.key}")
        logger.info(f"Token user: {token.user.username}")

        serializer = UserSerializer(user)
        return Response({
            'user': serializer.data,
            'access_token': token.key,  # Django REST Framework Token Î∞òÌôò
            'token_created': created,
            'naver_access_token': access_token,  # ÎÑ§Ïù¥Î≤Ñ Ïï°ÏÑ∏Ïä§ ÌÜ†ÌÅ∞
        })

    except Exception as e:
        logger.exception("Unexpected error in naver_callback")
        return Response({
            'error': str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# views.py
import logging
from rest_framework.decorators import api_view, permission_classes
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from django.contrib.auth import get_user_model

# logger = logging.getLogger(__name__)

# @api_view(['PUT'])
# @permission_classes([IsAuthenticated])
# def update_user_settings(request):
#     # Ï∂îÍ∞Ä Î°úÍπÖ Î∞è ÎîîÎ≤ÑÍπÖ
#     logger.info(f"User authentication status: {request.user.is_authenticated}")
#     logger.info(f"User: {request.user}")
#     logger.info(f"Request headers: {request.headers}")
    
#     try:
#         # Ïù∏Ï¶ù ÏÉÅÌÉú Î™ÖÏãúÏ†Å ÌôïÏù∏
#         if not request.user.is_authenticated:
#             logger.error("Unauthenticated user attempt")
#             return Response({
#                 'status': 'error',
#                 'message': 'Ïù∏Ï¶ùÎêòÏßÄ ÏïäÏùÄ ÏÇ¨Ïö©ÏûêÏûÖÎãàÎã§.'
#             }, status=401)
        
#         # UserProfile Î™®Îç∏Ïù¥ ÏûàÎã§Í≥† Í∞ÄÏ†ï
#         user = request.user
#         user_profile = user.userprofile
        
#         # ÏÑ§Ï†ï ÏóÖÎç∞Ïù¥Ìä∏
#         settings_data = request.data
#         user_profile.language = settings_data.get('language', user_profile.language)
#         user_profile.preferred_model = settings_data.get('preferredModel', user_profile.preferred_model)
#         user_profile.save()
        
#         return Response({
#             'status': 'success',
#             'message': 'ÏÑ§Ï†ïÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏóÖÎç∞Ïù¥Ìä∏ÎêòÏóàÏäµÎãàÎã§.',
#             'settings': {
#                 'language': user_profile.language,
#                 'preferredModel': user_profile.preferred_model
#             }
#         })
    
#     except Exception as e:
#         print("Error:", str(e))  # ÏóêÎü¨ Î°úÍπÖ
#         logger.error(f"Settings update error: {str(e)}")
#         return Response({
#             'status': 'error',
#             'message': f'Ïò§Î•ò Î∞úÏÉù: {str(e)}'
#         }, status=400)
# views.py
# Î∞±ÏóîÎìúÏóêÏÑú ÌÜ†ÌÅ∞ ÌòïÏãù ÌôïÏù∏
@api_view(['PUT'])
@authentication_classes([TokenAuthentication])
@permission_classes([IsAuthenticated])
def update_user_settings(request):
    try:
        # ÌÜ†ÌÅ∞ Î°úÍπÖ Ï∂îÍ∞Ä
        token_header = request.headers.get('Authorization')
        if not token_header or not token_header.startswith('Token '):
            return Response({'error': 'ÏûòÎ™ªÎêú ÌÜ†ÌÅ∞ ÌòïÏãù'}, status=status.HTTP_401_UNAUTHORIZED)
        
        user = request.user
        if not user.is_authenticated:
            return Response({'error': 'Ïù∏Ï¶ùÎêòÏßÄ ÏïäÏùÄ ÏÇ¨Ïö©Ïûê'}, status=status.HTTP_401_UNAUTHORIZED)
        
        settings_data = request.data
        print(f"Received settings data: {settings_data}")  # Îç∞Ïù¥ÌÑ∞ Î°úÍπÖ Ï∂îÍ∞Ä
        
        # UserSettings ÏóÖÎç∞Ïù¥Ìä∏ ÎòêÎäî ÏÉùÏÑ±
        settings, created = UserSettings.objects.get_or_create(user=user)
        
        # ÌïÑÎìú ÏóÖÎç∞Ïù¥Ìä∏
        if 'language' in settings_data:
            settings.language = settings_data['language']
        if 'preferredModel' in settings_data:
            settings.preferred_model = settings_data['preferredModel']
        
        settings.save()
        
        return Response({
            'message': 'Settings updated successfully',
            'settings': {
                'language': settings.language,
                'preferredModel': settings.preferred_model
            }
        })
        
    except Exception as e:
        logger.error(f"Settings update error: {str(e)}")
        return Response(
            {'error': str(e)},
            status=status.HTTP_400_BAD_REQUEST
        )
from rest_framework import status
from rest_framework.decorators import api_view, authentication_classes, permission_classes
from rest_framework.authentication import TokenAuthentication
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from django.core.exceptions import ObjectDoesNotExist

@api_view(['PUT'])
@authentication_classes([TokenAuthentication])
@permission_classes([IsAuthenticated])
def update_user_settings(request):
    try:
        user = request.user
        settings_data = request.data
        
        # UserProfile ÌôïÏù∏ Î∞è ÏÉùÏÑ±
        try:
            profile = user.userprofile
        except ObjectDoesNotExist:
            profile = UserProfile.objects.create(user=user)
            
        # UserSettings ÌôïÏù∏ Î∞è ÏÉùÏÑ±/ÏóÖÎç∞Ïù¥Ìä∏
        settings, created = UserSettings.objects.get_or_create(
            user=user,
            defaults={
                'language': settings_data.get('language', 'en'),
                'preferred_model': settings_data.get('preferredModel', 'default')
            }
        )
        
        if not created:
            settings.language = settings_data.get('language', settings.language)
            settings.preferred_model = settings_data.get('preferredModel', settings.preferred_model)
            settings.save()
            
        return Response({
            'message': 'Settings updated successfully',
            'settings': {
                'language': settings.language,
                'preferredModel': settings.preferred_model
            }
        })
            
    except Exception as e:
        print(f"Settings update error: {str(e)}")
        return Response(
            {'error': str(e)},
            status=status.HTTP_400_BAD_REQUEST
        )

import logging
import re
import math
import numpy as np
from collections import Counter
from typing import Dict, List, Union, Any
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
logger = logging.getLogger(__name__)

class SimilarityAnalyzer:
    """
    AI Î™®Îç∏ ÏùëÎãµ Í∞ÑÏùò Ïú†ÏÇ¨ÎèÑÎ•º Î∂ÑÏÑùÌïòÍ≥† ÏùëÎãµ ÌäπÏÑ±ÏùÑ Ï∂îÏ∂úÌïòÎäî ÌÅ¥ÎûòÏä§
    Îã§Íµ≠Ïñ¥ ÏßÄÏõêÏùÑ ÏúÑÌï¥ paraphrase-multilingual-MiniLM-L12-v2 Î™®Îç∏ ÏÇ¨Ïö©
    """
    
    def __init__(self, threshold=0., use_transformer=True):
        """
        Ï¥àÍ∏∞Ìôî
        
        Args:
            threshold (float): Ïú†ÏÇ¨ ÏùëÎãµÏúºÎ°ú Î∂ÑÎ•òÌï† ÏûÑÍ≥ÑÍ∞í (0~1)
            use_transformer (bool): SentenceTransformer Î™®Îç∏ ÏÇ¨Ïö© Ïó¨Î∂Ä
        """
        self.threshold = threshold
        self.use_transformer = use_transformer
        
        # Îã§Íµ≠Ïñ¥ SentenceTransformer Î™®Îç∏ Î°úÎìú
        if use_transformer:
            try:
                self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                logger.info("Îã§Íµ≠Ïñ¥ SentenceTransformer Î™®Îç∏ Î°úÎìú ÏôÑÎ£å")
            except Exception as e:
                logger.error(f"SentenceTransformer Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {str(e)}")
                self.use_transformer = False
                
        # FallbackÏö© TF-IDF Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä
        from sklearn.feature_extraction.text import TfidfVectorizer
        self.vectorizer = TfidfVectorizer(
            min_df=1, 
            analyzer='word',
            ngram_range=(1, 2),
            stop_words=None  # Îã§Íµ≠Ïñ¥ ÏßÄÏõêÏùÑ ÏúÑÌï¥ stop_words Ï†úÍ±∞
        )
    
    def preprocess_text(self, text: str) -> str:
        """
        ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨
        
        Args:
            text (str): Ï†ÑÏ≤òÎ¶¨Ìï† ÌÖçÏä§Ìä∏
            
        Returns:
            str: Ï†ÑÏ≤òÎ¶¨Îêú ÌÖçÏä§Ìä∏
        """
        # ÏÜåÎ¨∏Ïûê Î≥ÄÌôò (ÏòÅÏñ¥ ÌÖçÏä§Ìä∏Îßå Ìï¥Îãπ)
        # Îã§Íµ≠Ïñ¥ ÏßÄÏõêÏùÑ ÏúÑÌï¥ ÏòÅÏñ¥Í∞Ä ÏïÑÎãå Í≤ΩÏö∞ ÏõêÎûò ÏºÄÏù¥Ïä§ Ïú†ÏßÄ
        if text.isascii():
            text = text.lower()
        
        # ÏΩîÎìú Î∏îÎ°ù Ï†úÍ±∞ (Î∂ÑÏÑùÏóêÏÑú Ï†úÏô∏)
        text = re.sub(r'```.*?```', ' CODE_BLOCK ', text, flags=re.DOTALL)
        
        # HTML ÌÉúÍ∑∏ Ï†úÍ±∞
        text = re.sub(r'<.*?>', '', text)
        
        # ÌäπÏàò Î¨∏Ïûê Ï≤òÎ¶¨ (Îã§Íµ≠Ïñ¥ ÏßÄÏõêÏùÑ ÏúÑÌï¥ ÏôÑÏ†Ñ Ï†úÍ±∞ÌïòÏßÄ ÏïäÏùå)
        text = re.sub(r'[^\w\s\u0080-\uFFFF]', ' ', text)
        
        # Ïó¨Îü¨ Í≥µÎ∞±ÏùÑ ÌïòÎÇòÎ°ú ÏπòÌôò
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def calculate_similarity_matrix(self, responses: Dict[str, str]) -> Dict[str, Dict[str, float]]:
        """
        Î™®Îç∏ ÏùëÎãµ Í∞ÑÏùò Ïú†ÏÇ¨ÎèÑ ÌñâÎ†¨ Í≥ÑÏÇ∞
        
        Args:
            responses (dict): Î™®Îç∏ IDÎ•º ÌÇ§Î°ú, ÏùëÎãµ ÌÖçÏä§Ìä∏Î•º Í∞íÏúºÎ°ú ÌïòÎäî ÎîïÏÖîÎÑàÎ¶¨
            
        Returns:
            dict: Î™®Îç∏ Í∞Ñ Ïú†ÏÇ¨ÎèÑ ÌñâÎ†¨
        """
        try:
            model_ids = list(responses.keys())
            
            # ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨
            preprocessed_texts = [self.preprocess_text(responses[model_id]) for model_id in model_ids]
            
            if self.use_transformer and self.model:
                # SentenceTransformerÎ•º ÏÇ¨Ïö©Ìïú ÏûÑÎ≤†Îî© ÏÉùÏÑ±
                try:
                    embeddings = self.model.encode(preprocessed_texts)
                    # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
                    similarity_matrix = cosine_similarity(embeddings)
                except Exception as e:
                    logger.error(f"SentenceTransformer ÏûÑÎ≤†Îî© ÏÉùÏÑ± Ï§ë Ïò§Î•ò: {str(e)}")
                    # Fallback: TF-IDF ÏÇ¨Ïö©
                    tfidf_matrix = self.vectorizer.fit_transform(preprocessed_texts)
                    similarity_matrix = cosine_similarity(tfidf_matrix)
            else:
                # TF-IDF Î≤°ÌÑ∞Ìôî
                tfidf_matrix = self.vectorizer.fit_transform(preprocessed_texts)
                # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
                similarity_matrix = cosine_similarity(tfidf_matrix)
            
            # Í≤∞Í≥ºÎ•º ÎîïÏÖîÎÑàÎ¶¨Î°ú Î≥ÄÌôò
            result = {}
            for i, model1 in enumerate(model_ids):
                result[model1] = {}
                for j, model2 in enumerate(model_ids):
                    result[model1][model2] = float(similarity_matrix[i][j])
            
            return result
            
        except Exception as e:
            logger.error(f"Ïú†ÏÇ¨ÎèÑ ÌñâÎ†¨ Í≥ÑÏÇ∞ Ï§ë Ïò§Î•ò: {str(e)}")
            # Ïò§Î•ò Î∞úÏÉù Ïãú Îπà ÌñâÎ†¨ Î∞òÌôò
            return {model_id: {other_id: 0.0 for other_id in responses} for model_id in responses}
    
      
    def cluster_responses(self, responses):
        """
        ÏùëÎãµÏùÑ Ïú†ÏÇ¨ÎèÑÏóê Îî∞Îùº Íµ∞ÏßëÌôî
        
        Args:
            responses (dict): Î™®Îç∏ IDÎ•º ÌÇ§Î°ú, ÏùëÎãµ ÌÖçÏä§Ìä∏Î•º Í∞íÏúºÎ°ú ÌïòÎäî ÎîïÏÖîÎÑàÎ¶¨
            
        Returns:
            dict: Íµ∞ÏßëÌôî Í≤∞Í≥º
        """
        try:
            model_ids = list(responses.keys())
            if len(model_ids) <= 1:
                return {
                    "similarGroups": [model_ids],
                    "outliers": [],
                    "similarityMatrix": {}
                }
            
            # Ïú†ÏÇ¨ÎèÑ ÌñâÎ†¨ Í≥ÑÏÇ∞
            similarity_matrix = self.calculate_similarity_matrix(responses)
            
            # Í≥ÑÏ∏µÏ†Å ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ ÏàòÌñâ
            clusters = [[model_id] for model_id in model_ids]
            
            merge_happened = True
            while merge_happened and len(clusters) > 1:
                merge_happened = False
                max_similarity = -1
                merge_indices = [-1, -1]
                
                # Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Îëê ÌÅ¥Îü¨Ïä§ÌÑ∞ Ï∞æÍ∏∞
                for i in range(len(clusters)):
                    for j in range(i + 1, len(clusters)):
                        # Îëê ÌÅ¥Îü¨Ïä§ÌÑ∞ Í∞Ñ ÌèâÍ∑† Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
                        cluster_similarity = 0
                        pair_count = 0
                        
                        for model1 in clusters[i]:
                            for model2 in clusters[j]:
                                cluster_similarity += similarity_matrix[model1][model2]
                                pair_count += 1
                        
                        avg_similarity = cluster_similarity / max(1, pair_count)
                        
                        if avg_similarity > max_similarity:
                            max_similarity = avg_similarity
                            merge_indices = [i, j]
                
                # ÏûÑÍ≥ÑÍ∞íÎ≥¥Îã§ Ïú†ÏÇ¨ÎèÑÍ∞Ä ÎÜíÏúºÎ©¥ ÌÅ¥Îü¨Ïä§ÌÑ∞ Î≥ëÌï©
                if max_similarity >= self.threshold:
                    i, j = merge_indices
                    clusters[i].extend(clusters[j])
                    clusters.pop(j)
                    merge_happened = True
            
            # ÌÅ¥Îü¨Ïä§ÌÑ∞ ÌÅ¨Í∏∞Ïóê Îî∞Îùº Ï†ïÎ†¨
            clusters.sort(key=lambda x: -len(x))
            
            # Ï£ºÏöî Í∑∏Î£πÍ≥º Ïù¥ÏÉÅÏπò Íµ¨Î∂Ñ
            main_group = clusters[0] if clusters else []
            outliers = [model for cluster in clusters[1:] for model in cluster]
            
            # ÏùëÎãµ ÌäπÏÑ± Ï∂îÏ∂ú
            response_features = {model_id: self.extract_response_features(responses[model_id]) 
                                for model_id in model_ids}
            
            return {
                "similarGroups": clusters,
                "mainGroup": main_group,
                "outliers": outliers,
                "similarityMatrix": similarity_matrix,
                "responseFeatures": response_features
            }
            
        except Exception as e:
            logger.error(f"ÏùëÎãµ Íµ∞ÏßëÌôî Ï§ë Ïò§Î•ò: {str(e)}")
            # Ïò§Î•ò Î∞úÏÉù Ïãú Î™®Îì† Î™®Îç∏ÏùÑ ÌïòÎÇòÏùò Í∑∏Î£πÏúºÎ°ú Î∞òÌôò
            return {
                "similarGroups": [model_ids],
                "mainGroup": model_ids,
                "outliers": [],
                "similarityMatrix": {},
                "responseFeatures": {}
            }
    
    
    def extract_response_features(self, text: str) -> Dict[str, Union[int, float, bool]]:
        """
        ÏùëÎãµ ÌÖçÏä§Ìä∏ÏóêÏÑú ÌäπÏÑ± Ï∂îÏ∂ú
        
        Args:
            text (str): ÏùëÎãµ ÌÖçÏä§Ìä∏
            
        Returns:
            dict: ÏùëÎãµ ÌäπÏÑ± Ï†ïÎ≥¥
        """
        try:
            # ÏùëÎãµ Í∏∏Ïù¥
            length = len(text)
            
            # ÏΩîÎìú Î∏îÎ°ù Í∞úÏàò
            code_blocks = re.findall(r'```[\s\S]*?```', text)
            code_block_count = len(code_blocks)
            
            # ÎßÅÌÅ¨ Í∞úÏàò
            links = re.findall(r'\[.*?\]\(.*?\)', text) or re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)
            link_count = len(links)
            
            # Î™©Î°ù Ìï≠Î™© Í∞úÏàò
            list_items = re.findall(r'^[\s]*[-*+] |^[\s]*\d+\. ', text, re.MULTILINE)
            list_item_count = len(list_items)
            
            # Î¨∏Ïû• Î∂ÑÎ¶¨ (Îã§Íµ≠Ïñ¥ ÏßÄÏõê)
            sentences = re.split(r'[.!?„ÄÇÔºÅÔºü]+', text)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            # ÌèâÍ∑† Î¨∏Ïû• Í∏∏Ïù¥
            avg_sentence_length = sum(len(s) for s in sentences) / max(1, len(sentences))
            
            # Ïñ¥Ìúò Îã§ÏñëÏÑ± (Í≥†Ïú† Îã®Ïñ¥ ÎπÑÏú®)
            words = re.findall(r'\b\w+\b', text.lower())
            unique_words = set(words)
            vocabulary_diversity = len(unique_words) / max(1, len(words))
            
            # Ïñ∏Ïñ¥ Í∞êÏßÄ (Ï∂îÍ∞Ä Í∏∞Îä•)
            lang_features = self.detect_language_features(text)
            
            features = {
                "length": length,
                "codeBlockCount": code_block_count,
                "linkCount": link_count,
                "listItemCount": list_item_count,
                "sentenceCount": len(sentences),
                "avgSentenceLength": avg_sentence_length,
                "vocabularyDiversity": vocabulary_diversity,
                "hasCode": code_block_count > 0
            }
            
            # Ïñ∏Ïñ¥ ÌäπÏÑ± Ï∂îÍ∞Ä
            features.update(lang_features)
            
            return features
            
        except Exception as e:
            logger.error(f"ÏùëÎãµ ÌäπÏÑ± Ï∂îÏ∂ú Ï§ë Ïò§Î•ò: {str(e)}")
            # Ïò§Î•ò Î∞úÏÉù Ïãú Í∏∞Î≥∏Í∞í Î∞òÌôò
            return {
                "length": len(text),
                "codeBlockCount": 0,
                "linkCount": 0,
                "listItemCount": 0,
                "sentenceCount": 1,
                "avgSentenceLength": len(text),
                "vocabularyDiversity": 0,
                "hasCode": False,
                "detectedLang": "unknown"
            }
    
    def detect_language_features(self, text: str) -> Dict[str, Any]:
        """
        ÌÖçÏä§Ìä∏Ïùò Ïñ∏Ïñ¥ ÌäπÏÑ± Í∞êÏßÄ
        
        Args:
            text (str): Î∂ÑÏÑùÌï† ÌÖçÏä§Ìä∏
            
        Returns:
            dict: Ïñ∏Ïñ¥ ÌäπÏÑ± Ï†ïÎ≥¥
        """
        try:
            # Ïñ∏Ïñ¥ ÌäπÏÑ± Í∞êÏßÄÎ•º ÏúÑÌïú Í∞ÑÎã®Ìïú Ìú¥Î¶¨Ïä§Ìã±
            # Ïã§Ï†ú ÌîÑÎ°úÎçïÏÖòÏóêÏÑúÎäî langdetect Îì±Ïùò ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÇ¨Ïö© Í∂åÏû•
            
            # ÌïúÍµ≠Ïñ¥ ÌäπÏÑ± (ÌïúÍ∏Ä ÎπÑÏú®)
            korean_chars = len(re.findall(r'[„Ñ±-„Öé„Öè-„Ö£Í∞Ä-Ìû£]', text))
            
            # ÏòÅÏñ¥ ÌäπÏÑ± (ÏòÅÎ¨∏ ÎπÑÏú®)
            english_chars = len(re.findall(r'[a-zA-Z]', text))
            
            # ÏùºÎ≥∏Ïñ¥ ÌäπÏÑ± (ÏùºÎ≥∏Ïñ¥ Î¨∏Ïûê ÎπÑÏú®)
            japanese_chars = len(re.findall(r'[„ÅÅ-„Çì„Ç°-„É≥‰∏Ä-ÈæØ]', text))
            
            # Ï§ëÍµ≠Ïñ¥ ÌäπÏÑ± (Ï§ëÍµ≠Ïñ¥ Î¨∏Ïûê ÎπÑÏú®)
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
            
            # Í∏∞ÌÉÄ Î¨∏Ïûê (Ïà´Ïûê, ÌäπÏàòÎ¨∏Ïûê Ï†úÏô∏)
            total_chars = len(re.findall(r'[^\d\s\W]', text))
            
            # ÎπÑÏú® Í≥ÑÏÇ∞
            total = max(1, total_chars)
            korean_ratio = korean_chars / total
            english_ratio = english_chars / total
            japanese_ratio = japanese_chars / total
            chinese_ratio = chinese_chars / total
            
            # Ï£ºÏöî Ïñ∏Ïñ¥ Í≤∞Ï†ï
            lang_ratios = {
                "ko": korean_ratio,
                "en": english_ratio,
                "ja": japanese_ratio,
                "zh": chinese_ratio,
                "other": 1.0 - (korean_ratio + english_ratio + japanese_ratio + chinese_ratio)
            }
            
            detected_lang = max(lang_ratios.items(), key=lambda x: x[1])[0]
            
            return {
                "detectedLang": detected_lang,
                "langRatios": lang_ratios
            }
            
        except Exception as e:
            logger.error(f"Ïñ∏Ïñ¥ ÌäπÏÑ± Í∞êÏßÄ Ï§ë Ïò§Î•ò: {str(e)}")
            return {
                "detectedLang": "unknown",
                "langRatios": {"unknown": 1.0}
            }
    
    def compare_responses(self, response1: str, response2: str) -> Dict[str, Any]:
        """
        Îëê ÏùëÎãµ Í∞ÑÏùò Ïú†ÏÇ¨ÎèÑÏôÄ Ï∞®Ïù¥Ï†ê Î∂ÑÏÑù
        
        Args:
            response1 (str): Ï≤´ Î≤àÏß∏ ÏùëÎãµ
            response2 (str): Îëê Î≤àÏß∏ ÏùëÎãµ
            
        Returns:
            dict: Ïú†ÏÇ¨ÎèÑ Î∞è Ï∞®Ïù¥Ï†ê Î∂ÑÏÑù Í≤∞Í≥º
        """
        try:
            # ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨
            text1 = self.preprocess_text(response1)
            text2 = self.preprocess_text(response2)
            
            # ÏûÑÎ≤†Îî© ÏÉùÏÑ± Î∞è Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
            if self.use_transformer and self.model:
                embeddings = self.model.encode([text1, text2])
                similarity = float(cosine_similarity([embeddings[0]], [embeddings[1]])[0][0])
            else:
                # TF-IDFÎ•º ÏÇ¨Ïö©Ìïú Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
                tfidf_matrix = self.vectorizer.fit_transform([text1, text2])
                similarity = float(cosine_similarity(tfidf_matrix)[0][1])
            
            # ÏùëÎãµ ÌäπÏÑ± ÎπÑÍµê
            features1 = self.extract_response_features(response1)
            features2 = self.extract_response_features(response2)
            
            # ÌäπÏÑ± Ï∞®Ïù¥ Í≥ÑÏÇ∞
            feature_diffs = {}
            for key in set(features1.keys()) & set(features2.keys()):
                if isinstance(features1[key], (int, float)) and isinstance(features2[key], (int, float)):
                    feature_diffs[key] = features2[key] - features1[key]
            
            # Ï£ºÏöî Ï∞®Ïù¥Ï†ê Í≥†Ïú† Îã®Ïñ¥ Î∂ÑÏÑù
            words1 = re.findall(r'\b\w+\b', text1.lower())
            words2 = re.findall(r'\b\w+\b', text2.lower())
            
            counter1 = Counter(words1)
            counter2 = Counter(words2)
            
            unique_to_1 = [word for word, count in counter1.items() if word not in counter2]
            unique_to_2 = [word for word, count in counter2.items() if word not in counter1]
            
            # Í∞ÄÏû• ÎπàÎèÑÍ∞Ä ÎÜíÏùÄ Í≥†Ïú† Îã®Ïñ¥ (ÏµúÎåÄ 10Í∞ú)
            top_unique_to_1 = sorted(
                [(word, counter1[word]) for word in unique_to_1], 
                key=lambda x: x[1], 
                reverse=True
            )[:10]
            
            top_unique_to_2 = sorted(
                [(word, counter2[word]) for word in unique_to_2], 
                key=lambda x: x[1], 
                reverse=True
            )[:10]
            
            return {
                "similarity": similarity,
                "isSimilar": similarity >= self.threshold,
                "features1": features1,
                "features2": features2,
                "featureDiffs": feature_diffs,
                "uniqueWordsTo1": top_unique_to_1,
                "uniqueWordsTo2": top_unique_to_2
            }
            
        except Exception as e:
            logger.error(f"ÏùëÎãµ ÎπÑÍµê Ï§ë Ïò§Î•ò: {str(e)}")
            return {
                "similarity": 0.0,
                "isSimilar": False,
                "error": str(e)
            }
        
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
import logging
import json
import openai
import anthropic
from groq import Groq
from django.conf import settings
import time


logger = logging.getLogger(__name__)

class TextSimplificationView(APIView):
    """
    ÌÖçÏä§Ìä∏Î•º Ïâ¨Ïö¥ ÌëúÌòÑÏúºÎ°ú Î≥ÄÌôòÌïòÎäî API Î∑∞
    ÌäπÏ†ï ÎåÄÏÉÅ(Ïñ¥Î¶∞Ïù¥, Í≥†Î†πÏûê, Ïô∏Íµ≠Ïù∏ ÌïôÏäµÏûê Îì±)Ïóê ÎßûÏ∂∞ Î≥ÄÌôò
    """
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            logger.info("Ïâ¨Ïö¥ ÌëúÌòÑ Î≥ÄÌôò ÏöîÏ≤≠ Î∞õÏùå")
            
            data = request.data
            original_text = data.get('message')
            target_audience = data.get('targetAudience', 'general')
            language = data.get('language', 'ko')
            
            if not original_text:
                return Response({'error': 'Î≥ÄÌôòÌï† ÌÖçÏä§Ìä∏Í∞Ä ÏóÜÏäµÎãàÎã§.'}, 
                               status=status.HTTP_400_BAD_REQUEST)
            
            # ÌÖçÏä§Ìä∏ Îã®ÏàúÌôî ÏàòÌñâ
            simplifier = TextSimplifier(
                api_key=settings.OPENAI_API_KEY,
                model="gpt-4-turbo",  # ÎòêÎäî ÏÑ†Ìò∏ÌïòÎäî GPT Î™®Îç∏
                api_type="openai"
            )
            
            result = simplifier.simplify_text(
                original_text=original_text,
                target_audience=target_audience,
                language=language
            )
            
            return Response(result)
            
        except Exception as e:
            logger.error(f"ÌÖçÏä§Ìä∏ Îã®ÏàúÌôî Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}", exc_info=True)
            return Response({
                'error': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class TextSimplifier:
    """
    ÌÖçÏä§Ìä∏Î•º Ïâ¨Ïö¥ ÌëúÌòÑÏúºÎ°ú Î≥ÄÌôòÌïòÎäî ÌÅ¥ÎûòÏä§
    Îã§ÏñëÌïú AI Î™®Îç∏ÏùÑ ÌôúÏö©ÌïòÏó¨ ÎåÄÏÉÅÏûêÎ≥Ñ ÎßûÏ∂§Ìòï Îã®ÏàúÌôî ÏàòÌñâ
    """
    def __init__(self, api_key, model, api_type):
        self.model = model
        self.api_type = api_type
        self.api_key = api_key
        
        if api_type == 'openai':
            openai.api_key = api_key
        elif api_type == 'anthropic':
            self.client = anthropic.Anthropic(api_key=api_key)
        elif api_type == 'groq':
            self.client = Groq(api_key=api_key)
    
    def simplify_text(self, original_text, target_audience, language='ko'):
        """
        ÌÖçÏä§Ìä∏Î•º Îã®ÏàúÌôîÌïòÏó¨ Î∞òÌôò
        
        Args:
            original_text (str): ÏõêÎ≥∏ ÌÖçÏä§Ìä∏
            target_audience (str): ÎåÄÏÉÅÏûê Ïú†Ìòï (general, child, elderly, foreigner)
            language (str): Ïñ∏Ïñ¥ (Í∏∞Î≥∏Í∞í: ÌïúÍµ≠Ïñ¥)
            
        Returns:
            dict: Îã®ÏàúÌôî Í≤∞Í≥º
        """
        try:
            logger.info(f"ÌÖçÏä§Ìä∏ Îã®ÏàúÌôî ÏãúÏûë: ÎåÄÏÉÅ={target_audience}, Ïñ∏Ïñ¥={language}")
            
            # ÎåÄÏÉÅÏûêÏóê ÎßûÎäî ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±
            prompt = self._get_simplification_prompt(original_text, target_audience, language)
            
            # AI Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÌÖçÏä§Ìä∏ Îã®ÏàúÌôî
            simplified_text = self._generate_simplified_text(prompt)
            
            # Í≤∞Í≥º Î∞òÌôò
            result = {
                'original_text': original_text,
                'simplified_text': simplified_text,
                'target_audience': target_audience,
                'language': language,
                'timestamp': time.time()
            }
            
            logger.info("ÌÖçÏä§Ìä∏ Îã®ÏàúÌôî ÏôÑÎ£å")
            return result
            
        except Exception as e:
            logger.error(f"ÌÖçÏä§Ìä∏ Îã®ÏàúÌôî Ïò§Î•ò: {str(e)}", exc_info=True)
            raise
    
    def _get_simplification_prompt(self, original_text, target_audience, language):
        """ÎåÄÏÉÅÏûê ÎßûÏ∂§Ìòï Îã®ÏàúÌôî ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±"""
        
        base_prompt = f"""
Îã§Ïùå ÌÖçÏä§Ìä∏Î•º Îçî Ïâ¨Ïö¥ ÌëúÌòÑÏúºÎ°ú Î≥ÄÌôòÌï¥Ï£ºÏÑ∏Ïöî:

{original_text}

ÎåÄÏÉÅÏûê: {target_audience}
Ïñ∏Ïñ¥: {language}
"""
        
        if target_audience == 'child':
            base_prompt += """
[Ïñ¥Î¶∞Ïù¥Ïö© Î≥ÄÌôò ÏßÄÏπ®]
1. 7-12ÏÑ∏ Ïñ¥Î¶∞Ïù¥Í∞Ä Ïù¥Ìï¥Ìï† Ïàò ÏûàÎäî Îã®Ïñ¥ÏôÄ ÌëúÌòÑÏúºÎ°ú Î≥ÄÌôòÌïòÏÑ∏Ïöî.
2. ÏßßÍ≥† Í∞ÑÎã®Ìïú Î¨∏Ïû•ÏùÑ ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
3. Ï∂îÏÉÅÏ†ÅÏù∏ Í∞úÎÖêÏùÄ Íµ¨Ï≤¥Ï†ÅÏù∏ ÏòàÏãúÏôÄ Ìï®Íªò ÏÑ§Î™ÖÌïòÏÑ∏Ïöî.
4. Ïû¨ÎØ∏ÏûàÍ≥† Ìù•ÎØ∏Î°úÏö¥ ÌëúÌòÑÏùÑ ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
5. Ïñ¥Î†§Ïö¥ Îã®Ïñ¥Îäî Í∞ÑÎã®Ìïú ÎèôÏùòÏñ¥Î°ú ÎåÄÏ≤¥ÌïòÏÑ∏Ïöî.
6. ÌïÑÏöîÌïú Í≤ΩÏö∞ ÎπÑÏú†ÏôÄ ÏòàÏãúÎ•º ÌôúÏö©ÌïòÏÑ∏Ïöî.
7. Î¨∏Ïû• ÏÇ¨Ïù¥Ïóê Ï†ÅÏ†àÌïú Ï§ÑÎ∞îÍøàÏùÑ Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî.
"""
        elif target_audience == 'elderly':
            base_prompt += """
[Í≥†Î†πÏûêÏö© Î≥ÄÌôò ÏßÄÏπ®]
1. Î™ÖÌôïÌïòÍ≥† ÏßÅÏ†ëÏ†ÅÏù∏ ÌëúÌòÑÏùÑ ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
2. Ïô∏ÎûòÏñ¥ÎÇò ÏòÅÏñ¥ ÌëúÌòÑÏùÄ Í∞ÄÎä•Ìïú ÌïúÍµ≠Ïñ¥Î°ú ÎåÄÏ≤¥ÌïòÏÑ∏Ïöî.
3. Î≥µÏû°Ìïú Î¨∏Ïû• Íµ¨Ï°∞Î•º ÌîºÌïòÍ≥† Í∞ÑÍ≤∞ÌïòÍ≤å ÏûëÏÑ±ÌïòÏÑ∏Ïöî.
4. Ï†ÑÎ¨∏ Ïö©Ïñ¥Îäî ÏùºÏÉÅÏ†ÅÏù∏ Ïö©Ïñ¥Î°ú ÏÑ§Î™ÖÌïòÏÑ∏Ïöî.
5. ÏπúÏàôÌïú ÎπÑÏú†ÏôÄ ÏòàÏãúÎ•º ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
6. Ï§ëÏöîÌïú Ï†ïÎ≥¥Îäî Î∞òÎ≥µÌï¥ÏÑú Í∞ïÏ°∞ÌïòÏÑ∏Ïöî.
7. Î¨∏Ïû• ÏÇ¨Ïù¥Ïóê Ï†ÅÏ†àÌïú Ï§ÑÎ∞îÍøàÏùÑ Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî.
"""
        elif target_audience == 'foreigner':
            base_prompt += """
[Ïô∏Íµ≠Ïù∏ ÌïôÏäµÏûêÏö© Î≥ÄÌôò ÏßÄÏπ®]
1. ÌïúÍµ≠Ïñ¥ ÌïôÏäµÏûê(Ï¥àÍ∏â~Ï§ëÍ∏â)Í∞Ä Ïù¥Ìï¥Ìï† Ïàò ÏûàÎäî Í∏∞Î≥∏ Ïñ¥ÌúòÎ•º ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
2. Í¥ÄÏö©Ïñ¥, ÏÜçÎã¥, ÏùÄÏú†Ï†Å ÌëúÌòÑÏùÑ ÌîºÌïòÏÑ∏Ïöî.
3. ÌïúÏûêÏñ¥Îäî Í∞ÄÎä•Ìïú ÏàúÏö∞Î¶¨ÎßêÎ°ú ÎåÄÏ≤¥ÌïòÏÑ∏Ïöî.
4. Î¨∏Î≤ïÏ†ÅÏúºÎ°ú Îã®ÏàúÌïú Î¨∏Ïû• Íµ¨Ï°∞Î•º ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
5. Î≥µÏû°Ìïú Ïó∞Í≤∞Ïñ¥ÎØ∏ÎÇò Ï°∞ÏÇ¨ ÏÇ¨Ïö©ÏùÑ ÏµúÏÜåÌôîÌïòÏÑ∏Ïöî.
6. Ï§ëÏöîÌïú Í∞úÎÖêÏùÄ Í¥ÑÌò∏ ÏïàÏóê ÏòÅÏñ¥Î°ú Î≥ëÍ∏∞ÌïòÏÑ∏Ïöî.
7. Î¨∏Ïû• ÏÇ¨Ïù¥Ïóê Ï†ÅÏ†àÌïú Ï§ÑÎ∞îÍøàÏùÑ Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî.
"""
        else:  # general
            base_prompt += """
[ÏùºÎ∞òÏù∏Ïö© Î≥ÄÌôò ÏßÄÏπ®]
1. Î≥¥Ìé∏Ï†ÅÏù∏ ÍµêÏñë ÏàòÏ§ÄÏùò Ïñ¥ÌúòÏôÄ ÌëúÌòÑÏùÑ ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
2. Î∂àÌïÑÏöîÌïòÍ≤å Î≥µÏû°Ìïú Î¨∏Ïû• Íµ¨Ï°∞Î•º Îã®ÏàúÌôîÌïòÏÑ∏Ïöî.
3. Ï†ÑÎ¨∏ Ïö©Ïñ¥Îäî Í∞ÑÎã®Ìïú ÏÑ§Î™ÖÍ≥º Ìï®Íªò ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
4. ÎÖºÎ¶¨Ï†Å ÌùêÎ¶ÑÏùÑ Ïú†ÏßÄÌïòÎ©∞ Î™ÖÌôïÌïòÍ≤å ÌëúÌòÑÌïòÏÑ∏Ïöî.
5. ÎπÑÏú†ÏôÄ ÏòàÏãúÎ•º Ï†ÅÏ†àÌûà ÌôúÏö©ÌïòÏÑ∏Ïöî.
6. Ï§ëÏöîÌïú ÎÇ¥Ïö©ÏùÑ Í∞ïÏ°∞ÌïòÍ≥† ÌïµÏã¨ÏùÑ Î®ºÏ†Ä Ï†úÏãúÌïòÏÑ∏Ïöî.
7. Î¨∏Ïû• ÏÇ¨Ïù¥Ïóê Ï†ÅÏ†àÌïú Ï§ÑÎ∞îÍøàÏùÑ Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî.
"""
            
        return base_prompt
    
    def _generate_simplified_text(self, prompt):
        """AI Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Îã®ÏàúÌôîÎêú ÌÖçÏä§Ìä∏ ÏÉùÏÑ±"""
        try:
            # API Ïú†ÌòïÏóê Îî∞Î•∏ Î∂ÑÍ∏∞
            if self.api_type == 'openai':
                response = openai.ChatCompletion.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ÎãπÏã†ÏùÄ Î≥µÏû°Ìïú ÌÖçÏä§Ìä∏Î•º Îçî ÏâΩÍ≤å Ïù¥Ìï¥Ìï† Ïàò ÏûàÎäî ÌòïÌÉúÎ°ú Î≥ÄÌôòÌï¥Ï£ºÎäî Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.5,
                    max_tokens=2000
                )
                simplified_text = response['choices'][0]['message']['content']
                
            elif self.api_type == 'anthropic':
                message = self.client.messages.create(
                    model=self.model,
                    max_tokens=2000,
                    temperature=0.5,
                    system="ÎãπÏã†ÏùÄ Î≥µÏû°Ìïú ÌÖçÏä§Ìä∏Î•º Îçî ÏâΩÍ≤å Ïù¥Ìï¥Ìï† Ïàò ÏûàÎäî ÌòïÌÉúÎ°ú Î≥ÄÌôòÌï¥Ï£ºÎäî Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§.",
                    messages=[{
                        "role": "user", 
                        "content": prompt
                    }]
                )
                simplified_text = message.content[0].text
                
            elif self.api_type == 'groq':
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ÎãπÏã†ÏùÄ Î≥µÏû°Ìïú ÌÖçÏä§Ìä∏Î•º Îçî ÏâΩÍ≤å Ïù¥Ìï¥Ìï† Ïàò ÏûàÎäî ÌòïÌÉúÎ°ú Î≥ÄÌôòÌï¥Ï£ºÎäî Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.5,
                    max_tokens=2000
                )
                simplified_text = response.choices[0].message.content
            
            return simplified_text
            
        except Exception as e:
            logger.error(f"ÌÖçÏä§Ìä∏ ÏÉùÏÑ± Ïò§Î•ò: {str(e)}", exc_info=True)
            raise




    
import logging
import json
import os
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
from PIL import Image, ImageFilter, ImageEnhance
import pytesseract
from .models import OCRResult
from .serializers import OCRResultSerializer

import PyPDF2
import tempfile
from pdf2image import convert_from_path
import re

logger = logging.getLogger(__name__)

# OllamaClientÏôÄ GPTTranslator ÌÅ¥ÎûòÏä§ Í∞ÄÏ†∏Ïò§Í∏∞
from .ollama_client import OllamaClient
from .gpt_translator import GPTTranslator 

@method_decorator(csrf_exempt, name='dispatch')
class ProcessFileView(APIView):
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            logger.info("ProcessFileView ÏöîÏ≤≠ ÏàòÏã†: %s %s", request.method, request.path)
            
            # ÏöîÏ≤≠ Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏
            if 'file' not in request.FILES:
                logger.error("ÌååÏùºÏù¥ Ï†úÍ≥µÎêòÏßÄ ÏïäÏùå")
                return Response({'error': 'ÌååÏùºÏù¥ Ï†úÍ≥µÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§'}, status=status.HTTP_400_BAD_REQUEST)
            
            file_obj = request.FILES['file']
            file_name = file_obj.name.lower()
            logger.info("ÌååÏùº ÏóÖÎ°úÎìú: %s, ÌÅ¨Í∏∞: %s bytes", file_name, file_obj.size)
            
            # Ollama ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî
            ollama_base_url = os.environ.get('OLLAMA_API_URL', 'http://localhost:11434')
            ollama_client = OllamaClient(base_url=ollama_base_url)
            
            # GPT Î≤àÏó≠Í∏∞ Ï¥àÍ∏∞Ìôî
            gpt_translator = GPTTranslator()
            
            # Î≤àÏó≠ ÏòµÏÖò ÌôïÏù∏ (Í∏∞Î≥∏Í∞í: True)
            enable_translation = request.data.get('enable_translation', 'true').lower() == 'true'
            
            # ÌååÏùº Ïú†Ìòï ÌôïÏù∏
            if file_name.endswith(('.pdf')):
                file_type = 'pdf'
                
                # PDF ÌéòÏù¥ÏßÄ Î≤îÏúÑ ÌôïÏù∏
                start_page = int(request.data.get('start_page', 1))
                end_page = int(request.data.get('end_page', 0))  # 0ÏùÄ Ï†ÑÏ≤¥ ÌéòÏù¥ÏßÄÎ•º ÏùòÎØ∏
                
                logger.info("PDF Ï≤òÎ¶¨ Î≤îÏúÑ: %s ~ %s ÌéòÏù¥ÏßÄ", start_page, end_page if end_page > 0 else "ÎÅù")
                
            elif file_name.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif')):
                file_type = 'image'
            else:
                logger.error("ÏßÄÏõêÎêòÏßÄ ÏïäÎäî ÌååÏùº ÌòïÏãù: %s", file_name)
                return Response({'error': 'ÏßÄÏõêÎêòÏßÄ ÏïäÎäî ÌååÏùº ÌòïÏãùÏûÖÎãàÎã§. Ïù¥ÎØ∏ÏßÄÎÇò PDF ÌååÏùºÏùÑ ÏóÖÎ°úÎìúÌï¥Ï£ºÏÑ∏Ïöî.'},
                              status=status.HTTP_400_BAD_REQUEST)
            
            # OCR Í≤∞Í≥º Í∞ùÏ≤¥ ÏÉùÏÑ±
            ocr_result = OCRResult.objects.create(file=file_obj, file_type=file_type)
            logger.info("OCRResult Í∞ùÏ≤¥ ÏÉùÏÑ±: %s", ocr_result.id)
            
            # OCR Ï≤òÎ¶¨
            try:
                ocr_text = ""
                page_texts = []  # ÌéòÏù¥ÏßÄÎ≥Ñ ÌÖçÏä§Ìä∏ Ï†ÄÏû•
                
                if file_type == 'image':
                    # Ïù¥ÎØ∏ÏßÄ ÌååÏùº Ï≤òÎ¶¨ - Í∞úÏÑ†Îêú OCR Ï†ÅÏö©
                    img = Image.open(ocr_result.file.path)
                    # Ïù¥ÎØ∏ÏßÄ Ï†ïÎ≥¥ Î°úÍπÖ
                    logger.info(f"Ïù¥ÎØ∏ÏßÄ Ï†ïÎ≥¥: ÌÅ¨Í∏∞={img.size}, Î™®Îìú={img.mode}, Ìè¨Îß∑={img.format}")
                    
                    # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Î∞è OCR ÏàòÌñâ - OllamaClient Î©îÏÑúÎìú ÏÇ¨Ïö©
                    preprocessed_img = ollama_client.preprocess_image_for_ocr(img)
                    ocr_text = ollama_client.get_optimized_ocr_text(preprocessed_img, lang='kor+eng')
                    page_texts.append({"page": 1, "text": ocr_text})
                    logger.info("Ïù¥ÎØ∏ÏßÄ OCR Ï≤òÎ¶¨ ÏôÑÎ£å, Ï∂îÏ∂ú ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: %s", len(ocr_text))
                    logger.info("Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏ ÏÉòÌîå: %s", ocr_text[:200] if ocr_text else "ÌÖçÏä§Ìä∏ ÏóÜÏùå")
                
                elif file_type == 'pdf':
                    # PDF Ï≤òÎ¶¨ - ÏßÅÏ†ë Ï∂îÏ∂ú ÌõÑ ÌïÑÏöîÏãú OCR
                    logger.info("PDF Ï≤òÎ¶¨ ÏãúÏûë: %s", ocr_result.file.path)
                    
                    # ÏßÅÏ†ë ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú ÏãúÎèÑ (ÌéòÏù¥ÏßÄÎ≥Ñ)
                    direct_extract_success = False
                    try:
                        all_page_texts = self.extract_text_from_pdf_by_pages(ocr_result.file.path)
                        
                        # ÌéòÏù¥ÏßÄ Î≤îÏúÑ Ï≤òÎ¶¨
                        if start_page > 1 or (end_page > 0 and end_page < len(all_page_texts)):
                            if start_page <= len(all_page_texts):
                                if end_page > 0 and end_page >= start_page:
                                    page_texts = all_page_texts[start_page-1:end_page]
                                else:
                                    page_texts = all_page_texts[start_page-1:]
                            else:
                                page_texts = []
                        else:
                            page_texts = all_page_texts
                        
                        combined_text = "\n".join([page["text"] for page in page_texts])
                        
                        # Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏ Í∏∏Ïù¥Í∞Ä Ï∂©Î∂ÑÌïúÏßÄ ÌôïÏù∏ (Îçî ÏóÑÍ≤©Ìïú Ï°∞Í±¥)
                        if combined_text.strip() and len(combined_text.strip()) >= 50:
                            meaningful_chars = sum(1 for c in combined_text if c.isalnum())
                            if meaningful_chars > 30:  # ÏùòÎØ∏ÏûàÎäî Í∏ÄÏûêÍ∞Ä 30Ïûê Ïù¥ÏÉÅÏù¥Î©¥ ÏÑ±Í≥µÏúºÎ°ú Í∞ÑÏ£º
                                ocr_text = combined_text
                                direct_extract_success = True
                                logger.info("PDF ÏßÅÏ†ë ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú ÏÑ±Í≥µ, Ï¥ù %s ÌéòÏù¥ÏßÄ, ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: %s", 
                                          len(page_texts), len(ocr_text))
                                logger.info("Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏ ÏÉòÌîå: %s", ocr_text[:200] if ocr_text else "ÌÖçÏä§Ìä∏ ÏóÜÏùå")
                    except Exception as e:
                        logger.error(f"PDF ÏßÅÏ†ë ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ïã§Ìå®: {str(e)}")
                    
                    # ÏßÅÏ†ë ÌÖçÏä§Ìä∏ Ï∂îÏ∂úÏù¥ Ïã§Ìå®Ìïú Í≤ΩÏö∞, OCR ÏãúÎèÑ
                    if not direct_extract_success:
                        logger.info("PDF OCR Ï≤òÎ¶¨ ÏãúÏûë (ÏßÅÏ†ë Ï∂îÏ∂ú Ïã§Ìå® ÎòêÎäî ÌÖçÏä§Ìä∏ Î∂àÏ∂©Î∂Ñ)")
                        
                        # ÌéòÏù¥ÏßÄ Î≤îÏúÑ ÏÑ§Ï†ïÏúºÎ°ú OCR
                        all_page_texts = self.ocr_pdf_by_pages(ocr_result.file.path, ollama_client, start_page, end_page)
                        
                        # ÌéòÏù¥ÏßÄ Î≤îÏúÑ Ï≤òÎ¶¨ - ocr_pdf_by_pagesÏóêÏÑú Ï≤òÎ¶¨ÌñàÏúºÎØÄÎ°ú Ï†ÑÏ≤¥ ÏÇ¨Ïö©
                        page_texts = all_page_texts
                        
                        ocr_text = "\n".join([page["text"] for page in page_texts])
                        logger.info("PDF OCR Ï≤òÎ¶¨ ÏôÑÎ£å, Ï¥ù %s ÌéòÏù¥ÏßÄ, ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: %s", 
                                  len(page_texts), len(ocr_text))
                        logger.info("Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏ ÏÉòÌîå: %s", ocr_text[:200] if ocr_text else "ÌÖçÏä§Ìä∏ ÏóÜÏùå")
                
                # ÌÖçÏä§Ìä∏ Ï†ïÌôî - Í∞úÏÑ†Îêú Ìï®Ïàò ÏÇ¨Ïö©
                ocr_result.ocr_text = self.clean_text(ocr_text)
                
                # PDF ÌååÏùºÏùÄ Ìï≠ÏÉÅ ÌÖçÏä§Ìä∏ Í¥ÄÎ†® ÏûàÏùåÏúºÎ°ú ÏÑ§Ï†ï
                if file_type == 'pdf':
                    text_relevant = True
                
                # Î∂ÑÏÑù Ïú†Ìòï ÌôïÏù∏ (Í∏∞Î≥∏Í∞í: both)
                analysis_type = request.data.get('analysis_type', 'both')
                logger.info("Î∂ÑÏÑù Ïú†Ìòï: %s", analysis_type)
                
                # Í≤∞Í≥º Î≥ÄÏàò Ï¥àÍ∏∞Ìôî
                image_analysis = ""
                text_analysis = ""
                combined_analysis = ""
                
                # Î≤àÏó≠ Í≤∞Í≥º Î≥ÄÏàò Ï¥àÍ∏∞Ìôî
                translated_analysis = ""
                translation_success = False
                translation_error = ""
                
                # ÌéòÏù¥ÏßÄ Î∂ÑÌï† Î∂ÑÏÑù Ïó¨Î∂Ä ÌôïÏù∏
                analyze_by_page = request.data.get('analyze_by_page', 'true').lower() == 'true'
                
                # ÏÑ†ÌÉùÎêú Î∂ÑÏÑù Ïú†ÌòïÏóê Îî∞Îùº Ï≤òÎ¶¨
                if analysis_type in ['ollama', 'both']:
                    # Ïù¥ÎØ∏ÏßÄ ÌååÏùºÏù∏ Í≤ΩÏö∞
                    if file_type == 'image':
                        # ÏÇ¨Ïö©Ïûê Ï†ïÏùò ÌîÑÎ°¨ÌîÑÌä∏ ÏÑ§Ï†ï (ÏöîÏïΩÎêú Í∞ÑÍ≤∞Ìïú ÏÑ§Î™ÖÏùÑ ÏúÑÌï¥)
                        custom_prompt = f"""Ïù¥ÎØ∏ÏßÄÎ•º Í∞ùÍ¥ÄÏ†ÅÏúºÎ°ú Í¥ÄÏ∞∞ÌïòÍ≥† Îã§Ïùå ÏßÄÏπ®Ïóê Îî∞Îùº ÏùëÎãµÌïòÏÑ∏Ïöî:

ÌïÑÏàò Ìè¨Ìï® ÏÇ¨Ìï≠:
- Ïù¥ÎØ∏ÏßÄÏóê Ïã§Ï†úÎ°ú Î≥¥Ïù¥Îäî ÏÇ¨Îûå, ÎèôÎ¨º, Î¨ºÏ≤¥Îßå Ïñ∏Í∏â (ÏóÜÏúºÎ©¥ Ïñ∏Í∏âÌïòÏßÄ ÏïäÏùå)
- ÎßåÏïΩ ÎèôÎ¨ºÏù¥ÎùºÎ©¥, Ïñ¥Îñ§ Ï¢ÖÏùò ÎèôÎ¨ºÏù∏ÏßÄÎèÑ Ï∂úÎ†•
- ÌôïÏã§Ìûà Î≥¥Ïù¥Îäî ÏÉâÏÉÅÎßå Ïñ∏Í∏â (Î∞∞Í≤ΩÏÉâ, Ïò∑ ÏÉâÏÉÅ Îì±)
- Î™ÖÌôïÌûà Î≥¥Ïù¥Îäî ÏûêÏÑ∏ÎÇò ÏúÑÏπò Í¥ÄÍ≥Ñ (Ï†ïÎ©¥, Ï∏°Î©¥ Îì±)

Ï†àÎåÄ Ìè¨Ìï®ÌïòÏßÄ Îßê Í≤É:
- Ï∂îÏ∏°Ïù¥ÎÇò Ìï¥ÏÑù ("~Î°ú Î≥¥ÏûÖÎãàÎã§", "~Í∞ôÏäµÎãàÎã§" ÌëúÌòÑ Í∏àÏßÄ)
- Î≥¥Ïù¥ÏßÄ ÏïäÎäî Î∂ÄÎ∂ÑÏóê ÎåÄÌïú Ïñ∏Í∏â ("Î≥¥Ïù¥ÏßÄ ÏïäÎäîÎã§", "ÏóÜÎã§" Îì±Ïùò ÌëúÌòÑ Í∏àÏßÄ)
- Î∞òÎ≥µÏ†ÅÏù∏ ÏÑ§Î™Ö
- Í∞êÏ†ïÏù¥ÎÇò Î∂ÑÏúÑÍ∏∞ Î¨òÏÇ¨

ÌòïÏãù:
- 1-2Î¨∏Ïû•ÏúºÎ°ú Îß§Ïö∞ Í∞ÑÍ≤∞ÌïòÍ≤å ÏûëÏÑ±
- Îã®Ïàú ÏÇ¨Ïã§ ÎÇòÏó¥ ÌòïÏãù (Ïòà: "Ïù¥ÎØ∏ÏßÄÏóêÎäî Í≤ÄÏùÄ Î®∏Î¶¨ Ïó¨ÏÑ±Ïù¥ ÏûàÍ≥†, Î∞∞Í≤ΩÏùÄ Ìù∞ÏÉâÏù¥Îã§.")

OCR ÌÖçÏä§Ìä∏ (Ï∞∏Í≥†Ïö©, Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄÏóê Î≥¥Ïù¥Îäî Í≤ΩÏö∞Îßå Ïñ∏Í∏â): {ocr_result.ocr_text}

ÏòÅÏñ¥Î°ú Í∞ÑÍ≤∞ÌïòÍ≤å ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî."""

                        
                        # OCR ÌÖçÏä§Ìä∏Î•º Ï†ÑÎã¨ (analyze_image ÎÇ¥Î∂ÄÏóêÏÑú Í¥ÄÎ†®ÏÑ± ÌåêÎã®)
                        image_analysis = ollama_client.analyze_image(
                            ocr_result.file.path, 
                            custom_prompt,
                            ocr_text=ocr_result.ocr_text
                        )
                        
                        # OCR ÌÖçÏä§Ìä∏ Î∂ÑÏÑù (ÌÖçÏä§Ìä∏Í∞Ä ÏûàÍ≥† both Î™®ÎìúÏù∏ Í≤ΩÏö∞)
                        if ocr_result.ocr_text and analysis_type == 'both':
                            # ÌéòÏù¥ÏßÄÎ≥Ñ ÌÖçÏä§Ìä∏ Ï†ïÎ¶¨Î•º ÏúÑÌïú ÌîÑÎ°¨ÌîÑÌä∏
                            text_prompt = f"""Îã§Ïùå OCRÎ°ú Ï∂îÏ∂úÌïú ÌÖçÏä§Ìä∏Î•º ÏûêÏÑ∏Ìûà Î∂ÑÏÑùÌïòÍ≥† Î™ÖÌôïÌïòÍ≤å Ï†ïÎ¶¨Ìï¥Ï£ºÏÑ∏Ïöî:

{ocr_result.ocr_text}

Î∂ÑÏÑù ÏßÄÏπ®:
1. ÌÖçÏä§Ìä∏Ïùò Ï£ºÏöî ÎÇ¥Ïö©Í≥º Íµ¨Ï°∞Î•º ÌååÏïÖÌïòÏó¨ Ï†ïÎ¶¨
2. Îã®Ïàú ÏöîÏïΩÏù¥ ÏïÑÎãå, ÌÖçÏä§Ìä∏Ïùò ÌïµÏã¨ Ï†ïÎ≥¥Î•º Ï∂©Ïã§ÌïòÍ≤å Ï†ïÎ¶¨
3. Ï§ëÏöîÌïú ÏÑ∏Î∂Ä Ï†ïÎ≥¥Î•º Ìè¨Ìï®
4. ÎÇ¥Ïö©Ïù¥ Ïù¥ÎØ∏ÏßÄÏôÄ Í¥ÄÎ†®Ïù¥ ÏûàÏùÑ Ïàò ÏûàÏúºÎØÄÎ°ú Î¨∏Îß•ÏùÑ Í≥†Î†§ÌïòÏó¨ Ï†ïÎ¶¨

Î∞òÎìúÏãú "ÏòÅÏñ¥(En)"Î°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî."""
                            
                            try:
                                text_analysis = ollama_client.analyze_text(ocr_result.ocr_text, text_prompt)
                            except Exception as e:
                                logger.error(f"ÌÖçÏä§Ìä∏ Î∂ÑÏÑù Ïò§Î•ò: {str(e)}")
                                text_analysis = f"ÌÖçÏä§Ìä∏ Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}"
                            
                            # Îëê Î∂ÑÏÑù Í≤∞Í≥º Í≤∞Ìï©
                            combined_analysis = f"Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Í≤∞Í≥º:\n{image_analysis}\n\nÌÖçÏä§Ìä∏ Î∂ÑÏÑù Í≤∞Í≥º:\n{text_analysis}"
                        else:
                            # OCR ÏóÜÏù¥ Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑùÎßå ÏàòÌñâ
                            combined_analysis = image_analysis
                        
                    else:  # PDF ÌååÏùºÏù∏ Í≤ΩÏö∞
                        if ocr_result.ocr_text:
                            if analyze_by_page and len(page_texts) > 1:
                                # Í∞úÏÑ†Îêú ÌéòÏù¥ÏßÄÎ≥Ñ Î∂ÑÏÑù ÏàòÌñâ - OllamaClientÏùò Î∂ÑÏÑù Í∏∞Îä• ÌôúÏö©
                                try:
                                    combined_analysis = ollama_client.analyze_text(ocr_result.ocr_text, None, page_texts)
                                    logger.info("ÌéòÏù¥ÏßÄÎ≥Ñ Î∂ÑÏÑù ÏôÑÎ£å")
                                except Exception as e:
                                    logger.error(f"ÌéòÏù¥ÏßÄÎ≥Ñ Î∂ÑÏÑù Ïò§Î•ò: {str(e)}")
                                    combined_analysis = f"ÌéòÏù¥ÏßÄÎ≥Ñ Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}"
                            else:
                                # Î¨∏ÏÑú Ï†ÑÏ≤¥ Î∂ÑÏÑù - ÌéòÏù¥ÏßÄÎ≥Ñ Íµ¨Ï°∞Ìôî ÏöîÏ≤≠
                                text_prompt = f"""Îã§Ïùå PDFÏóêÏÑú Ï∂îÏ∂úÌïú ÌÖçÏä§Ìä∏Î•º ÌéòÏù¥ÏßÄÎ≥ÑÎ°ú Î™ÖÌôïÌïòÍ≤å Ï†ïÎ¶¨Ìï¥Ï£ºÏÑ∏Ïöî:

{ocr_result.ocr_text}

Î∂ÑÏÑù ÏßÄÏπ®:
1. ÌÖçÏä§Ìä∏Î•º ÌéòÏù¥ÏßÄÎÇò ÏÑπÏÖò Îã®ÏúÑÎ°ú Íµ¨Î∂ÑÌïòÏó¨ Ï†ïÎ¶¨Ìï¥Ï£ºÏÑ∏Ïöî.
2. ÎÇ¥Ïö©ÏùÑ ÏöîÏïΩÌïòÏßÄ ÎßêÍ≥†, Í∞Å ÏÑπÏÖòÏùò ÌïµÏã¨ Ï†ïÎ≥¥Î•º Ï∂©Ïã§ÌïòÍ≤å Ï†ïÎ¶¨Ìï¥Ï£ºÏÑ∏Ïöî.
3. Î™®Îì† Ï§ëÏöîÌïú ÏÑ∏Î∂Ä Ï†ïÎ≥¥Î•º Ìè¨Ìï®Ìï¥Ï£ºÏÑ∏Ïöî.
4. ÎÇ¥Ïö©ÏùÑ Îã®Ïàú ÏöîÏïΩÌïòÏßÄ ÎßêÍ≥†, Íµ¨Ï°∞ÌôîÎêú ÌòïÏãùÏúºÎ°ú Ï†ïÎ¶¨Ìï¥Ï£ºÏÑ∏Ïöî.

Îã§ÏùåÍ≥º Í∞ôÏùÄ ÌòïÏãùÏúºÎ°ú Ï†ïÎ¶¨Ìï¥Ï£ºÏÑ∏Ïöî:
===== ÌéòÏù¥ÏßÄ 1 (ÎòêÎäî ÏÑπÏÖò 1) =====
- Ï£ºÏöî ÎÇ¥Ïö© Ï†ïÎ¶¨
- Ï§ëÏöî Í∞úÎÖê ÏÑ§Î™Ö
- ÌïµÏã¨ Ï†ïÎ≥¥ ÎÇòÏó¥

===== ÌéòÏù¥ÏßÄ 2 (ÎòêÎäî ÏÑπÏÖò 2) =====
- Ï£ºÏöî ÎÇ¥Ïö© Ï†ïÎ¶¨
...

Î∞òÎìúÏãú "ÏòÅÏñ¥Î°ú" ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî."""
                                
                                try:
                                    text_analysis = ollama_client.analyze_text(ocr_result.ocr_text, text_prompt)
                                    combined_analysis = text_analysis
                                except Exception as e:
                                    logger.error(f"Î¨∏ÏÑú Ï†ÑÏ≤¥ Î∂ÑÏÑù Ïò§Î•ò: {str(e)}")
                                    combined_analysis = f"Î¨∏ÏÑú Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}\n\nOCR Í≤∞Í≥º: {ocr_result.ocr_text[:500]}..."
                    
                    logger.info("Ïù¥ÎØ∏ÏßÄ/ÌÖçÏä§Ìä∏ Î∂ÑÏÑù ÏôÑÎ£å")
                
                # GPT Î≤àÏó≠ ÏàòÌñâ (Î≤àÏó≠Ïù¥ ÌôúÏÑ±ÌôîÎêú Í≤ΩÏö∞)
                if enable_translation and combined_analysis and gpt_translator.is_available:
                    logger.info("GPT Î≤àÏó≠ ÏãúÏûë")
                    try:
                        # Î∂ÑÏÑù Ïú†ÌòïÏóê Îî∞Î•∏ Î≤àÏó≠
                        if file_type == 'pdf' and analyze_by_page and len(page_texts) > 1:
                            # ÌéòÏù¥ÏßÄÎ≥Ñ Î∂ÑÏÑù Í≤∞Í≥º Î≤àÏó≠
                            translation_result = gpt_translator.translate_paged_analysis(combined_analysis)
                        else:
                            # ÏùºÎ∞ò Î∂ÑÏÑù Í≤∞Í≥º Î≤àÏó≠
                            translation_result = gpt_translator.translate_analysis_result(combined_analysis, file_type)
                        
                        if translation_result and translation_result.get("success"):
                            translated_analysis = translation_result["translated_analysis"]
                            translation_success = True
                            logger.info("GPT Î≤àÏó≠ ÏÑ±Í≥µ")
                        else:
                            error_msg = translation_result.get('error', 'Unknown error') if translation_result else 'No translation result'
                            logger.error(f"GPT Î≤àÏó≠ Ïã§Ìå®: {error_msg}")
                            translated_analysis = f"Î≤àÏó≠ Ïã§Ìå®: {error_msg}"
                            translation_error = error_msg
                            
                    except Exception as e:
                        logger.error(f"GPT Î≤àÏó≠ Ï§ë ÏòàÏô∏ Î∞úÏÉù: {str(e)}")
                        translated_analysis = f"Î≤àÏó≠ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"
                        translation_error = str(e)
                
                # Î≤àÏó≠ Í¥ÄÎ†® Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
                ocr_result.translation_enabled = enable_translation
                ocr_result.translation_success = translation_success
                ocr_result.analysis_type = analysis_type
                ocr_result.analyze_by_page = analyze_by_page
                
                # MySQL Ï†ÄÏû•ÏùÑ ÏúÑÌïú ÌÖçÏä§Ìä∏ Ï†ïÌôî
                ocr_result.llm_response = self.clean_text(combined_analysis)
                
                # Î≤àÏó≠ Í≤∞Í≥ºÎèÑ Ï†ÄÏû•
                if enable_translation and translated_analysis:
                    if translation_success:
                        # ÏÑ±Í≥µÌïú Î≤àÏó≠ Í≤∞Í≥º Ï†ÄÏû•
                        ocr_result.llm_response_korean = self.clean_text(translated_analysis)
                        ocr_result.translation_model = gpt_translator.model if gpt_translator else "unknown"
                    else:
                        # Ïã§Ìå®Ìïú Í≤ΩÏö∞ Ïò§Î•ò Î©îÏãúÏßÄ Ï†ÄÏû• (ÎîîÎ≤ÑÍπÖÏö©)
                        ocr_result.llm_response_korean = f"Î≤àÏó≠ Ïã§Ìå®: {translation_error}"
                
                # ÌÖçÏä§Ìä∏ Í¥ÄÎ†®ÏÑ± Ï†ïÎ≥¥ Ï†ÄÏû• - PDFÎäî Ìï≠ÏÉÅ True, Ïù¥ÎØ∏ÏßÄÎäî Î∂ÑÏÑù Í≥ºÏ†ïÏóêÏÑú Í≤∞Ï†ï
                if file_type == 'pdf':
                    ocr_result.text_relevant = True
                
            except Exception as e:
                logger.error("Ï≤òÎ¶¨ Ïã§Ìå®: %s", str(e), exc_info=True)
                return Response({'error': f'Ï≤òÎ¶¨ Ïã§Ìå®: {str(e)}'}, 
                               status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # Í≤∞Í≥º Ï†ÄÏû•
            try:
                ocr_result.save()
                logger.info("OCRResult Ï†ÄÏû• ÏôÑÎ£å (ID: %s)", ocr_result.id)
            except Exception as e:
                logger.error(f"Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï†ÄÏû• Ïã§Ìå®: {str(e)}")
                return Response({'error': f'Í≤∞Í≥º Ï†ÄÏû• Ïã§Ìå®: {str(e)}'}, 
                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # ÏùëÎãµ Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ± - Î™ÖÏãúÏ†ÅÏúºÎ°ú ÌïÑÎìú ÏßÄÏ†ï
            try:
                # Í∏∞Î≥∏ ÏãúÎ¶¨ÏñºÎùºÏù¥Ï†Ä Îç∞Ïù¥ÌÑ∞
                response_data = OCRResultSerializer(ocr_result).data
                
                # Î≤àÏó≠ Í¥ÄÎ†® Ï†ïÎ≥¥ Î™ÖÏãúÏ†Å Ï∂îÍ∞Ä
                response_data['translation_enabled'] = enable_translation
                response_data['translation_success'] = translation_success
                
                # ÏòÅÏñ¥ ÏõêÎ¨∏Í≥º ÌïúÍµ≠Ïñ¥ Î≤àÏó≠ÏùÑ Î™ÖÌôïÌûà Íµ¨Î∂Ñ
                response_data['llm_response'] = ocr_result.llm_response  # ÏòÅÏñ¥ ÏõêÎ¨∏
                
                if enable_translation and translation_success:
                    # Î≤àÏó≠ ÏÑ±Í≥µ Ïãú ÌïúÍµ≠Ïñ¥ Î≤àÏó≠ Ï∂îÍ∞Ä
                    response_data['llm_response_korean'] = ocr_result.llm_response_korean
                    logger.info("ÏùëÎãµÏóê ÌïúÍµ≠Ïñ¥ Î≤àÏó≠ Ìè¨Ìï®")
                elif enable_translation and not translation_success:
                    # Î≤àÏó≠ Ïã§Ìå® Ïãú Ïò§Î•ò Ï†ïÎ≥¥ Ï∂îÍ∞Ä
                    response_data['llm_response_korean'] = None
                    response_data['translation_error'] = translation_error if translation_error else "Î≤àÏó≠ Ïã§Ìå®"
                    logger.info("Î≤àÏó≠ Ïã§Ìå® - ÏòÅÏñ¥ ÏõêÎ¨∏Îßå Ìè¨Ìï®")
                else:
                    # Î≤àÏó≠ ÎπÑÌôúÏÑ±Ìôî Ïãú
                    response_data['llm_response_korean'] = None
                    logger.info("Î≤àÏó≠ ÎπÑÌôúÏÑ±Ìôî - ÏòÅÏñ¥ ÏõêÎ¨∏Îßå Ìè¨Ìï®")
                
                # ÎîîÎ≤ÑÍπÖÏö© Î°úÍ∑∏
                logger.info(f"ÏùëÎãµ Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ± ÏôÑÎ£å:")
                logger.info(f"  - ÏòÅÏñ¥ ÏõêÎ¨∏ Í∏∏Ïù¥: {len(response_data.get('llm_response', ''))}")
                logger.info(f"  - ÌïúÍµ≠Ïñ¥ Î≤àÏó≠ Í∏∏Ïù¥: {len(response_data.get('llm_response_korean', '') or '')}")
                logger.info(f"  - Î≤àÏó≠ ÏÑ±Í≥µ: {response_data.get('translation_success', False)}")
                
            except Exception as e:
                logger.error(f"ÏùëÎãµ Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ± Ïã§Ìå®: {str(e)}")
                return Response({'error': f'ÏùëÎãµ Íµ¨ÏÑ± Ïã§Ìå®: {str(e)}'}, 
                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # ÏùëÎãµ Î∞òÌôò
            return Response(response_data, status=status.HTTP_201_CREATED)
                
        except Exception as e:
            logger.error("Ï≤òÎ¶¨ Ï§ë ÏòàÍ∏∞Ïπò ÏïäÏùÄ Ïò§Î•ò: %s", str(e), exc_info=True)
            return Response({'error': f'ÏÑúÎ≤Ñ Ïò§Î•ò: {str(e)}'}, 
                           status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def extract_text_from_pdf_by_pages(self, pdf_path):
        """PDFÏóêÏÑú ÏßÅÏ†ë ÌÖçÏä§Ìä∏Î•º ÌéòÏù¥ÏßÄÎ≥ÑÎ°ú Ï∂îÏ∂ú"""
        pages = []
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                total_pages = len(reader.pages)
                
                for i in range(total_pages):
                    page = reader.pages[i]
                    text = page.extract_text()
                    pages.append({"page": i + 1, "text": text})
                    
            return pages
        except Exception as e:
            logger.error(f"PDF ÌÖçÏä§Ìä∏ ÏßÅÏ†ë Ï∂îÏ∂ú Ïò§Î•ò: {str(e)}")
            raise
    
    def ocr_pdf_by_pages(self, pdf_path, ollama_client, start_page=1, end_page=0):
        """PDFÎ•º OCRÎ°ú Ï≤òÎ¶¨ÌïòÏó¨ ÌéòÏù¥ÏßÄÎ≥Ñ ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú"""
        pages = []
        
        try:
            # PDF2ImageÎ°ú Ïù¥ÎØ∏ÏßÄ Î≥ÄÌôò
            with tempfile.TemporaryDirectory() as path:
                # ÌéòÏù¥ÏßÄ Î≤àÌò∏Îäî 1Î∂ÄÌÑ∞ ÏãúÏûëÌïòÏßÄÎßå, convert_from_pathÎäî 0Î∂ÄÌÑ∞ ÏãúÏûëÌïòÎØÄÎ°ú Ï°∞Ï†ï
                first_page = start_page
                last_page = None if end_page <= 0 else end_page
                
                images = convert_from_path(
                    pdf_path, 
                    dpi=300, 
                    output_folder=path, 
                    first_page=first_page,
                    last_page=last_page
                )
                
                # Í∞Å ÌéòÏù¥ÏßÄ Ïù¥ÎØ∏ÏßÄ OCR Ï≤òÎ¶¨
                for i, image in enumerate(images):
                    # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨
                    preprocessed_img = ollama_client.preprocess_image_for_ocr(image)
                    # OCR ÏàòÌñâ
                    text = ollama_client.get_optimized_ocr_text(preprocessed_img, lang='kor+eng')
                    
                    # ÌéòÏù¥ÏßÄ Î≤àÌò∏ Í≥ÑÏÇ∞ (ÏãúÏûë ÌéòÏù¥ÏßÄ Í≥†Î†§)
                    page_num = start_page + i
                    pages.append({"page": page_num, "text": text})
                    
            return pages
        except Exception as e:
            logger.error(f"PDF OCR Ï≤òÎ¶¨ Ïò§Î•ò: {str(e)}")
            raise
    
    def clean_text(self, text):
        """ÌÖçÏä§Ìä∏ Ï†ïÌôî Ìï®Ïàò"""
        if not text:
            return ""
            
        # Ïó∞ÏÜçÎêú Í≥µÎ∞± Ï†úÍ±∞
        text = re.sub(r'\s+', ' ', text)
        # Ïó∞ÏÜçÎêú Ï§ÑÎ∞îÍøà Ï†úÍ±∞
        text = re.sub(r'\n\s*\n', '\n\n', text)
        # ÏïûÎí§ Í≥µÎ∞± Ï†úÍ±∞
        text = text.strip()
        
        return text
    
# chat/views.pyÏóê Ï∂îÍ∞ÄÌï† Î∑∞Îì§

from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.decorators import api_view, permission_classes
from rest_framework.permissions import IsAuthenticated, AllowAny
from django.shortcuts import get_object_or_404
from datetime import datetime, timedelta
import json
import re
from .models import Schedule, ScheduleRequest, ConflictResolution
from .serializers import (
    ScheduleSerializer, ScheduleRequestSerializer, 
    ConflictResolutionSerializer, ScheduleRequestInputSerializer
)

# Í∏∞Ï°¥ ChatBot ÌÅ¥ÎûòÏä§ÏôÄ ChatViewÎäî Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ...
# chat/views.py ÏàòÏ†ï Î≤ÑÏ†Ñ

# from rest_framework.views import APIView
# from rest_framework.response import Response
# from rest_framework import status
# from rest_framework.decorators import api_view, permission_classes
# from rest_framework.permissions import IsAuthenticated, AllowAny
# from django.shortcuts import get_object_or_404
# from datetime import datetime, timedelta
# import json
# import re
# from .models import Schedule, ScheduleRequest, ConflictResolution
# from .serializers import (
#     ScheduleSerializer, ScheduleRequestSerializer, 
#     ConflictResolutionSerializer, ScheduleRequestInputSerializer
# )

# # Í∏∞Ï°¥ ChatBot ÌÅ¥ÎûòÏä§Îäî Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ...


# chatbots = {
#     'gpt': ChatBot(OPENAI_API_KEY, 'gpt-3.5-turbo', 'openai'),
#     'claude': ChatBot(ANTHROPIC_API_KEY, 'claude-3-5-haiku-20241022', 'anthropic'), 
#     'mixtral': ChatBot(GROQ_API_KEY, 'llama-3.1-8b-instant', 'groq'),
# }

# # Î∞±ÏóîÎìú views.pyÏóê Ï∂îÍ∞ÄÌï† Ìï®ÏàòÎì§

# def parse_date_from_request(request_text):
#     """ÏûêÏó∞Ïñ¥ ÎÇ†ÏßúÎ•º Ïã§Ï†ú ÎÇ†ÏßúÎ°ú Î≥ÄÌôò"""
#     today = datetime.now().date()
    
#     # Ïò§Îäò/ÎÇ¥Ïùº/Î™®Î†à Îì± ÌïúÍµ≠Ïñ¥ ÎÇ†Ïßú ÌëúÌòÑ Ï≤òÎ¶¨
#     if 'Ïò§Îäò' in request_text:
#         return today
#     elif 'ÎÇ¥Ïùº' in request_text:
#         return today + timedelta(days=1)
#     elif 'Î™®Î†à' in request_text or 'Î™®Îûò' in request_text:
#         return today + timedelta(days=2)
#     elif 'Ïù¥Î≤à Ï£º' in request_text:
#         # Ïù¥Î≤à Ï£º Í∏àÏöîÏùºÎ°ú ÏÑ§Ï†ï
#         days_until_friday = (4 - today.weekday()) % 7
#         if days_until_friday == 0:  # Ïò§ÎäòÏù¥ Í∏àÏöîÏùºÏù¥Î©¥ Îã§Ïùå Ï£º Í∏àÏöîÏùº
#             days_until_friday = 7
#         return today + timedelta(days=days_until_friday)
#     elif 'Îã§Ïùå Ï£º' in request_text:
#         return today + timedelta(days=7)
#     else:
#         # Í∏∞Î≥∏Í∞í: ÎÇ¥Ïùº
#         return today + timedelta(days=1)

# def parse_multiple_schedules_backend(request_text):
#     """Î∞±ÏóîÎìúÏóêÏÑú Ïó¨Îü¨ ÏùºÏ†ï ÌååÏã±"""
#     # ÏâºÌëú, "Í∑∏Î¶¨Í≥†", "Î∞è" Îì±ÏúºÎ°ú Î∂ÑÎ¶¨
#     separators = [',', 'Ôºå', 'Í∑∏Î¶¨Í≥†', 'Î∞è', 'ÏôÄ', 'Í≥º']
    
#     parts = [request_text]
#     for sep in separators:
#         new_parts = []
#         for part in parts:
#             new_parts.extend(part.split(sep))
#         parts = new_parts
    
#     # Ï†ïÎ¶¨Îêú ÏöîÏ≤≠Îì§ Î∞òÌôò
#     cleaned_requests = []
#     for part in parts:
#         cleaned = part.strip()
#         if cleaned and len(cleaned) > 2:  # ÎÑàÎ¨¥ ÏßßÏùÄ ÌÖçÏä§Ìä∏ Ï†úÏô∏
#             cleaned_requests.append(cleaned)
    
#     return cleaned_requests if len(cleaned_requests) > 1 else [request_text]
# class ScheduleOptimizerBot:
#     """ÏùºÏ†ï ÏµúÏ†ÅÌôîÎ•º ÏúÑÌïú AI Î¥á ÌÅ¥ÎûòÏä§ - Ïó¨Îü¨ AI Î™®Îç∏ Ïó∞Îèô"""
    
#     def __init__(self):
#         self.chatbots = {
#                 'gpt': ChatBot(OPENAI_API_KEY, 'gpt-3.5-turbo', 'openai'),
#                 'claude': ChatBot(ANTHROPIC_API_KEY, 'claude-3-5-haiku-20241022', 'anthropic'), 
#                 'mixtral': ChatBot(GROQ_API_KEY, 'llama-3.1-8b-instant', 'groq'),
#             }
        
#     def create_schedule_prompt(self, request_text, user_context=None, existing_schedules=None):
#         """ÏùºÏ†ï ÏÉùÏÑ±ÏùÑ ÏúÑÌïú ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ± - Îπà ÏãúÍ∞Ñ Î∂ÑÏÑù Ìè¨Ìï®"""
#         base_prompt = f"""
#         ÏÇ¨Ïö©ÏûêÏùò ÏùºÏ†ï ÏöîÏ≤≠ÏùÑ Î∂ÑÏÑùÌïòÏó¨ Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º Ï∂©ÎèåÌïòÏßÄ ÏïäÎäî ÏµúÏ†ÅÏùò Îπà ÏãúÍ∞ÑÏùÑ Ï∞æÏïÑ Ï†úÏïàÌï¥Ï£ºÏÑ∏Ïöî.

#         ÏöîÏ≤≠ ÎÇ¥Ïö©: {request_text}
        
#         Í∏∞Ï°¥ ÏùºÏ†ïÎì§: {existing_schedules or []}
        
#         Î∂ÑÏÑù Î∞©Î≤ï:
#         1. Í∏∞Ï°¥ ÏùºÏ†ïÎì§Ïùò ÏãúÍ∞ÑÎåÄÎ•º ÌôïÏù∏ÌïòÏó¨ ÏÇ¨Ïö©ÏûêÍ∞Ä ÏûÖÎ†•Ìïú ÎÇ†Ïùò Îπà ÏãúÍ∞ÑÏùÑ Ï∞æÏïÑÏ£ºÏÑ∏Ïöî
#         2. ÏöîÏ≤≠Îêú ÏùºÏ†ïÏùò ÏÑ±Í≤©Ïóê ÎßûÎäî ÏµúÏ†ÅÏùò ÏãúÍ∞ÑÎåÄÎ•º Ï∂îÏ≤úÌï¥Ï£ºÏÑ∏Ïöî
#         3. ÏùºÏ†ï Í∞Ñ Ïó¨Ïú† ÏãúÍ∞ÑÎèÑ Í≥†Î†§Ìï¥Ï£ºÏÑ∏Ïöî
        
#         Îã§Ïùå ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî:
#         {{
#             "title": "ÏùºÏ†ï Ï†úÎ™©",
#             "description": "ÏÉÅÏÑ∏ ÏÑ§Î™Ö",
#             "suggested_date": "YYYY-MM-DD",
#             "suggested_start_time": "HH:MM",
#             "suggested_end_time": "HH:MM",
#             "location": "Ïû•ÏÜå (ÏÑ†ÌÉùÏÇ¨Ìï≠)",
#             "priority": "HIGH/MEDIUM/LOW/URGENT",
#             "attendees": ["Ï∞∏ÏÑùÏûê1", "Ï∞∏ÏÑùÏûê2"],
#             "reasoning": "Ïù¥ ÏãúÍ∞ÑÏùÑ Ï†úÏïàÌïòÎäî Ïù¥Ïú† (Îπà ÏãúÍ∞Ñ Î∂ÑÏÑù Í≤∞Í≥º Ìè¨Ìï®)"
#         }}
        
#         ÏÇ¨Ïö©ÏûêÏùò Îß•ÎùΩ Ï†ïÎ≥¥: {user_context or "ÏóÜÏùå"}
#         """
#         return base_prompt

#     def create_conflict_resolution_prompt(self, conflicting_schedules, new_request):
#         """ÏùºÏ†ï Ï∂©Îèå Ìï¥Í≤∞ÏùÑ ÏúÑÌïú ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±"""
#         prompt = f"""
#         Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º ÏÉàÎ°úÏö¥ ÏùºÏ†ï ÏöîÏ≤≠ ÏÇ¨Ïù¥Ïóê Ï∂©ÎèåÏù¥ Î∞úÏÉùÌñàÏäµÎãàÎã§. 
#         Ïó¨Îü¨ AIÏùò Í¥ÄÏ†êÏóêÏÑú ÏµúÏ†ÅÏùò Ìï¥Í≤∞ Î∞©ÏïàÏùÑ Ï†úÏïàÌï¥Ï£ºÏÑ∏Ïöî.

#         Í∏∞Ï°¥ Ï∂©Îèå ÏùºÏ†ïÎì§:
#         {json.dumps(conflicting_schedules, ensure_ascii=False, indent=2)}

#         ÏÉàÎ°úÏö¥ ÏùºÏ†ï ÏöîÏ≤≠: {new_request}

#         Îã§Ïùå ÌòïÏãùÏúºÎ°ú Ìï¥Í≤∞ Î∞©ÏïàÏùÑ Ï†úÏïàÌï¥Ï£ºÏÑ∏Ïöî:
#         {{
#             "resolution_options": [
#                 {{
#                     "option": "Î∞©Ïïà 1",
#                     "description": "ÏÉÅÏÑ∏ ÏÑ§Î™Ö",
#                     "impact": "ÏòÅÌñ•ÎèÑ Î∂ÑÏÑù",
#                     "recommended": true/false
#                 }},
#                 {{
#                     "option": "Î∞©Ïïà 2", 
#                     "description": "ÏÉÅÏÑ∏ ÏÑ§Î™Ö",
#                     "impact": "ÏòÅÌñ•ÎèÑ Î∂ÑÏÑù",
#                     "recommended": true/false
#                 }}
#             ],
#             "best_recommendation": "Í∞ÄÏû• Ï∂îÏ≤úÌïòÎäî Î∞©ÏïàÍ≥º Ïù¥Ïú†"
#         }}
#         """
#         return prompt
    
#     def get_ai_suggestions(self, prompt, suggestion_type="schedule"):
#         """Ïó¨Îü¨ AI Î™®Îç∏Î°úÎ∂ÄÌÑ∞ Ï†úÏïàÎ∞õÍ∏∞"""
#         suggestions = {}
        
#         for model_name, chatbot in self.chatbots.items():
#             try:
#                 response = chatbot.chat(prompt)
#                 suggestions[f"{model_name}_suggestion"] = response
#             except Exception as e:
#                 suggestions[f"{model_name}_suggestion"] = f"Ïò§Î•ò Î∞úÏÉù: {str(e)}"
        
#         return suggestions
    
#     def analyze_and_optimize_suggestions(self, suggestions, query, selected_models=['GPT', 'Claude', 'Mixtral']):
#         """Ïó¨Îü¨ AI Ï†úÏïàÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÏµúÏ†ÅÌôîÎêú Í≤∞Í≥º ÏÉùÏÑ± - Í∏∞Ï°¥ analyze_responses ÌôúÏö©"""
#         try:
#             # ChatBotÏùò analyze_responses Í∏∞Îä• ÌôúÏö©
#             analyzer = self.chatbots['claude']  # ClaudeÎ•º Î∂ÑÏÑùÏö©ÏúºÎ°ú ÏÇ¨Ïö©
            
#             # Ï†úÏïàÏùÑ Î∂ÑÏÑùÏö© ÌòïÌÉúÎ°ú Î≥ÄÌôò
#             responses_for_analysis = {}
#             for key, suggestion in suggestions.items():
#                 model_name = key.replace('_suggestion', '')
#                 responses_for_analysis[model_name] = suggestion
            
#             # Í∏∞Ï°¥ analyze_responses Î©îÏÑúÎìú ÌôúÏö©
#             analysis_result = analyzer.analyze_responses(
#                 responses_for_analysis, 
#                 query, 
#                 'Korean',  # Í∏∞Î≥∏ Ïñ∏Ïñ¥
#                 selected_models
#             )
            
#             # JSON ÏùëÎãµÏóêÏÑú ÏµúÏ†ÅÌôîÎêú ÏùºÏ†ï Ï†ïÎ≥¥ Ï∂îÏ∂ú
#             try:
#                 # best_responseÏóêÏÑú JSON Î∂ÄÎ∂Ñ Ï∂îÏ∂ú
#                 json_match = re.search(r'\{.*\}', analysis_result.get('best_response', ''), re.DOTALL)
#                 if json_match:
#                     optimized = json.loads(json_match.group())
#                 else:
#                     # fallback: Ï≤´ Î≤àÏß∏ Ïú†Ìö®Ìïú Ï†úÏïà ÏÇ¨Ïö©
#                     optimized = self._extract_first_valid_suggestion(suggestions)
#             except:
#                 optimized = self._extract_first_valid_suggestion(suggestions)
            
#             confidence = self._calculate_confidence_from_analysis(analysis_result)
            
#             return {
#                 "optimized_suggestion": optimized,
#                 "confidence_score": confidence,
#                 "ai_analysis": analysis_result,
#                 "individual_suggestions": self._parse_individual_suggestions(suggestions)
#             }
            
#         except Exception as e:
#             print(f"Analysis error: {str(e)}")
#             return {"error": f"ÏµúÏ†ÅÌôî Í≥ºÏ†ïÏóêÏÑú Ïò§Î•ò Î∞úÏÉù: {str(e)}"}
    
#     def _extract_first_valid_suggestion(self, suggestions):
#         """Ï≤´ Î≤àÏß∏ Ïú†Ìö®Ìïú Ï†úÏïà Ï∂îÏ∂ú"""
#         for key, suggestion in suggestions.items():
#             try:
#                 json_match = re.search(r'\{.*\}', suggestion, re.DOTALL)
#                 if json_match:
#                     return json.loads(json_match.group())
#             except:
#                 continue
        
#         # Í∏∞Î≥∏ Ï†úÏïà Î∞òÌôò
#         return {
#             "title": "ÏÉà ÏùºÏ†ï",
#             "description": "AIÍ∞Ä Ï†úÏïàÌïú ÏùºÏ†ïÏûÖÎãàÎã§",
#             "suggested_date": datetime.now().strftime('%Y-%m-%d'),
#             "suggested_start_time": "09:00",
#             "suggested_end_time": "10:00",
#             "location": "",
#             "priority": "MEDIUM",
#             "attendees": [],
#             "reasoning": "Ïó¨Îü¨ AI Î™®Îç∏Ïùò Ï†úÏïàÏùÑ Ï¢ÖÌï©Ìïú Í≤∞Í≥ºÏûÖÎãàÎã§."
#         }
    
#     def _calculate_confidence_from_analysis(self, analysis_result):
#         """Î∂ÑÏÑù Í≤∞Í≥ºÏóêÏÑú Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞"""
#         reasoning = analysis_result.get('reasoning', '')
        
#         # ÌÇ§ÏõåÎìú Í∏∞Î∞ò Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞
#         confidence_keywords = ['ÏùºÏπò', 'Í≥µÌÜµ', 'Ï†ïÌôï', 'ÏµúÏ†Å', 'Ï∂îÏ≤ú']
#         uncertainty_keywords = ['Î∂àÌôïÏã§', 'Ï∂îÏ†ï', 'Í∞ÄÎä•ÏÑ±', 'Ïñ¥Î†§ÏõÄ']
        
#         confidence_score = 0.5  # Í∏∞Î≥∏Í∞í
        
#         for keyword in confidence_keywords:
#             if keyword in reasoning:
#                 confidence_score += 0.1
        
#         for keyword in uncertainty_keywords:
#             if keyword in reasoning:
#                 confidence_score -= 0.1
        
#         return max(0.1, min(1.0, confidence_score))
    
#     def _parse_individual_suggestions(self, suggestions):
#         """Í∞úÎ≥Ñ Ï†úÏïàÎì§ÏùÑ ÌååÏã±"""
#         parsed = []
#         for key, suggestion in suggestions.items():
#             try:
#                 json_match = re.search(r'\{.*\}', suggestion, re.DOTALL)
#                 if json_match:
#                     parsed_suggestion = json.loads(json_match.group())
#                     parsed_suggestion['source'] = key.replace('_suggestion', '')
#                     parsed.append(parsed_suggestion)
#             except:
#                 continue
#         return parsed

# class ScheduleManagementView(APIView):
#     """ÏùºÏ†ï Í¥ÄÎ¶¨ Î©îÏù∏ Î∑∞ - Í∂åÌïú ÏàòÏ†ï"""
#     # ÏûÑÏãúÎ°ú AllowAnyÎ°ú Î≥ÄÍ≤Ω (Í∞úÎ∞ú/ÌÖåÏä§Ìä∏Ïö©)
#     permission_classes = [IsAuthenticated]
    
#     def __init__(self):
#         super().__init__()
#         self.optimizer = ScheduleOptimizerBot()
    
#     def get(self, request):
#         """ÏÇ¨Ïö©ÏûêÏùò ÏùºÏ†ï Î™©Î°ù Ï°∞Ìöå"""
#         # üö´ Í∏∞Ï°¥ ÎçîÎØ∏ ÏÇ¨Ïö©Ïûê Î°úÏßÅ Ï†úÍ±∞
#         if not request.user.is_authenticated:
#             return Response({'error': 'Ïù∏Ï¶ùÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.'}, status=status.HTTP_401_UNAUTHORIZED)
        
#         schedules = Schedule.objects.filter(user=request.user).order_by('start_time')
        
#         # ÎÇ†Ïßú ÌïÑÌÑ∞ÎßÅ (Í∏∞Ï°¥ ÏΩîÎìú Ïú†ÏßÄ)
#         start_date = request.query_params.get('start_date')
#         end_date = request.query_params.get('end_date')
        
#         if start_date:
#             schedules = schedules.filter(start_time__date__gte=start_date)
#         if end_date:
#             schedules = schedules.filter(end_time__date__lte=end_date)
        
#         serializer = ScheduleSerializer(schedules, many=True)
#         return Response(serializer.data)
#     def post(self, request):
#         """ÏÉàÎ°úÏö¥ ÏùºÏ†ï ÏÉùÏÑ± ÏöîÏ≤≠ - Ïó¨Îü¨ ÏùºÏ†ï ÏßÄÏõê Í∞úÏÑ†"""
#         try:
#             request_text = request.data.get('request_text', '')
#             existing_schedules = request.data.get('existing_schedules', [])
            
#             if not request_text:
#                 return Response({'error': 'ÏöîÏ≤≠ ÌÖçÏä§Ìä∏Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.'}, 
#                             status=status.HTTP_400_BAD_REQUEST)
#             if not request.user.is_authenticated:
#                 return Response({'error': 'Ïù∏Ï¶ùÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.'}, status=status.HTTP_401_UNAUTHORIZED)
        
#             user = request.user

         
            
#             # Ïó¨Îü¨ ÏùºÏ†ï ÏöîÏ≤≠Ïù∏ÏßÄ ÌôïÏù∏
#             schedule_requests = parse_multiple_schedules_backend(request_text)
#             target_date = parse_date_from_request(request_text)
            
#             if len(schedule_requests) > 1:
#                 # Ïó¨Îü¨ ÏùºÏ†ï Ï≤òÎ¶¨
#                 multiple_schedules = []
#                 all_individual_suggestions = []
                
#                 for i, single_request in enumerate(schedule_requests):
#                     # Í∞Å ÏùºÏ†ïÏùò ÏãúÏûë ÏãúÍ∞ÑÏùÑ Îã§Î•¥Í≤å ÏÑ§Ï†ï
#                     schedule_date = target_date
#                     if i > 0:  # Îëê Î≤àÏß∏ ÏùºÏ†ïÎ∂ÄÌÑ∞Îäî 2ÏãúÍ∞ÑÏî© Îí§Î°ú
#                         base_hour = 9 + (i * 2)
#                     else:
#                         base_hour = 9
                    
#                     # Í∞úÎ≥Ñ ÏùºÏ†ï ÏÉùÏÑ±
#                     optimized_schedule = {
#                         "title": self._extract_schedule_title(single_request),
#                         "description": f"AIÍ∞Ä Î∂ÑÏÑùÌïú {self._extract_schedule_title(single_request)} ÏùºÏ†ïÏûÖÎãàÎã§.",
#                         "suggested_date": schedule_date.strftime('%Y-%m-%d'),
#                         "suggested_start_time": f"{base_hour:02d}:00",
#                         "suggested_end_time": f"{base_hour + 2:02d}:00",
#                         "location": self._extract_schedule_location(single_request),
#                         "priority": "HIGH",
#                         "attendees": [],
#                         "reasoning": f"{i + 1}Î≤àÏß∏ ÏùºÏ†ï: {single_request}. Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º Ï∂©ÎèåÌïòÏßÄ ÏïäÎäî ÏãúÍ∞ÑÏúºÎ°ú Î∞∞Ï†ïÌñàÏäµÎãàÎã§."
#                     }
#                     multiple_schedules.append(optimized_schedule)
                    
#                     # Í∞Å AIÎ≥Ñ Í∞úÎ≥Ñ Ï†úÏïà ÏÉùÏÑ±
#                     for ai_type in ['gpt', 'claude', 'mixtral']:
#                         individual_suggestion = optimized_schedule.copy()
#                         individual_suggestion['source'] = ai_type
#                         individual_suggestion['reasoning'] = f"{ai_type.upper()}Í∞Ä Î∂ÑÏÑùÌïú {self._extract_schedule_title(single_request)} ÏµúÏ†Å ÏãúÍ∞ÑÏûÖÎãàÎã§."
#                         all_individual_suggestions.append(individual_suggestion)
                
#                 # Ïó¨Îü¨ ÏùºÏ†ï ÏùëÎãµ ÏÉùÏÑ±
#                 response_data = {
#                     'request_id': int(datetime.now().timestamp()),
#                     'multiple_schedules': multiple_schedules,
#                     'optimized_suggestion': multiple_schedules[0],
#                     'confidence_score': 0.92,
#                     'individual_suggestions': all_individual_suggestions,
#                     'ai_analysis': {
#                         'analysis_summary': f"Ï¥ù {len(schedule_requests)}Í∞úÏùò ÏùºÏ†ïÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÏµúÏ†ÅÏùò ÏãúÍ∞ÑÎåÄÎ°ú Î∞∞Ï†ïÌñàÏäµÎãàÎã§.",
#                         'reasoning': f"Ïó¨Îü¨ ÏùºÏ†ïÏùÑ {target_date.strftime('%YÎÖÑ %mÏõî %dÏùº')}Ïóê ÏãúÍ∞Ñ ÏàúÏÑúÎåÄÎ°ú Î∞∞ÏπòÌïòÏó¨ Ï∂©ÎèåÏùÑ Î∞©ÏßÄÌñàÏäµÎãàÎã§.",
#                         'models_used': ["gpt", "claude", "mixtral"]
#                     },
#                     'has_conflicts': False,
#                     'conflicts': [],
#                     'analysis_summary': f"{len(schedule_requests)}Í∞ú ÏùºÏ†ïÏóê ÎåÄÌï¥ 3Í∞ú AI Î™®Îç∏Ïù¥ Î∂ÑÏÑùÌïú Í≤∞Í≥ºÏûÖÎãàÎã§.",
#                     'is_multiple_schedule': True
#                 }
                
#             else:
#                 # Îã®Ïùº ÏùºÏ†ï Ï≤òÎ¶¨ (Í∏∞Ï°¥ Î°úÏßÅ ÏÇ¨Ïö©ÌïòÎêò ÎÇ†Ïßú Î∞òÏòÅ)
#                 user_context = self._get_user_context(user)
                
#                 # ÎÇ†ÏßúÍ∞Ä Î∞òÏòÅÎêú ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±
#                 enhanced_prompt = f"""
#                 ÏÇ¨Ïö©ÏûêÏùò ÏùºÏ†ï ÏöîÏ≤≠ÏùÑ Î∂ÑÏÑùÌïòÏó¨ Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º Ï∂©ÎèåÌïòÏßÄ ÏïäÎäî ÏµúÏ†ÅÏùò Îπà ÏãúÍ∞ÑÏùÑ Ï∞æÏïÑ Ï†úÏïàÌï¥Ï£ºÏÑ∏Ïöî.
                
#                 ÏöîÏ≤≠ ÎÇ¥Ïö©: {request_text}
#                 Î™©Ìëú ÎÇ†Ïßú: {target_date.strftime('%YÎÖÑ %mÏõî %dÏùº')} ({self._get_weekday_korean(target_date)})
#                 Í∏∞Ï°¥ ÏùºÏ†ïÎì§: {existing_schedules or []}
                
#                 Îã§Ïùå ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî:
#                 {{
#                     "title": "ÏùºÏ†ï Ï†úÎ™©",
#                     "description": "ÏÉÅÏÑ∏ ÏÑ§Î™Ö",
#                     "suggested_date": "{target_date.strftime('%Y-%m-%d')}",
#                     "suggested_start_time": "HH:MM",
#                     "suggested_end_time": "HH:MM",
#                     "location": "Ïû•ÏÜå",
#                     "priority": "HIGH/MEDIUM/LOW/URGENT",
#                     "attendees": [],
#                     "reasoning": "Ïù¥ ÏãúÍ∞ÑÏùÑ Ï†úÏïàÌïòÎäî Ïù¥Ïú†"
#                 }}
#                 """
                
#                 # Í∏∞Ï°¥ Îã®Ïùº ÏùºÏ†ï Î°úÏßÅ Í≥ÑÏÜç...
#                 suggestions = self.optimizer.get_ai_suggestions(enhanced_prompt)
#                 optimized_result = self.optimizer.analyze_and_optimize_suggestions(
#                     suggestions, f"ÏùºÏ†ï ÏöîÏ≤≠: {request_text}"
#                 )
                
#                 response_data = {
#                     'request_id': int(datetime.now().timestamp()),
#                     'optimized_suggestion': optimized_result.get('optimized_suggestion', {}),
#                     'confidence_score': optimized_result.get('confidence_score', 0.0),
#                     'ai_analysis': optimized_result.get('ai_analysis', {}),
#                     'individual_suggestions': optimized_result.get('individual_suggestions', []),
#                     'has_conflicts': False,
#                     'conflicts': [],
#                     'analysis_summary': "3Í∞ú AI Î™®Îç∏Ïù¥ Î∂ÑÏÑùÌïú Í≤∞Í≥ºÏûÖÎãàÎã§.",
#                     'is_multiple_schedule': False
#                 }
            
#             return Response(response_data, status=status.HTTP_201_CREATED)
            
#         except Exception as e:
#                     return Response({
#                         'error': f'ÏùºÏ†ï ÏÉùÏÑ± ÏöîÏ≤≠ Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {str(e)}'
#                     }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

#     def _extract_schedule_title(self, request):
#             """ÏöîÏ≤≠ÏóêÏÑú ÏùºÏ†ï Ï†úÎ™© Ï∂îÏ∂ú"""
#             if 'Ïö¥Îèô' in request:
#                 return 'Ïö¥Îèô'
#             elif 'ÎØ∏ÌåÖ' in request or 'ÌöåÏùò' in request:
#                 return 'ÌåÄ ÎØ∏ÌåÖ'
#             elif 'Í≥µÎ∂Ä' in request or 'ÌïôÏäµ' in request:
#                 return 'ÌïôÏäµ ÏãúÍ∞Ñ'
#             elif 'ÏûëÏóÖ' in request or 'ÏóÖÎ¨¥' in request:
#                 return 'ÏßëÏ§ë ÏûëÏóÖ'
#             elif 'ÏïΩÏÜç' in request:
#                 return 'ÏïΩÏÜç'
#             else:
#                 return 'ÏÉà ÏùºÏ†ï'

#     def _extract_schedule_location(self, request):
#             """ÏöîÏ≤≠ÏóêÏÑú Ïû•ÏÜå Ï∂îÏ∂ú"""
#             if 'Ïö¥Îèô' in request:
#                 return 'Ìó¨Ïä§Ïû•'
#             elif 'ÎØ∏ÌåÖ' in request or 'ÌöåÏùò' in request:
#                 return 'ÌöåÏùòÏã§'
#             elif 'Í≥µÎ∂Ä' in request or 'ÌïôÏäµ' in request:
#                 return 'ÎèÑÏÑúÍ¥Ä'
#             elif 'Ïª§Ìîº' in request:
#                 return 'Ïπ¥Ìéò'
#             else:
#                 return 'ÏÇ¨Î¨¥Ïã§'

#     def _get_weekday_korean(self, date):
#             """ÏöîÏùºÏùÑ ÌïúÍµ≠Ïñ¥Î°ú Î∞òÌôò"""
#             weekdays = ['ÏõîÏöîÏùº', 'ÌôîÏöîÏùº', 'ÏàòÏöîÏùº', 'Î™©ÏöîÏùº', 'Í∏àÏöîÏùº', 'ÌÜ†ÏöîÏùº', 'ÏùºÏöîÏùº']
#             return weekdays[date.weekday()]
            
   
#     def _check_schedule_conflicts(self, user, suggestion):
#         """ÏùºÏ†ï Ï∂©Îèå Í≤ÄÏÇ¨"""
#         if not suggestion or 'suggested_date' not in suggestion:
#             return []
        
#         try:
#             suggested_date = datetime.strptime(suggestion['suggested_date'], '%Y-%m-%d').date()
#             start_time = datetime.strptime(suggestion.get('suggested_start_time', '09:00'), '%H:%M').time()
#             end_time = datetime.strptime(suggestion.get('suggested_end_time', '10:00'), '%H:%M').time()
            
#             suggested_start = datetime.combine(suggested_date, start_time)
#             suggested_end = datetime.combine(suggested_date, end_time)
            
#             conflicts = Schedule.objects.filter(
#                 user=user,
#                 start_time__date=suggested_date,
#                 start_time__lt=suggested_end,
#                 end_time__gt=suggested_start
#             )
            
#             return [ScheduleSerializer(conflict).data for conflict in conflicts]
            
#         except Exception as e:
#             return []

# # Í∂åÌïú ÏàòÏ†ïÎêú Ìï®ÏàòÎì§
# @api_view(['POST'])
# @permission_classes([IsAuthenticated])  # üîß Í∂åÌïú Î≥ÄÍ≤Ω
# def confirm_schedule(request, request_id):
#     """AI Ï†úÏïàÎêú ÏùºÏ†ïÏùÑ ÌôïÏ†ïÌïòÏó¨ Ïã§Ï†ú ÏùºÏ†ïÏúºÎ°ú ÏÉùÏÑ±"""
#     try:
#         user = request.user
        
#         # üö´ ScheduleRequest.DoesNotExistÏóêÏÑú ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Ï†úÍ±∞
#         try:
#             schedule_request = ScheduleRequest.objects.get(id=request_id, user=user)
#             optimized_suggestion = json.loads(schedule_request.optimized_suggestion)
#         except ScheduleRequest.DoesNotExist:
#             return Response({
#                 'error': f'ÏöîÏ≤≠ ID {request_id}Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
#             }, status=status.HTTP_404_NOT_FOUND)
#                 # ÎÇ†Ïßú/ÏãúÍ∞Ñ ÌååÏã± Í∞úÏÑ†
#         try:
#             suggested_date = optimized_suggestion.get('suggested_date')
#             suggested_start_time = optimized_suggestion.get('suggested_start_time', '09:00')
#             suggested_end_time = optimized_suggestion.get('suggested_end_time', '10:00')
            
#             # ÎÇ†Ïßú ÌòïÏãù ÌôïÏù∏ Î∞è Î≥ÄÌôò
#             if isinstance(suggested_date, str):
#                 if 'T' in suggested_date:  # ISO ÌòïÏãùÏù∏ Í≤ΩÏö∞
#                     suggested_date = suggested_date.split('T')[0]
                
#                 start_datetime = datetime.strptime(
#                     f"{suggested_date} {suggested_start_time}",
#                     '%Y-%m-%d %H:%M'
#                 )
#                 end_datetime = datetime.strptime(
#                     f"{suggested_date} {suggested_end_time}",
#                     '%Y-%m-%d %H:%M'
#                 )
#             else:
#                 # ÎÇ†ÏßúÍ∞Ä ÏóÜÏúºÎ©¥ Ïò§ÎäòÎ°ú ÏÑ§Ï†ï
#                 today = datetime.now().date()
#                 start_datetime = datetime.strptime(
#                     f"{today} {suggested_start_time}",
#                     '%Y-%m-%d %H:%M'
#                 )
#                 end_datetime = datetime.strptime(
#                     f"{today} {suggested_end_time}",
#                     '%Y-%m-%d %H:%M'
#                 )
                
#         except (ValueError, TypeError) as e:
#             print(f"DateTime parsing error: {e}")
#             # Í∏∞Î≥∏Í∞íÏúºÎ°ú Ìè¥Î∞±
#             now = datetime.now()
#             start_datetime = now.replace(hour=9, minute=0, second=0, microsecond=0)
#             end_datetime = now.replace(hour=10, minute=0, second=0, microsecond=0)
        
#         # Schedule Í∞ùÏ≤¥ ÏÉùÏÑ±
#         schedule_data = {
#             'user': user,
#             'title': optimized_suggestion.get('title', 'ÏÉà ÏùºÏ†ï'),
#             'description': optimized_suggestion.get('description', 'AIÍ∞Ä Ï†úÏïàÌïú ÏùºÏ†ïÏûÖÎãàÎã§.'),
#             'start_time': start_datetime,
#             'end_time': end_datetime,
#             'location': optimized_suggestion.get('location', ''),
#             'priority': optimized_suggestion.get('priority', 'MEDIUM'),
#             'attendees': json.dumps(optimized_suggestion.get('attendees', []), ensure_ascii=False)
#         }
        
#         schedule = Schedule.objects.create(**schedule_data)
#         serializer = ScheduleSerializer(schedule)
        
#         print(f"Schedule created successfully: {schedule.id}")
        
#         return Response({
#             'message': 'Ïó¨Îü¨ AIÏùò Î∂ÑÏÑùÏùÑ ÌÜµÌï¥ ÏµúÏ†ÅÌôîÎêú ÏùºÏ†ïÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.',
#             'schedule': serializer.data
#         }, status=status.HTTP_201_CREATED)
        
#     except Exception as e:
#         print(f"Confirm schedule error: {str(e)}")
#         import traceback
#         traceback.print_exc()
        
#         return Response({
#             'error': f'ÏùºÏ†ï ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}'
#         }, status=status.HTTP_400_BAD_REQUEST)


# # Alternative solution: Convert to Class-Based View
# class ConfirmScheduleView(APIView):
#     """AI Ï†úÏïàÎêú ÏùºÏ†ïÏùÑ ÌôïÏ†ïÌïòÏó¨ Ïã§Ï†ú ÏùºÏ†ïÏúºÎ°ú ÏÉùÏÑ±"""
#     permission_classes = [AllowAny]  # ÏûÑÏãúÎ°ú AllowAny
    
#     def post(self, request, request_id):
#         try:
#             # ÏÇ¨Ïö©Ïûê Ï≤òÎ¶¨
#             if not request.user.is_authenticated:
#                 return Response({'error': 'Ïù∏Ï¶ùÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.'}, status=status.HTTP_401_UNAUTHORIZED)

#             user = request.user
            
#             # request_idÎ°ú ScheduleRequestÎ•º Ï∞æÍ±∞ÎÇò ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
#             try:
#                 schedule_request = ScheduleRequest.objects.get(id=request_id, user=user)
#                 optimized_suggestion = json.loads(schedule_request.optimized_suggestion)
#             except ScheduleRequest.DoesNotExist:
#                 # ÎçîÎØ∏ Î™®Îìú: request_idÎ•º Í∏∞Î∞òÏúºÎ°ú Í∏∞Î≥∏ ÏùºÏ†ï ÏÉùÏÑ±
#                 print(f"ScheduleRequest {request_id} not found, creating dummy schedule")
#                 optimized_suggestion = {
#                     'title': 'AI Ï†úÏïà ÏùºÏ†ï',
#                     'description': 'AIÍ∞Ä Ï†úÏïàÌïú ÏµúÏ†ÅÏùò ÏùºÏ†ïÏûÖÎãàÎã§.',
#                     'suggested_date': datetime.now().strftime('%Y-%m-%d'),
#                     'suggested_start_time': '09:00',
#                     'suggested_end_time': '10:00',
#                     'location': 'ÏÇ¨Î¨¥Ïã§',
#                     'priority': 'MEDIUM',
#                     'attendees': []
#                 }
#             except json.JSONDecodeError as e:
#                 print(f"JSON decode error: {e}")
#                 return Response({
#                     'error': f'ÏùºÏ†ï Îç∞Ïù¥ÌÑ∞ ÌååÏã± Ïò§Î•ò: {str(e)}'
#                 }, status=status.HTTP_400_BAD_REQUEST)
            
#             # ÎÇ†Ïßú/ÏãúÍ∞Ñ ÌååÏã±
#             try:
#                 suggested_date = optimized_suggestion.get('suggested_date')
#                 suggested_start_time = optimized_suggestion.get('suggested_start_time', '09:00')
#                 suggested_end_time = optimized_suggestion.get('suggested_end_time', '10:00')
                
#                 # ÎÇ†Ïßú ÌòïÏãù ÌôïÏù∏ Î∞è Î≥ÄÌôò
#                 if isinstance(suggested_date, str):
#                     if 'T' in suggested_date:  # ISO ÌòïÏãùÏù∏ Í≤ΩÏö∞
#                         suggested_date = suggested_date.split('T')[0]
                    
#                     start_datetime = datetime.strptime(
#                         f"{suggested_date} {suggested_start_time}",
#                         '%Y-%m-%d %H:%M'
#                     )
#                     end_datetime = datetime.strptime(
#                         f"{suggested_date} {suggested_end_time}",
#                         '%Y-%m-%d %H:%M'
#                     )
#                 else:
#                     # ÎÇ†ÏßúÍ∞Ä ÏóÜÏúºÎ©¥ Ïò§ÎäòÎ°ú ÏÑ§Ï†ï
#                     today = datetime.now().date()
#                     start_datetime = datetime.strptime(
#                         f"{today} {suggested_start_time}",
#                         '%Y-%m-%d %H:%M'
#                     )
#                     end_datetime = datetime.strptime(
#                         f"{today} {suggested_end_time}",
#                         '%Y-%m-%d %H:%M'
#                     )
                    
#             except (ValueError, TypeError) as e:
#                 print(f"DateTime parsing error: {e}")
#                 # Í∏∞Î≥∏Í∞íÏúºÎ°ú Ìè¥Î∞±
#                 now = datetime.now()
#                 start_datetime = now.replace(hour=9, minute=0, second=0, microsecond=0)
#                 end_datetime = now.replace(hour=10, minute=0, second=0, microsecond=0)
            
#             # Schedule Í∞ùÏ≤¥ ÏÉùÏÑ±
#             schedule_data = {
#                 'user': user,
#                 'title': optimized_suggestion.get('title', 'ÏÉà ÏùºÏ†ï'),
#                 'description': optimized_suggestion.get('description', 'AIÍ∞Ä Ï†úÏïàÌïú ÏùºÏ†ïÏûÖÎãàÎã§.'),
#                 'start_time': start_datetime,
#                 'end_time': end_datetime,
#                 'location': optimized_suggestion.get('location', ''),
#                 'priority': optimized_suggestion.get('priority', 'MEDIUM'),
#                 'attendees': json.dumps(optimized_suggestion.get('attendees', []), ensure_ascii=False)
#             }
            
#             schedule = Schedule.objects.create(**schedule_data)
#             serializer = ScheduleSerializer(schedule)
            
#             print(f"Schedule created successfully: {schedule.id}")
            
#             return Response({
#                 'message': 'Ïó¨Îü¨ AIÏùò Î∂ÑÏÑùÏùÑ ÌÜµÌï¥ ÏµúÏ†ÅÌôîÎêú ÏùºÏ†ïÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.',
#                 'schedule': serializer.data
#             }, status=status.HTTP_201_CREATED)
            
#         except Exception as e:
#             print(f"Confirm schedule error: {str(e)}")
#             import traceback
#             traceback.print_exc()
            
#             return Response({
#                 'error': f'ÏùºÏ†ï ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}'
#             }, status=status.HTTP_400_BAD_REQUEST)


# # Fix for resolve_schedule_conflict function
# @api_view(['POST'])
# @permission_classes([IsAuthenticated])  # üîß Í∂åÌïú Î≥ÄÍ≤Ω
# def resolve_schedule_conflict(request):
#     """ÏùºÏ†ï Ï∂©Îèå Ìï¥Í≤∞ Î∞©Ïïà Ï†úÍ≥µ"""
#     # üö´ ÎçîÎØ∏ ÏÇ¨Ïö©Ïûê Î°úÏßÅ Ï†úÍ±∞
#     user = request.user
    
#     conflicting_schedule_ids = request.data.get('conflicting_schedule_ids', [])
#     new_request = request.data.get('new_request', '')
    
#     # ÎÇòÎ®∏ÏßÄ Î°úÏßÅÏùÄ Í∑∏ÎåÄÎ°ú...
    
#     if not conflicting_schedule_ids or not new_request:
#         return Response({
#             'error': 'Ï∂©Îèå ÏùºÏ†ï IDÏôÄ ÏÉàÎ°úÏö¥ ÏöîÏ≤≠Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§.'
#         }, status=status.HTTP_400_BAD_REQUEST)
    
#     try:
#         # ÏÇ¨Ïö©Ïûê Ï≤òÎ¶¨
#         if request.user.is_authenticated:
#             user = request.user
#         else:
#             from django.contrib.auth.models import User
#             user, created = User.objects.get_or_create(
#                 username='dummy_user',
#                 defaults={'email': 'dummy@example.com'}
#             )
        
#         # Ï∂©Îèå ÏùºÏ†ïÎì§ Ï°∞Ìöå
#         conflicting_schedules = Schedule.objects.filter(
#             id__in=conflicting_schedule_ids,
#             user=user
#         )
        
#         conflicting_data = [ScheduleSerializer(schedule).data for schedule in conflicting_schedules]
        
#         # Îã§Ï§ë AI Î™®Îç∏Îì§Î°úÎ∂ÄÌÑ∞ Ìï¥Í≤∞ Î∞©Ïïà Î∞õÍ∏∞
#         optimizer = ScheduleOptimizerBot()
#         prompt = optimizer.create_conflict_resolution_prompt(conflicting_data, new_request)
#         suggestions = optimizer.get_ai_suggestions(prompt, "conflict_resolution")
        
#         # AI Î∂ÑÏÑùÏùÑ ÌÜµÌïú ÏµúÏ†Å Ìï¥Í≤∞Î∞©Ïïà ÎèÑÏ∂ú
#         analysis_result = optimizer.analyze_and_optimize_suggestions(
#             suggestions,
#             f"Ï∂©Îèå Ìï¥Í≤∞: {new_request}"
#         )
        
#         # Ìï¥Í≤∞ Î∞©Ïïà Ï†ÄÏû•
#         conflict_resolution = ConflictResolution.objects.create(
#             user=user,
#             conflicting_schedules=json.dumps(conflicting_data, ensure_ascii=False),
#             resolution_options=json.dumps(suggestions, ensure_ascii=False),
#             ai_recommendations=json.dumps(analysis_result, ensure_ascii=False)
#         )
        
#         return Response({
#             'resolution_id': conflict_resolution.id,
#             'conflicting_schedules': conflicting_data,
#             'ai_suggestions': suggestions,
#             'optimized_resolution': analysis_result,
#             'message': f'{len(suggestions)}Í∞ú AI Î™®Îç∏Ïù¥ Î∂ÑÏÑùÌïú Ï∂©Îèå Ìï¥Í≤∞ Î∞©ÏïàÏù¥ ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.'
#         }, status=status.HTTP_201_CREATED)
        
#     except Exception as e:
#         return Response({
#             'error': f'Ï∂©Îèå Ìï¥Í≤∞ Î∞©Ïïà ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}'
#         }, status=status.HTTP_400_BAD_REQUEST)


# # Alternative Class-Based View for conflict resolution
# class ResolveScheduleConflictView(APIView):
#     """ÏùºÏ†ï Ï∂©Îèå Ìï¥Í≤∞ Î∞©Ïïà Ï†úÍ≥µ - Îã§Ï§ë AI Î∂ÑÏÑù"""
#     permission_classes = [IsAuthenticated]
    
#     def post(self, request):
#         conflicting_schedule_ids = request.data.get('conflicting_schedule_ids', [])
#         new_request = request.data.get('new_request', '')
        
#         if not conflicting_schedule_ids or not new_request:
#             return Response({
#                 'error': 'Ï∂©Îèå ÏùºÏ†ï IDÏôÄ ÏÉàÎ°úÏö¥ ÏöîÏ≤≠Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§.'
#             }, status=status.HTTP_400_BAD_REQUEST)
        
#         try:
#             # ÏÇ¨Ïö©Ïûê Ï≤òÎ¶¨
#             if request.user.is_authenticated:
#                 user = request.user
#             else:
#                 from django.contrib.auth.models import User
#                 user, created = User.objects.get_or_create(
#                     username='dummy_user',
#                     defaults={'email': 'dummy@example.com'}
#                 )
            
#             # Ï∂©Îèå ÏùºÏ†ïÎì§ Ï°∞Ìöå
#             conflicting_schedules = Schedule.objects.filter(
#                 id__in=conflicting_schedule_ids,
#                 user=user
#             )
            
#             conflicting_data = [ScheduleSerializer(schedule).data for schedule in conflicting_schedules]
            
#             # Îã§Ï§ë AI Î™®Îç∏Îì§Î°úÎ∂ÄÌÑ∞ Ìï¥Í≤∞ Î∞©Ïïà Î∞õÍ∏∞
#             optimizer = ScheduleOptimizerBot()
#             prompt = optimizer.create_conflict_resolution_prompt(conflicting_data, new_request)
#             suggestions = optimizer.get_ai_suggestions(prompt, "conflict_resolution")
            
#             # AI Î∂ÑÏÑùÏùÑ ÌÜµÌïú ÏµúÏ†Å Ìï¥Í≤∞Î∞©Ïïà ÎèÑÏ∂ú
#             analysis_result = optimizer.analyze_and_optimize_suggestions(
#                 suggestions,
#                 f"Ï∂©Îèå Ìï¥Í≤∞: {new_request}"
#             )
            
#             # Ìï¥Í≤∞ Î∞©Ïïà Ï†ÄÏû•
#             conflict_resolution = ConflictResolution.objects.create(
#                 user=user,
#                 conflicting_schedules=json.dumps(conflicting_data, ensure_ascii=False),
#                 resolution_options=json.dumps(suggestions, ensure_ascii=False),
#                 ai_recommendations=json.dumps(analysis_result, ensure_ascii=False)
#             )
            
#             return Response({
#                 'resolution_id': conflict_resolution.id,
#                 'conflicting_schedules': conflicting_data,
#                 'ai_suggestions': suggestions,
#                 'optimized_resolution': analysis_result,
#                 'message': f'{len(suggestions)}Í∞ú AI Î™®Îç∏Ïù¥ Î∂ÑÏÑùÌïú Ï∂©Îèå Ìï¥Í≤∞ Î∞©ÏïàÏù¥ ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.'
#             }, status=status.HTTP_201_CREATED)
            
#         except Exception as e:
#             return Response({
#                 'error': f'Ï∂©Îèå Ìï¥Í≤∞ Î∞©Ïïà ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}'
#             }, status=status.HTTP_400_BAD_REQUEST)
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.decorators import api_view, permission_classes, authentication_classes
from rest_framework.permissions import IsAuthenticated, AllowAny
from rest_framework.authentication import TokenAuthentication, SessionAuthentication
from rest_framework.authtoken.models import Token
from rest_framework import exceptions
from django.shortcuts import get_object_or_404
from datetime import datetime, timedelta
import json
import re
import os
from .models import Schedule, ScheduleRequest, ConflictResolution
from .serializers import (
    ScheduleSerializer, ScheduleRequestSerializer, 
    ConflictResolutionSerializer, ScheduleRequestInputSerializer
)
import logging

# ÌÉÄÏûÑÏ°¥ import ÏàòÏ†ï
import pytz
KST = pytz.timezone('Asia/Seoul')

def get_current_datetime():
    return datetime.now(KST)

logger = logging.getLogger(__name__)

# API ÌÇ§Îì§
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")  
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# ÌÜ†ÌÅ∞ ÎîîÎ≤ÑÍπÖÏùÑ ÏúÑÌïú Ïª§Ïä§ÌÖÄ Ïù∏Ï¶ù ÌÅ¥ÎûòÏä§
class DebugTokenAuthentication(TokenAuthentication):
    """ÎîîÎ≤ÑÍπÖÏù¥ Ìè¨Ìï®Îêú ÌÜ†ÌÅ∞ Ïù∏Ï¶ù ÌÅ¥ÎûòÏä§"""
    
    def authenticate(self, request):
        logger.info("=== Í∞úÏÑ†Îêú ÌÜ†ÌÅ∞ Ïù∏Ï¶ù ÎîîÎ≤ÑÍπÖ ÏãúÏûë ===")
        
        auth_header = request.META.get('HTTP_AUTHORIZATION', '')
        logger.info(f"Authorization Ìó§Îçî: '{auth_header}'")
        
        if not auth_header:
            logger.warning("‚ùå Authorization Ìó§ÎçîÍ∞Ä ÏóÜÏäµÎãàÎã§")
            return None
            
        if not auth_header.startswith('Bearer '):
            logger.warning(f"‚ùå Bearer ÌÜ†ÌÅ∞ ÌòïÏãùÏù¥ ÏïÑÎãôÎãàÎã§: {auth_header}")
            return None
            
        token = auth_header.split(' ')[1]
        logger.info(f"üì± Ï∂îÏ∂úÎêú ÌÜ†ÌÅ∞: {token[:10]}...{token[-10:]}")
        
        try:
            token_obj = Token.objects.select_related('user').get(key=token)
            logger.info(f"‚úÖ DBÏóêÏÑú ÌÜ†ÌÅ∞ Î∞úÍ≤¨: {token_obj.key[:10]}...{token_obj.key[-10:]}")
            logger.info(f"üë§ ÌÜ†ÌÅ∞ ÏÜåÏú†Ïûê: {token_obj.user.username} (ID: {token_obj.user.id})")
            
            if not token_obj.user.is_active:
                logger.warning(f"‚ùå ÏÇ¨Ïö©ÏûêÍ∞Ä ÎπÑÌôúÏÑ±ÌôîÎê®: {token_obj.user.username}")
                raise exceptions.AuthenticationFailed('User inactive or deleted.')
            
            logger.info("‚úÖ ÌÜ†ÌÅ∞ Ïù∏Ï¶ù ÏÑ±Í≥µ!")
            logger.info("=== Í∞úÏÑ†Îêú ÌÜ†ÌÅ∞ Ïù∏Ï¶ù ÎîîÎ≤ÑÍπÖ Ï¢ÖÎ£å ===")
            return (token_obj.user, token_obj)
            
        except Token.DoesNotExist:
            logger.error(f"‚ùå DBÏóê Ìï¥Îãπ ÌÜ†ÌÅ∞Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå: {token[:10]}...{token[-10:]}")
            raise exceptions.AuthenticationFailed('Invalid token.')
        
        except Exception as e:
            logger.error(f"‚ùå ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•ò: {str(e)}")
            raise exceptions.AuthenticationFailed('Authentication error.')


# Ïã§Ï†ú AI ChatBot ÌÅ¥ÎûòÏä§
class RealChatBot:
    def __init__(self, api_key, model_name, provider):
        self.api_key = api_key
        self.model_name = model_name
        self.provider = provider
    
    def chat(self, prompt):
        """Ïã§Ï†ú AI API Ìò∏Ï∂ú"""
        try:
            if self.provider == 'openai' and self.api_key:
                return self._call_openai_api(prompt)
            elif self.provider == 'anthropic' and self.api_key:
                return self._call_anthropic_api(prompt)
            elif self.provider == 'groq' and self.api_key:
                return self._call_groq_api(prompt)
            else:
                return self._generate_fallback_response(prompt)
        except Exception as e:
            logger.error(f"AI API Ìò∏Ï∂ú Ïã§Ìå® ({self.provider}): {e}")
            return self._generate_fallback_response(prompt)
    
    def _call_openai_api(self, prompt):
        """OpenAI API Ìò∏Ï∂ú - ÏÉà Î≤ÑÏ†Ñ Î¨∏Î≤ï ÏÇ¨Ïö©"""
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=self.api_key)
            
            response = client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ÎãπÏã†ÏùÄ ÏùºÏ†ï Í¥ÄÎ¶¨Î•º ÎèÑÏôÄÏ£ºÎäî AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. JSON ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            
            return response.choices[0].message.content
            
        except ImportError:
            logger.error("openai Ìå®ÌÇ§ÏßÄÍ∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§")
            return self._generate_fallback_response(prompt)
        except Exception as e:
            logger.error(f"OpenAI API Ïò§Î•ò: {e}")
            return self._generate_fallback_response(prompt)
    
    def _call_anthropic_api(self, prompt):
        """Anthropic API Ìò∏Ï∂ú"""
        try:
            import anthropic
            
            client = anthropic.Anthropic(api_key=self.api_key)
            
            response = client.messages.create(
                model=self.model_name,
                max_tokens=1000,
                messages=[
                    {"role": "user", "content": f"ÎãπÏã†ÏùÄ ÏùºÏ†ï Í¥ÄÎ¶¨Î•º ÎèÑÏôÄÏ£ºÎäî AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. JSON ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî.\n\n{prompt}"}
                ]
            )
            
            return response.content[0].text
            
        except ImportError:
            logger.error("anthropic Ìå®ÌÇ§ÏßÄÍ∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§")
            return self._generate_fallback_response(prompt)
        except Exception as e:
            logger.error(f"Anthropic API Ïò§Î•ò: {e}")
            return self._generate_fallback_response(prompt)
    
    def _call_groq_api(self, prompt):
        """Groq API Ìò∏Ï∂ú"""
        try:
            from groq import Groq
            
            client = Groq(api_key=self.api_key)
            
            response = client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ÎãπÏã†ÏùÄ ÏùºÏ†ï Í¥ÄÎ¶¨Î•º ÎèÑÏôÄÏ£ºÎäî AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. JSON ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            
            return response.choices[0].message.content
            
        except ImportError:
            logger.error("groq Ìå®ÌÇ§ÏßÄÍ∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§")
            return self._generate_fallback_response(prompt)
        except Exception as e:
            logger.error(f"Groq API Ïò§Î•ò: {e}")
            return self._generate_fallback_response(prompt)
    
    def _generate_fallback_response(self, prompt):
        """API Ìò∏Ï∂ú Ïã§Ìå® Ïãú Í∏∞Î≥∏ ÏùëÎãµ ÏÉùÏÑ±"""
        current_time = get_current_datetime()
        
        # ÌîÑÎ°¨ÌîÑÌä∏ÏóêÏÑú ÌÇ§ÏõåÎìú Ï∂îÏ∂úÌïòÏó¨ Ï†ÅÏ†àÌïú Ï†úÎ™© ÏÉùÏÑ±
        title = "ÏÉà ÏùºÏ†ï"
        if "Ïö¥Îèô" in prompt:
            title = "Ïö¥Îèô"
        elif "ÌöåÏùò" in prompt or "ÎØ∏ÌåÖ" in prompt:
            title = "ÌöåÏùò"
        elif "Í≥µÎ∂Ä" in prompt or "ÌïôÏäµ" in prompt:
            title = "Í≥µÎ∂Ä"
        elif "ÏïΩÏÜç" in prompt:
            title = "ÏïΩÏÜç"
        elif "ÏûëÏóÖ" in prompt:
            title = "ÏûëÏóÖ"
        
        return f"""{{
            "title": "{title}",
            "description": "ÏùºÏ†ïÏù¥ ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§",
            "suggested_date": "{current_time.strftime('%Y-%m-%d')}",
            "suggested_start_time": "09:00",
            "suggested_end_time": "10:00",
            "location": "",
            "priority": "MEDIUM",
            "attendees": [],
            "reasoning": "Í∏∞Î≥∏ ÏùºÏ†ï Ï†úÏïàÏûÖÎãàÎã§."
        }}"""


# ÏùºÏ†ï Í¥ÄÎ¶¨ Î∑∞ - LLM ÏûêÎèô Ï†úÎ™© ÏÉùÏÑ± Í∞ïÌôî
class ScheduleManagementView(APIView):
    """ÏùºÏ†ï Í¥ÄÎ¶¨ Î©îÏù∏ Î∑∞ - ÌÜ†ÌÅ∞ Ïù∏Ï¶ù Ï†ÅÏö©"""
    authentication_classes = [DebugTokenAuthentication, SessionAuthentication]
    permission_classes = [IsAuthenticated]
    
    def __init__(self):
        super().__init__()
    
    def get_optimizer(self):
        """ÌïÑÏöîÌï† ÎïåÎßå optimizer Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±"""
        if not hasattr(self, '_optimizer'):
            self._optimizer = ScheduleOptimizerBot()
        return self._optimizer
    
    def get(self, request):
        """ÏÇ¨Ïö©ÏûêÏùò ÏùºÏ†ï Î™©Î°ù Ï°∞Ìöå"""
        logger.info(f"ÏùºÏ†ï Ï°∞Ìöå ÏöîÏ≤≠ - ÏÇ¨Ïö©Ïûê: {request.user.username}")
        
        try:
            schedules = Schedule.objects.filter(user=request.user).order_by('start_time')
            
            # ÎÇ†Ïßú ÌïÑÌÑ∞ÎßÅ
            start_date = request.query_params.get('start_date')
            end_date = request.query_params.get('end_date')
            
            if start_date:
                schedules = schedules.filter(start_time__date__gte=start_date)
            if end_date:
                schedules = schedules.filter(end_time__date__lte=end_date)
            
            serializer = ScheduleSerializer(schedules, many=True)
            logger.info(f"ÏùºÏ†ï Ï°∞Ìöå ÏÑ±Í≥µ: {len(serializer.data)}Í∞ú ÏùºÏ†ï Î∞òÌôò")
            return Response(serializer.data)
            
        except Exception as e:
            logger.error(f"ÏùºÏ†ï Ï°∞Ìöå Ïã§Ìå®: {str(e)}")
            return Response(
                {'error': f'ÏùºÏ†ï Ï°∞Ìöå Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}'}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _generate_smart_title(self, request_text):
        """LLMÏùÑ ÏÇ¨Ïö©Ìï¥ Ïä§ÎßàÌä∏Ìïú ÏùºÏ†ï Ï†úÎ™© ÏÉùÏÑ±"""
        try:
            optimizer = self.get_optimizer()
            
            # Ï†úÎ™© ÏÉùÏÑ± Ï†ÑÏö© ÌîÑÎ°¨ÌîÑÌä∏
            title_prompt = f"""
            Îã§Ïùå ÏùºÏ†ï ÏöîÏ≤≠ÏóêÏÑú Ï†ÅÏ†àÌïú ÏùºÏ†ï Ï†úÎ™©ÏùÑ ÌïúÍµ≠Ïñ¥Î°ú Í∞ÑÎã®ÌïòÍ≥† Î™ÖÌôïÌïòÍ≤å ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
            
            ÏöîÏ≤≠ ÎÇ¥Ïö©: "{request_text}"
            
            Í∑úÏπô:
            1. 10Í∏ÄÏûê Ïù¥ÎÇ¥Î°ú Í∞ÑÎã®ÌïòÍ≤å
            2. Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† ÏùòÎØ∏ÏûàÍ≤å
            3. Ïù¥Î™®ÏßÄ ÏÇ¨Ïö©ÌïòÏßÄ ÎßàÏÑ∏Ïöî
            4. Ï†úÎ™©Îßå Î∞òÌôòÌïòÏÑ∏Ïöî (Îî∞Ïò¥ÌëúÎÇò ÏÑ§Î™Ö ÏóÜÏù¥)
            
            ÏòàÏãú:
            - "ÎÇ¥Ïùº Ïò§ÌõÑ 2ÏãúÏóê ÌöåÏùò" ‚Üí "ÌåÄ ÌöåÏùò"
            - "Ï£ºÎßêÏóê Ïö¥ÎèôÌïòÍ∏∞" ‚Üí "Ï£ºÎßê Ïö¥Îèô"
            - "ÏπúÍµ¨ÏôÄ Ïπ¥ÌéòÏóêÏÑú ÎßåÎÇòÍ∏∞" ‚Üí "ÏπúÍµ¨ÏôÄ Ïπ¥Ìéò"
            - "ÌîÑÎ°úÏ†ùÌä∏ ÏûëÏóÖ" ‚Üí "ÌîÑÎ°úÏ†ùÌä∏ ÏûëÏóÖ"
            """
            
            suggestions = optimizer.get_ai_suggestions(title_prompt, "title")
            
            # AI ÏùëÎãµÏóêÏÑú Ï†úÎ™© Ï∂îÏ∂ú
            for key, response in suggestions.items():
                if response and len(response.strip()) > 0:
                    # ÏùëÎãµÏóêÏÑú ÍπîÎÅîÌïú Ï†úÎ™©Îßå Ï∂îÏ∂ú
                    lines = response.strip().split('\n')
                    for line in lines:
                        clean_line = line.strip().strip('"\'`').strip()
                        # ÎÑàÎ¨¥ Í∏∏Í±∞ÎÇò ÏßßÏßÄ ÏïäÏùÄ Ï†ÅÏ†àÌïú Ï†úÎ™© Ï∞æÍ∏∞
                        if 2 <= len(clean_line) <= 20 and not clean_line.startswith('Ï†úÎ™©:'):
                            logger.info(f"LLM ÏÉùÏÑ± Ï†úÎ™©: {clean_line}")
                            return clean_line
            
            # AI ÏùëÎãµÏù¥ Î∂ÄÏ†ÅÏ†àÌïòÎ©¥ Í∏∞Î≥∏ Ï†úÎ™© ÏÉùÏÑ±
            logger.warning("LLM Ï†úÎ™© ÏÉùÏÑ± Ïã§Ìå®, Í∏∞Î≥∏ Ï†úÎ™© ÏÇ¨Ïö©")
            return self._extract_schedule_title(request_text)
            
        except Exception as e:
            logger.error(f"Ïä§ÎßàÌä∏ Ï†úÎ™© ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")
            return self._extract_schedule_title(request_text)
    
    def post(self, request):
        """ÏÉàÎ°úÏö¥ ÏùºÏ†ï ÏÉùÏÑ± ÏöîÏ≤≠"""
        logger.info(f"ÏùºÏ†ï ÏÉùÏÑ± ÏöîÏ≤≠ - ÏÇ¨Ïö©Ïûê: {request.user.username}")
        
        try:
            request_text = request.data.get('request_text', '')
            existing_schedules = request.data.get('existing_schedules', [])
            
            if not request_text:
                return Response({'error': 'ÏöîÏ≤≠ ÌÖçÏä§Ìä∏Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.'}, 
                            status=status.HTTP_400_BAD_REQUEST)
            
            user = request.user
            
            # Ïó¨Îü¨ ÏùºÏ†ï ÏöîÏ≤≠Ïù∏ÏßÄ ÌôïÏù∏
            schedule_requests = parse_multiple_schedules_backend(request_text)
            target_date = parse_date_from_request(request_text)
            target_datetime = get_current_datetime()
            
            logger.info(f"ÌååÏã±Îêú ÏùºÏ†ï ÏöîÏ≤≠: {len(schedule_requests)}Í∞ú")

            if len(schedule_requests) > 1:
                # Ïó¨Îü¨ ÏùºÏ†ï Ï≤òÎ¶¨ - Ï∂©Îèå Î∞©ÏßÄ Í∞ïÌôî
                multiple_schedules = []
                all_individual_suggestions = []
                cumulative_existing_schedules = existing_schedules.copy()  # ÎàÑÏ†Å ÏùºÏ†ï Í¥ÄÎ¶¨
                
                logger.info(f"Ïó¨Îü¨ ÏùºÏ†ï Ï≤òÎ¶¨ ÏãúÏûë: {len(schedule_requests)}Í∞ú ÏùºÏ†ï")
                
                for i, single_request in enumerate(schedule_requests):
                    logger.info(f"Ï≤òÎ¶¨ Ï§ëÏù∏ ÏùºÏ†ï {i+1}/{len(schedule_requests)}: {single_request}")
                    
                    # LLMÏù¥ Í∞Å ÏùºÏ†ïÏùò Ï†úÎ™©ÏùÑ ÏûêÎèô ÏÉùÏÑ±
                    smart_title = self._generate_smart_title(single_request)
                    
                    # ÏãúÍ∞Ñ Ï†ïÎ≥¥ Ï∂îÏ∂ú
                    parsed_start, parsed_duration = self._extract_time_info(single_request)
                    
                    # Í∏∞Î≥∏ ÏãúÏûë ÏãúÍ∞Ñ ÏÑ§Ï†ï (ÏÇ¨Ïö©Ïûê ÏßÄÏ†ïÏù¥ ÏóÜÏúºÎ©¥ ÏàúÏ∞®Ï†ÅÏúºÎ°ú)
                    if parsed_start is not None:
                        start_hour = parsed_start
                        logger.info(f"ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï ÏãúÍ∞Ñ ÏÇ¨Ïö©: {start_hour}Ïãú")
                    else:
                        start_hour = 9 + (i * 2)  # 9Ïãú, 11Ïãú, 13Ïãú, 15Ïãú... ÏàúÏ∞®Ï†Å Î∞∞Ï†ï
                        logger.info(f"Í∏∞Î≥∏ ÏãúÍ∞Ñ ÏÇ¨Ïö©: {start_hour}Ïãú")
                        
                    duration_hours = parsed_duration or 1

                    # Ï∂©Îèå Î∞©ÏßÄ ÏãúÍ∞Ñ Í≥ÑÏÇ∞ - ÎàÑÏ†ÅÎêú Í∏∞Ï°¥ ÏùºÏ†ï Ìè¨Ìï®
                    logger.info(f"Ï∂©Îèå Î∞©ÏßÄ Í≥ÑÏÇ∞ ÏãúÏûë - ÌòÑÏû¨ ÎàÑÏ†Å ÏùºÏ†ï Í∞úÏàò: {len(cumulative_existing_schedules)}")
                    schedule_start_dt, schedule_end_dt = self._find_non_conflicting_time(
                        cumulative_existing_schedules, start_hour, duration_hours, target_date
                    )

                    optimized_schedule = {
                        "title": smart_title,  # LLMÏù¥ ÏÉùÏÑ±Ìïú Ïä§ÎßàÌä∏ Ï†úÎ™©
                        "description": f"AIÍ∞Ä Î∂ÑÏÑùÌïú ÏùºÏ†ïÏûÖÎãàÎã§: {single_request}",
                        "suggested_date": target_datetime.strftime('%Y-%m-%d'),
                        "suggested_start_time": schedule_start_dt.strftime('%H:%M'),
                        "suggested_end_time": schedule_end_dt.strftime('%H:%M'),
                        "location": self._extract_schedule_location(single_request),
                        "priority": "HIGH",
                        "attendees": [],
                        "reasoning": f"{i + 1}Î≤àÏß∏ ÏùºÏ†ï: {single_request}. Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º Ï∂©ÎèåÌïòÏßÄ ÏïäÎäî {schedule_start_dt.strftime('%H:%M')}-{schedule_end_dt.strftime('%H:%M')} ÏãúÍ∞ÑÏúºÎ°ú Î∞∞Ï†ïÌñàÏäµÎãàÎã§."
                    }

                    multiple_schedules.append(optimized_schedule)
                    
                    # Î∞©Í∏à Ï∂îÍ∞ÄÌïú ÏùºÏ†ïÏùÑ ÎàÑÏ†Å ÏùºÏ†ï Î™©Î°ùÏóê Ï∂îÍ∞Ä (Îã§Ïùå ÏùºÏ†ï Ï≤òÎ¶¨ Ïãú Í≥†Î†§)
                    cumulative_existing_schedules.append({
                        'start_time': schedule_start_dt.strftime('%Y-%m-%dT%H:%M:%S'),
                        'end_time': schedule_end_dt.strftime('%Y-%m-%dT%H:%M:%S'),
                        'title': smart_title
                    })
                    
                    logger.info(f"ÏùºÏ†ï {i+1} ÏôÑÎ£å: {smart_title} ({schedule_start_dt.strftime('%H:%M')}-{schedule_end_dt.strftime('%H:%M')})")
                    
                    # Í∞Å AIÎ≥Ñ Í∞úÎ≥Ñ Ï†úÏïà ÏÉùÏÑ±
                    for ai_type in ['gpt', 'claude', 'mixtral']:
                        individual_suggestion = optimized_schedule.copy()
                        individual_suggestion['source'] = ai_type
                        individual_suggestion['reasoning'] = f"{ai_type.upper()}Í∞Ä Î∂ÑÏÑùÌïú Í≤∞Í≥ºÏûÖÎãàÎã§."
                        all_individual_suggestions.append(individual_suggestion)
                
                logger.info(f"Ïó¨Îü¨ ÏùºÏ†ï Ï≤òÎ¶¨ ÏôÑÎ£å: Ï¥ù {len(multiple_schedules)}Í∞ú ÏùºÏ†ï ÏÉùÏÑ±")
                
                response_data = {
                    'request_id': int(datetime.now().timestamp()),
                    'multiple_schedules': multiple_schedules,
                    'optimized_suggestion': multiple_schedules[0] if multiple_schedules else {},
                    'confidence_score': 0.92,
                    'individual_suggestions': all_individual_suggestions,
                    'ai_analysis': {
                        'analysis_summary': f"Ï¥ù {len(schedule_requests)}Í∞úÏùò ÏùºÏ†ïÏùÑ Î∂ÑÏÑùÌïòÏó¨ Ï∂©Îèå ÏóÜÎäî ÏµúÏ†Å ÏãúÍ∞ÑÎåÄÎ°ú Î∞∞Ï†ïÌñàÏäµÎãàÎã§.",
                        'reasoning': f"Ïó¨Îü¨ ÏùºÏ†ïÏùÑ {target_date.strftime('%YÎÖÑ %mÏõî %dÏùº')}Ïóê ÏãúÍ∞Ñ ÏàúÏÑúÎåÄÎ°ú Î∞∞ÏπòÌïòÏó¨ Î™®Îì† Ï∂©ÎèåÏùÑ Î∞©ÏßÄÌñàÏäµÎãàÎã§.",
                        'models_used': ["gpt", "claude", "mixtral"]
                    },
                    'has_conflicts': False,
                    'conflicts': [],
                    'analysis_summary': f"{len(schedule_requests)}Í∞ú ÏùºÏ†ïÏóê ÎåÄÌï¥ AIÍ∞Ä Ï∂©Îèå ÏóÜÏù¥ Î∂ÑÏÑùÌïú Í≤∞Í≥ºÏûÖÎãàÎã§.",
                    'is_multiple_schedule': True
                }
                
            else:
                # Îã®Ïùº ÏùºÏ†ï Ï≤òÎ¶¨ - Ï∂©Îèå Î∞©ÏßÄ Í∞ïÌôî
                logger.info("Îã®Ïùº ÏùºÏ†ï Ï≤òÎ¶¨ ÏãúÏûë")
                
                # LLMÏù¥ Ï†úÎ™©ÏùÑ ÏûêÎèô ÏÉùÏÑ±
                smart_title = self._generate_smart_title(request_text)
                
                # ÏÇ¨Ïö©ÏûêÍ∞Ä ÏõêÌïòÎäî ÏãúÍ∞Ñ Ï∂îÏ∂ú
                parsed_start, parsed_duration = self._extract_time_info(request_text)
                preferred_start_hour = parsed_start if parsed_start is not None else 9
                duration_hours = parsed_duration or 1
                
                logger.info(f"Îã®Ïùº ÏùºÏ†ï ÏãúÍ∞Ñ Î∂ÑÏÑù: ÏÑ†Ìò∏ ÏãúÏûëÏãúÍ∞Ñ {preferred_start_hour}Ïãú, ÏßÄÏÜçÏãúÍ∞Ñ {duration_hours}ÏãúÍ∞Ñ")
                
                # Ï∂©Îèå Î∞©ÏßÄ ÏãúÍ∞Ñ Í≥ÑÏÇ∞
                schedule_start_dt, schedule_end_dt = self._find_non_conflicting_time(
                    existing_schedules, preferred_start_hour, duration_hours, target_date
                )
                
                # AIÏóêÍ≤å Ï∂©Îèå ÏóÜÎäî ÏãúÍ∞ÑÏúºÎ°ú Ï†úÏïà ÏöîÏ≤≠
                enhanced_prompt = f"""
                ÏÇ¨Ïö©ÏûêÏùò ÏùºÏ†ï ÏöîÏ≤≠ÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÏµúÏ†ÅÏùò ÏùºÏ†ïÏùÑ Ï†úÏïàÌï¥Ï£ºÏÑ∏Ïöî.
                
                ÏöîÏ≤≠ ÎÇ¥Ïö©: {request_text}
                Î™©Ìëú ÎÇ†Ïßú: {target_date.strftime('%YÎÖÑ %mÏõî %dÏùº')} ({self._get_weekday_korean(target_date)})
                Í∏∞Ï°¥ ÏùºÏ†ïÎì§: {len(existing_schedules)}Í∞ú ÏùºÏ†ïÏù¥ Ïù¥ÎØ∏ ÏûàÏäµÎãàÎã§
                Î∞∞Ï†ïÎêú ÏãúÍ∞Ñ: {schedule_start_dt.strftime('%H:%M')}-{schedule_end_dt.strftime('%H:%M')} (Ï∂©Îèå Î∞©ÏßÄÎê®)
                
                Îã§Ïùå ÌòïÏãùÏúºÎ°ú JSON ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî:
                {{
                    "title": "{smart_title}",
                    "description": "ÏÉÅÏÑ∏ ÏÑ§Î™Ö",
                    "suggested_date": "{target_date.strftime('%Y-%m-%d')}",
                    "suggested_start_time": "{schedule_start_dt.strftime('%H:%M')}",
                    "suggested_end_time": "{schedule_end_dt.strftime('%H:%M')}",
                    "location": "Ïû•ÏÜå",
                    "priority": "MEDIUM",
                    "attendees": [],
                    "reasoning": "Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º Ï∂©ÎèåÌïòÏßÄ ÏïäÎäî ÏµúÏ†Å ÏãúÍ∞ÑÏûÖÎãàÎã§"
                }}
                """
                
                try:
                    optimizer = self.get_optimizer()
                    suggestions = optimizer.get_ai_suggestions(enhanced_prompt)
                    optimized_result = optimizer.analyze_and_optimize_suggestions(
                        suggestions, f"ÏùºÏ†ï ÏöîÏ≤≠: {request_text}"
                    )
                    
                    # Ï∂©Îèå Î∞©ÏßÄÎêú ÏãúÍ∞ÑÏúºÎ°ú ÎçÆÏñ¥Ïì∞Í∏∞ Î≥¥Ïû•
                    if 'optimized_suggestion' in optimized_result:
                        optimized_result['optimized_suggestion']['title'] = smart_title
                        optimized_result['optimized_suggestion']['suggested_date'] = target_date.strftime('%Y-%m-%d')
                        optimized_result['optimized_suggestion']['suggested_start_time'] = schedule_start_dt.strftime('%H:%M')
                        optimized_result['optimized_suggestion']['suggested_end_time'] = schedule_end_dt.strftime('%H:%M')
                        optimized_result['optimized_suggestion']['reasoning'] = f"Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º Ï∂©ÎèåÌïòÏßÄ ÏïäÎäî {schedule_start_dt.strftime('%H:%M')}-{schedule_end_dt.strftime('%H:%M')} ÏãúÍ∞ÑÏúºÎ°ú Î∞∞Ï†ïÌñàÏäµÎãàÎã§."
                    
                    response_data = {
                        'request_id': int(datetime.now().timestamp()),
                        'optimized_suggestion': optimized_result.get('optimized_suggestion', {}),
                        'confidence_score': optimized_result.get('confidence_score', 0.9),
                        'ai_analysis': optimized_result.get('ai_analysis', {}),
                        'individual_suggestions': optimized_result.get('individual_suggestions', []),
                        'has_conflicts': False,
                        'conflicts': [],
                        'analysis_summary': "AIÍ∞Ä Í∏∞Ï°¥ ÏùºÏ†ïÍ≥ºÏùò Ï∂©ÎèåÏùÑ Î∞©ÏßÄÌïòÏó¨ ÏµúÏ†Å ÏãúÍ∞ÑÏùÑ Î∞∞Ï†ïÌñàÏäµÎãàÎã§.",
                        'is_multiple_schedule': False
                    }
                    
                except Exception as e:
                    logger.error(f"Îã®Ïùº ÏùºÏ†ï AI Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {e}")
                    # AI Ï≤òÎ¶¨ Ïã§Ìå® ÏãúÏóêÎèÑ Ï∂©Îèå Î∞©ÏßÄÎêú Í∏∞Î≥∏ ÏùëÎãµ ÏÉùÏÑ±
                    response_data = {
                        'request_id': int(datetime.now().timestamp()),
                        'optimized_suggestion': {
                            "title": smart_title,
                            "description": f"ÏöîÏ≤≠ÌïòÏã† ÏùºÏ†ïÏûÖÎãàÎã§: {request_text}",
                            "suggested_date": target_date.strftime('%Y-%m-%d'),
                            "suggested_start_time": schedule_start_dt.strftime('%H:%M'),
                            "suggested_end_time": schedule_end_dt.strftime('%H:%M'),
                            "location": self._extract_schedule_location(request_text),
                            "priority": "MEDIUM",
                            "attendees": [],
                            "reasoning": f"Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º Ï∂©ÎèåÌïòÏßÄ ÏïäÎäî {schedule_start_dt.strftime('%H:%M')}-{schedule_end_dt.strftime('%H:%M')} ÏãúÍ∞ÑÏúºÎ°ú Î∞∞Ï†ïÌñàÏäµÎãàÎã§."
                        },
                        'confidence_score': 0.8,
                        'ai_analysis': {
                            'analysis_summary': 'Ï∂©Îèå Î∞©ÏßÄ ÏãúÍ∞Ñ Î∞∞Ï†ïÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§.',
                            'reasoning': 'Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º Í≤πÏπòÏßÄ ÏïäÎäî ÏµúÏ†Å ÏãúÍ∞ÑÏùÑ Ï∞æÏïòÏäµÎãàÎã§.',
                            'models_used': []
                        },
                        'individual_suggestions': [],
                        'has_conflicts': False,
                        'conflicts': [],
                        'analysis_summary': "Ï∂©Îèå Î∞©ÏßÄ ÏïåÍ≥†Î¶¨Ï¶òÏù¥ ÏµúÏ†Å ÏãúÍ∞ÑÏùÑ Î∞∞Ï†ïÌñàÏäµÎãàÎã§.",
                        'is_multiple_schedule': False
                    }
                
                logger.info(f"Îã®Ïùº ÏùºÏ†ï Ï≤òÎ¶¨ ÏôÑÎ£å: {smart_title} ({schedule_start_dt.strftime('%H:%M')}-{schedule_end_dt.strftime('%H:%M')})")
            
            logger.info("ÏùºÏ†ï ÏÉùÏÑ± ÏöîÏ≤≠ Ï≤òÎ¶¨ ÏôÑÎ£å")
            return Response(response_data, status=status.HTTP_201_CREATED)
            
        except Exception as e:
            logger.error(f"ÏùºÏ†ï ÏÉùÏÑ± ÏöîÏ≤≠ Ï≤òÎ¶¨ Ïã§Ìå®: {str(e)}")
            return Response({
                'error': f'ÏùºÏ†ï ÏÉùÏÑ± ÏöîÏ≤≠ Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _extract_time_info(self, text):
        """ÌÖçÏä§Ìä∏ÏóêÏÑú ÏãúÍ∞Ñ Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        import re
        start_hour = None
        duration_hours = 1

        is_pm = 'Ïò§ÌõÑ' in text
        is_am = 'Ïò§Ï†Ñ' in text

        # "Ïò§ÌõÑ 3-5Ïãú"ÏôÄ Í∞ôÏùÄ Í≤ΩÏö∞ Ï≤òÎ¶¨
        time_range = re.search(r'(\d{1,2})\s*[-~]\s*(\d{1,2})\s*Ïãú', text)
        if time_range:
            start = int(time_range.group(1))
            end = int(time_range.group(2))

            if is_pm:
                if start < 12:
                    start += 12
                if end < 12:
                    end += 12
            elif is_am:
                if start == 12:
                    start = 0
                if end == 12:
                    end = 0

            start_hour = start
            duration_hours = end - start
            return start_hour, duration_hours

        # "2ÏãúÍ∞Ñ"Îßå ÏûàÎäî Í≤ΩÏö∞
        dur_match = re.search(r'(\d{1,2})\s*ÏãúÍ∞Ñ', text)
        if dur_match:
            duration_hours = int(dur_match.group(1))

        # Îã®Ïùº ÏãúÍ∞Å: "Ïò§ÌõÑ 3Ïãú"
        single_time_match = re.search(r'(Ïò§Ï†Ñ|Ïò§ÌõÑ)?\s*(\d{1,2})\s*Ïãú', text)
        if single_time_match:
            hour = int(single_time_match.group(2))
            if single_time_match.group(1) == 'Ïò§ÌõÑ' and hour < 12:
                hour += 12
            elif single_time_match.group(1) == 'Ïò§Ï†Ñ' and hour == 12:
                hour = 0
            start_hour = hour

        return start_hour, duration_hours

    def _find_non_conflicting_time(self, existing_schedules, start_hour, duration_hours, target_date):
        """Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º Í≤πÏπòÏßÄ ÏïäÎäî ÏãúÍ∞ÑÎåÄÎ•º ÌÉêÏÉâ - ÌòÑÏã§Ï†ÅÏù∏ ÏãúÍ∞ÑÎåÄÎßå Í≥†Î†§"""
        from datetime import datetime, timedelta, time
        
        logger.info(f"Ï∂©Îèå Î∞©ÏßÄ ÏãúÍ∞Ñ ÌÉêÏÉâ ÏãúÏûë: ÏõêÌïòÎäî ÏãúÏûëÏãúÍ∞Ñ {start_hour}Ïãú, ÏßÄÏÜçÏãúÍ∞Ñ {duration_hours}ÏãúÍ∞Ñ")
        logger.info(f"Í∏∞Ï°¥ ÏùºÏ†ï Í∞úÏàò: {len(existing_schedules)}")

        # ÌòÑÏã§Ï†ÅÏù∏ ÌôúÎèô ÏãúÍ∞ÑÎåÄ Ï†ïÏùò
        WORK_START = 7   # Ïò§Ï†Ñ 7ÏãúÎ∂ÄÌÑ∞
        WORK_END = 22    # Ïò§ÌõÑ 10ÏãúÍπåÏßÄ
        
        def parse_existing_schedule_time(schedule):
            """Í∏∞Ï°¥ ÏùºÏ†ï ÏãúÍ∞Ñ ÌååÏã±"""
            try:
                if 'start_time' in schedule and 'end_time' in schedule:
                    start_str = schedule['start_time']
                    end_str = schedule['end_time']
                    
                    if 'T' in start_str:
                        start_dt = datetime.strptime(start_str, '%Y-%m-%dT%H:%M:%S')
                        end_dt = datetime.strptime(end_str, '%Y-%m-%dT%H:%M:%S')
                    else:
                        start_dt = datetime.strptime(start_str, '%Y-%m-%d %H:%M:%S')
                        end_dt = datetime.strptime(end_str, '%Y-%m-%d %H:%M:%S')
                        
                    return start_dt, end_dt
                        
            except (ValueError, KeyError) as e:
                logger.warning(f"ÏùºÏ†ï ÏãúÍ∞Ñ ÌååÏã± Ïã§Ìå®: {schedule}, Ïò§Î•ò: {e}")
                return None, None
            
            return None, None

        def is_conflicting(new_start, new_end, schedules):
            """ÏãúÍ∞Ñ Í≤πÏπ® Í≤ÄÏÇ¨"""
            logger.info(f"Ï∂©Îèå Í≤ÄÏÇ¨: ÏÉà ÏùºÏ†ï {new_start.strftime('%H:%M')}-{new_end.strftime('%H:%M')}")
            
            for i, schedule in enumerate(schedules):
                existing_start, existing_end = parse_existing_schedule_time(schedule)
                
                if existing_start is None or existing_end is None:
                    continue
                
                # Í∞ôÏùÄ ÎÇ†ÏßúÏù∏ÏßÄ ÌôïÏù∏
                if existing_start.date() != target_date:
                    continue
                    
                logger.info(f"Í∏∞Ï°¥ ÏùºÏ†ï {i+1}: {existing_start.strftime('%H:%M')}-{existing_end.strftime('%H:%M')}")
                
                # Í≤πÏπòÎäîÏßÄ Í≤ÄÏÇ¨
                if (new_start < existing_end) and (existing_start < new_end):
                    logger.warning(f"‚ö†Ô∏è ÏãúÍ∞Ñ Ï∂©Îèå Î∞úÍ≤¨! ÏÉà ÏùºÏ†ïÍ≥º Í∏∞Ï°¥ ÏùºÏ†ï {i+1}Ïù¥ Í≤πÏπ®")
                    return True
                    
            return False

        def is_realistic_time(hour, duration):
            """ÌòÑÏã§Ï†ÅÏù∏ ÏãúÍ∞ÑÏù∏ÏßÄ ÌôïÏù∏"""
            end_hour = hour + duration
            
            # ÏóÖÎ¨¥ÏãúÍ∞Ñ ÎÇ¥Ïóê ÏûàÎäîÏßÄ ÌôïÏù∏
            if hour < WORK_START or end_hour > WORK_END:
                return False
                
            # Ï†êÏã¨ÏãúÍ∞Ñ ÌîºÌïòÍ∏∞ (12-13Ïãú)
            if hour <= 12 and end_hour >= 13:
                return False
                
            return True

        # ÏÇ¨Ïö©ÏûêÍ∞Ä ÏßÄÏ†ïÌïú ÏãúÍ∞ÑÏù¥ ÌòÑÏã§Ï†ÅÏù∏ÏßÄ Î®ºÏ†Ä ÌôïÏù∏
        if start_hour < WORK_START or start_hour >= WORK_END:
            logger.warning(f"ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï ÏãúÍ∞Ñ {start_hour}ÏãúÍ∞Ä ÎπÑÌòÑÏã§Ï†ÅÏûÑ. Ïò§Ï†Ñ 9ÏãúÎ°ú Ï°∞Ï†ï")
            start_hour = 9

        # ÌòÑÏã§Ï†ÅÏù∏ ÏãúÍ∞ÑÎåÄÏóêÏÑúÎßå ÌÉêÏÉâ
        realistic_hours = []
        
        # Ïò§Ï†Ñ ÏãúÍ∞ÑÎåÄ (7-12Ïãú)
        for h in range(7, 12):
            if h + duration_hours <= 12:  # Ï†êÏã¨ÏãúÍ∞Ñ Ï†ÑÏóê ÎÅùÎÇòÏïº Ìï®
                realistic_hours.append(h)
        
        # Ïò§ÌõÑ ÏãúÍ∞ÑÎåÄ (13-22Ïãú)
        for h in range(13, 22):
            if h + duration_hours <= 22:  # Ï†ÄÎÖÅ 10Ïãú Ï†ÑÏóê ÎÅùÎÇòÏïº Ìï®
                realistic_hours.append(h)

        # ÏÇ¨Ïö©Ïûê ÏÑ†Ìò∏ ÏãúÍ∞ÑÎ∂ÄÌÑ∞ ÏãúÏûëÌïòÏó¨ ÌÉêÏÉâ
        search_order = []
        if start_hour in realistic_hours:
            search_order.append(start_hour)
        
        # ÏÑ†Ìò∏ ÏãúÍ∞Ñ Ï£ºÎ≥ÄÎ∂ÄÌÑ∞ ÌôïÏû• ÌÉêÏÉâ
        for offset in range(1, 8):
            if start_hour + offset in realistic_hours and start_hour + offset not in search_order:
                search_order.append(start_hour + offset)
            if start_hour - offset in realistic_hours and start_hour - offset not in search_order:
                search_order.append(start_hour - offset)
        
        # ÎÇòÎ®∏ÏßÄ ÌòÑÏã§Ï†ÅÏù∏ ÏãúÍ∞ÑÎì§ Ï∂îÍ∞Ä
        for h in realistic_hours:
            if h not in search_order:
                search_order.append(h)

        logger.info(f"ÌòÑÏã§Ï†ÅÏù∏ ÏãúÍ∞ÑÎåÄ ÌÉêÏÉâ ÏàúÏÑú: {search_order}")

        # ÌòÑÏã§Ï†ÅÏù∏ ÏãúÍ∞ÑÎåÄÏóêÏÑú Ï∂©Îèå ÏóÜÎäî ÏãúÍ∞Ñ Ï∞æÍ∏∞
        for attempt, hour in enumerate(search_order):
            try:
                candidate_start = datetime.combine(target_date, time(hour))
                candidate_end = candidate_start + timedelta(hours=duration_hours)
                
                logger.info(f"ÏãúÎèÑ {attempt + 1}: {candidate_start.strftime('%H:%M')}-{candidate_end.strftime('%H:%M')}")
                
                if not is_conflicting(candidate_start, candidate_end, existing_schedules):
                    logger.info(f"‚úÖ Ï∂©Îèå ÏóÜÎäî ÌòÑÏã§Ï†ÅÏù∏ ÏãúÍ∞Ñ Î∞úÍ≤¨: {candidate_start.strftime('%H:%M')}-{candidate_end.strftime('%H:%M')}")
                    return candidate_start, candidate_end
                    
            except Exception as e:
                logger.error(f"ÏãúÍ∞Ñ Í≥ÑÏÇ∞ Ïò§Î•ò: {e}")
                continue

        # Î™®Îì† ÌòÑÏã§Ï†ÅÏù∏ ÏãúÍ∞ÑÎåÄÏóê Ï∂©ÎèåÏù¥ ÏûàÎäî Í≤ΩÏö∞
        logger.warning("Î™®Îì† ÌòÑÏã§Ï†ÅÏù∏ ÏãúÍ∞ÑÎåÄÏóê Ï∂©Îèå Î∞úÏÉù")
        
        # Í∞ÄÏû• Îπà ÏãúÍ∞ÑÎåÄ Ï∞æÍ∏∞
        best_hour = 9  # Í∏∞Î≥∏Í∞í
        min_conflicts = float('inf')
        
        for hour in realistic_hours:
            try:
                candidate_start = datetime.combine(target_date, time(hour))
                candidate_end = candidate_start + timedelta(hours=duration_hours)
                
                # Ïù¥ ÏãúÍ∞ÑÎåÄÏùò Ï∂©Îèå Í∞úÏàò ÏÑ∏Í∏∞
                conflict_count = 0
                for schedule in existing_schedules:
                    existing_start, existing_end = parse_existing_schedule_time(schedule)
                    if existing_start and existing_start.date() == target_date:
                        if (candidate_start < existing_end) and (existing_start < candidate_end):
                            conflict_count += 1
                
                if conflict_count < min_conflicts:
                    min_conflicts = conflict_count
                    best_hour = hour
                    
            except Exception as e:
                logger.error(f"Ï∂©Îèå Í≥ÑÏÇ∞ Ïò§Î•ò: {e}")
                continue
        
        # ÏµúÏÜå Ï∂©Îèå ÏãúÍ∞ÑÎåÄÎ°ú Î∞∞Ï†ï
        final_start = datetime.combine(target_date, time(best_hour))
        final_end = final_start + timedelta(hours=duration_hours)
        
        logger.info(f"üîÑ ÏµúÏÜå Ï∂©Îèå ÏãúÍ∞ÑÏúºÎ°ú Î∞∞Ï†ï: {final_start.strftime('%H:%M')}-{final_end.strftime('%H:%M')} (Ï∂©Îèå {min_conflicts}Í∞ú)")
        return final_start, final_end

    def _extract_time_info(self, text):
        """ÌÖçÏä§Ìä∏ÏóêÏÑú ÏãúÍ∞Ñ Ï†ïÎ≥¥ Ï∂îÏ∂ú - ÌòÑÏã§Ï†ÅÏù∏ ÏãúÍ∞ÑÎßå Î∞òÌôò"""
        import re
        start_hour = None
        duration_hours = 1

        is_pm = 'Ïò§ÌõÑ' in text
        is_am = 'Ïò§Ï†Ñ' in text

        # "Ïò§ÌõÑ 3-5Ïãú"ÏôÄ Í∞ôÏùÄ Í≤ΩÏö∞ Ï≤òÎ¶¨
        time_range = re.search(r'(\d{1,2})\s*[-~]\s*(\d{1,2})\s*Ïãú', text)
        if time_range:
            start = int(time_range.group(1))
            end = int(time_range.group(2))

            if is_pm:
                if start < 12:
                    start += 12
                if end < 12:
                    end += 12
            elif is_am:
                if start == 12:
                    start = 0
                if end == 12:
                    end = 0

            # ÌòÑÏã§Ï†ÅÏù∏ ÏãúÍ∞ÑÏù∏ÏßÄ ÌôïÏù∏
            if 7 <= start <= 22 and 7 <= end <= 22:
                start_hour = start
                duration_hours = end - start
                return start_hour, duration_hours

        # "2ÏãúÍ∞Ñ"Îßå ÏûàÎäî Í≤ΩÏö∞
        dur_match = re.search(r'(\d{1,2})\s*ÏãúÍ∞Ñ', text)
        if dur_match:
            duration_hours = int(dur_match.group(1))

        # Îã®Ïùº ÏãúÍ∞Å: "Ïò§ÌõÑ 3Ïãú"
        single_time_match = re.search(r'(Ïò§Ï†Ñ|Ïò§ÌõÑ)?\s*(\d{1,2})\s*Ïãú', text)
        if single_time_match:
            hour = int(single_time_match.group(2))
            if single_time_match.group(1) == 'Ïò§ÌõÑ' and hour < 12:
                hour += 12
            elif single_time_match.group(1) == 'Ïò§Ï†Ñ' and hour == 12:
                hour = 0
            
            # ÌòÑÏã§Ï†ÅÏù∏ ÏãúÍ∞ÑÏù∏ÏßÄ ÌôïÏù∏
            if 7 <= hour <= 22:
                start_hour = hour

        # ÎπÑÌòÑÏã§Ï†ÅÏù∏ ÏãúÍ∞ÑÏù¥Î©¥ Í∏∞Î≥∏Í∞íÏúºÎ°ú ÏÑ§Ï†ï
        if start_hour is not None and (start_hour < 7 or start_hour > 22):
            logger.warning(f"ÎπÑÌòÑÏã§Ï†ÅÏù∏ ÏãúÍ∞Ñ {start_hour}Ïãú Í∞êÏßÄ, Ïò§Ï†Ñ 9ÏãúÎ°ú Ï°∞Ï†ï")
            start_hour = 9

        return start_hour, duration_hours
    def _extract_schedule_title(self, request):
        """ÏöîÏ≤≠ÏóêÏÑú Í∏∞Î≥∏ ÏùºÏ†ï Ï†úÎ™© Ï∂îÏ∂ú (fallbackÏö©)"""
        if 'Ïö¥Îèô' in request:
            return 'Ïö¥Îèô'
        elif 'ÎØ∏ÌåÖ' in request or 'ÌöåÏùò' in request:
            return 'ÌåÄ ÎØ∏ÌåÖ'
        elif 'Í≥µÎ∂Ä' in request or 'ÌïôÏäµ' in request:
            return 'ÌïôÏäµ ÏãúÍ∞Ñ'
        elif 'ÏûëÏóÖ' in request or 'ÏóÖÎ¨¥' in request:
            return 'ÏßëÏ§ë ÏûëÏóÖ'
        elif 'ÏïΩÏÜç' in request:
            return 'ÏïΩÏÜç'
        else:
            return 'ÏÉà ÏùºÏ†ï'

    def _extract_schedule_location(self, request):
        """ÏöîÏ≤≠ÏóêÏÑú Ïû•ÏÜå Ï∂îÏ∂ú"""
        if 'Ïö¥Îèô' in request or 'Ìó¨Ïä§' in request:
            return 'Ìó¨Ïä§Ïû•'
        elif 'ÎØ∏ÌåÖ' in request or 'ÌöåÏùò' in request:
            return 'ÌöåÏùòÏã§'
        elif 'Í≥µÎ∂Ä' in request or 'ÌïôÏäµ' in request:
            return 'ÎèÑÏÑúÍ¥Ä'
        elif 'Ïª§Ìîº' in request or 'Ïπ¥Ìéò' in request:
            return 'Ïπ¥Ìéò'
        else:
            return 'ÏÇ¨Î¨¥Ïã§'

    def _get_weekday_korean(self, date):
        """ÏöîÏùºÏùÑ ÌïúÍµ≠Ïñ¥Î°ú Î∞òÌôò"""
        weekdays = ['ÏõîÏöîÏùº', 'ÌôîÏöîÏùº', 'ÏàòÏöîÏùº', 'Î™©ÏöîÏùº', 'Í∏àÏöîÏùº', 'ÌÜ†ÏöîÏùº', 'ÏùºÏöîÏùº']
        return weekdays[date.weekday()]


# ÏàòÎèô ÏùºÏ†ï ÏÉùÏÑ± Î∑∞ (ÏÉàÎ°úÏö¥ ÏóîÎìúÌè¨Ïù∏Ìä∏)
@api_view(['POST'])
@authentication_classes([DebugTokenAuthentication, SessionAuthentication])
@permission_classes([IsAuthenticated])
def create_schedule(request):
    """ÏàòÎèôÏúºÎ°ú ÏùºÏ†ï ÏÉùÏÑ± - /api/schedule/create/ ÏóîÎìúÌè¨Ïù∏Ìä∏"""
    logger.info(f"ÏàòÎèô ÏùºÏ†ï ÏÉùÏÑ± ÏöîÏ≤≠ - ÏÇ¨Ïö©Ïûê: {request.user.username}")
    
    try:
        data = request.data.copy()
        
        # Ï†úÎ™©Ïù¥ ÏóÜÏúºÎ©¥ LLMÏù¥ ÏûêÎèô ÏÉùÏÑ±
        if not data.get('title'):
            description = data.get('description', '')
            if description:
                # Í∞ÑÎã®Ìïú Ï†úÎ™© ÏÉùÏÑ± Î°úÏßÅ
                try:
                    optimizer = ScheduleOptimizerBot()
                    title_prompt = f"""
                    Îã§Ïùå ÏÑ§Î™ÖÏóêÏÑú Ï†ÅÏ†àÌïú ÏùºÏ†ï Ï†úÎ™©ÏùÑ 10Í∏ÄÏûê Ïù¥ÎÇ¥Î°ú Í∞ÑÎã®ÌïòÍ≤å ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî:
                    "{description}"
                    
                    Ï†úÎ™©Îßå Î∞òÌôòÌïòÏÑ∏Ïöî (Îî∞Ïò¥ÌëúÎÇò ÏÑ§Î™Ö ÏóÜÏù¥).
                    """
                    suggestions = optimizer.get_ai_suggestions(title_prompt, "title")
                    
                    # AI ÏùëÎãµÏóêÏÑú Ï†úÎ™© Ï∂îÏ∂ú
                    for key, response in suggestions.items():
                        if response and len(response.strip()) > 0:
                            lines = response.strip().split('\n')
                            for line in lines:
                                clean_line = line.strip().strip('"\'`').strip()
                                if 2 <= len(clean_line) <= 15:
                                    data['title'] = clean_line
                                    logger.info(f"ÏûêÎèô ÏÉùÏÑ±Îêú Ï†úÎ™©: {clean_line}")
                                    break
                            if data.get('title'):
                                break
                    
                    # Ï†úÎ™© ÏÉùÏÑ±Ïóê Ïã§Ìå®ÌïòÎ©¥ Í∏∞Î≥∏Í∞í
                    if not data.get('title'):
                        data['title'] = 'ÏÉà ÏùºÏ†ï'
                        
                except Exception as e:
                    logger.error(f"Ï†úÎ™© ÏûêÎèô ÏÉùÏÑ± Ïã§Ìå®: {e}")
                    data['title'] = 'ÏÉà ÏùºÏ†ï'
            else:
                data['title'] = 'ÏÉà ÏùºÏ†ï'
        
        data['user'] = request.user.id
        
        serializer = ScheduleSerializer(data=data)
        if serializer.is_valid():
            schedule = serializer.save(user=request.user)
            logger.info(f"ÏàòÎèô ÏùºÏ†ï ÏÉùÏÑ± ÏÑ±Í≥µ: {schedule.id} - {schedule.title}")
            return Response({
                'message': 'ÏùºÏ†ïÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.',
                'schedule': serializer.data
            }, status=status.HTTP_201_CREATED)
        else:
            logger.warning(f"ÏàòÎèô ÏùºÏ†ï ÏÉùÏÑ± Ïã§Ìå® - Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù Ïò§Î•ò: {serializer.errors}")
            return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)
            
    except Exception as e:
        logger.error(f"ÏàòÎèô ÏùºÏ†ï ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")
        return Response({
            'error': f'ÏùºÏ†ï ÏÉùÏÑ± Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


# Í∏∞Ï°¥ Î∑∞Îì§...
@api_view(['POST'])
@authentication_classes([DebugTokenAuthentication, SessionAuthentication])
@permission_classes([IsAuthenticated])
def confirm_schedule(request, request_id):
    """AI Ï†úÏïàÎêú ÏùºÏ†ïÏùÑ ÌôïÏ†ïÌïòÏó¨ Ïã§Ï†ú ÏùºÏ†ïÏúºÎ°ú ÏÉùÏÑ±"""
    logger.info(f"ÏùºÏ†ï ÌôïÏ†ï ÏöîÏ≤≠ - ÏÇ¨Ïö©Ïûê: {request.user.username}, request_id: {request_id}")
    
    try:
        user = request.user
        ai_suggestion_data = request.data.get('ai_suggestion')
        
        if not ai_suggestion_data:
            return Response({
                'error': 'AI Ï†úÏïà Îç∞Ïù¥ÌÑ∞Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.'
            }, status=status.HTTP_400_BAD_REQUEST)
        
        is_multiple = ai_suggestion_data.get('is_multiple_schedule', False)
        
        if is_multiple and ai_suggestion_data.get('multiple_schedules'):
            # Ïó¨Îü¨ ÏùºÏ†ï Ï≤òÎ¶¨
            created_schedules = []
            
            for schedule_data in ai_suggestion_data['multiple_schedules']:
                try:
                    suggested_date = schedule_data.get('suggested_date')
                    suggested_start_time = schedule_data.get('suggested_start_time', '09:00')
                    suggested_end_time = schedule_data.get('suggested_end_time', '10:00')
                    
                    start_datetime = datetime.strptime(
                        f"{suggested_date} {suggested_start_time}",
                        '%Y-%m-%d %H:%M'
                    )
                    end_datetime = datetime.strptime(
                        f"{suggested_date} {suggested_end_time}",
                        '%Y-%m-%d %H:%M'
                    )
                    
                    schedule = Schedule.objects.create(
                        user=user,
                        title=schedule_data.get('title', 'ÏÉà ÏùºÏ†ï'),
                        description=schedule_data.get('description', 'AIÍ∞Ä Ï†úÏïàÌïú ÏùºÏ†ïÏûÖÎãàÎã§.'),
                        start_time=start_datetime,
                        end_time=end_datetime,
                        location=schedule_data.get('location', ''),
                        priority=schedule_data.get('priority', 'MEDIUM'),
                        attendees=json.dumps(schedule_data.get('attendees', []), ensure_ascii=False)
                    )
                    
                    created_schedules.append(schedule)
                    logger.info(f"Îã§Ï§ë ÏùºÏ†ï ÏÉùÏÑ± ÏÑ±Í≥µ: {schedule.id} - {schedule.title}")
                    
                except Exception as e:
                    logger.error(f"Í∞úÎ≥Ñ ÏùºÏ†ï ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")
                    continue
            
            if created_schedules:
                serializer = ScheduleSerializer(created_schedules, many=True)
                return Response({
                    'message': f'{len(created_schedules)}Í∞úÏùò ÏùºÏ†ïÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.',
                    'schedules': serializer.data
                }, status=status.HTTP_201_CREATED)
            else:
                return Response({
                    'error': 'ÏùºÏ†ï ÏÉùÏÑ±Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§.'
                }, status=status.HTTP_400_BAD_REQUEST)
        
        else:
            # Îã®Ïùº ÏùºÏ†ï Ï≤òÎ¶¨
            optimized_suggestion = ai_suggestion_data.get('optimized_suggestion')
            if not optimized_suggestion:
                return Response({
                    'error': 'ÏµúÏ†ÅÌôîÎêú Ï†úÏïà Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            try:
                suggested_date = optimized_suggestion.get('suggested_date')
                suggested_start_time = optimized_suggestion.get('suggested_start_time', '09:00')
                suggested_end_time = optimized_suggestion.get('suggested_end_time', '10:00')
                
                if 'T' in suggested_date:
                    suggested_date = suggested_date.split('T')[0]
                
                start_datetime = datetime.strptime(
                    f"{suggested_date} {suggested_start_time}",
                    '%Y-%m-%d %H:%M'
                )
                end_datetime = datetime.strptime(
                    f"{suggested_date} {suggested_end_time}",
                    '%Y-%m-%d %H:%M'
                )
                
            except (ValueError, TypeError) as e:
                logger.error(f"DateTime parsing error: {e}")
                now = datetime.now()
                start_datetime = now.replace(hour=9, minute=0, second=0, microsecond=0)
                end_datetime = now.replace(hour=10, minute=0, second=0, microsecond=0)
            
            schedule = Schedule.objects.create(
                user=user,
                title=optimized_suggestion.get('title', 'ÏÉà ÏùºÏ†ï'),
                description=optimized_suggestion.get('description', 'AIÍ∞Ä Ï†úÏïàÌïú ÏùºÏ†ïÏûÖÎãàÎã§.'),
                start_time=start_datetime,
                end_time=end_datetime,
                location=optimized_suggestion.get('location', ''),
                priority=optimized_suggestion.get('priority', 'MEDIUM'),
                attendees=json.dumps(optimized_suggestion.get('attendees', []), ensure_ascii=False)
            )
            
            serializer = ScheduleSerializer(schedule)
            logger.info(f"Îã®Ïùº ÏùºÏ†ï ÏÉùÏÑ± ÏÑ±Í≥µ: {schedule.id} - {schedule.title}")
            
            return Response({
                'message': 'AIÏùò Î∂ÑÏÑùÏùÑ ÌÜµÌï¥ ÏµúÏ†ÅÌôîÎêú ÏùºÏ†ïÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.',
                'schedule': serializer.data
            }, status=status.HTTP_201_CREATED)
            
    except Exception as e:
        logger.error(f"ÏùºÏ†ï ÌôïÏ†ï Ïã§Ìå®: {str(e)}")
        return Response({
            'error': f'ÏùºÏ†ï ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}'
        }, status=status.HTTP_400_BAD_REQUEST)


@api_view(['PUT', 'DELETE'])
@authentication_classes([DebugTokenAuthentication, SessionAuthentication])
@permission_classes([IsAuthenticated])
def manage_schedule(request, schedule_id):
    """ÏùºÏ†ï ÏàòÏ†ï ÎòêÎäî ÏÇ≠Ï†ú"""
    try:
        schedule = get_object_or_404(Schedule, id=schedule_id, user=request.user)
        
        if request.method == 'PUT':
            # ÏùºÏ†ï ÏàòÏ†ï
            serializer = ScheduleSerializer(schedule, data=request.data, partial=True)
            if serializer.is_valid():
                serializer.save()
                logger.info(f"ÏùºÏ†ï ÏàòÏ†ï ÏÑ±Í≥µ: {schedule_id}")
                return Response(serializer.data)
            else:
                return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)
        
        elif request.method == 'DELETE':
            # ÏùºÏ†ï ÏÇ≠Ï†ú
            schedule.delete()
            logger.info(f"ÏùºÏ†ï ÏÇ≠Ï†ú ÏÑ±Í≥µ: {schedule_id}")
            return Response({'message': 'ÏùºÏ†ïÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏÇ≠Ï†úÎêòÏóàÏäµÎãàÎã§.'}, 
                          status=status.HTTP_204_NO_CONTENT)
            
    except Exception as e:
        logger.error(f"ÏùºÏ†ï Í¥ÄÎ¶¨ Ïã§Ìå®: {str(e)}")
        return Response({
            'error': f'ÏùºÏ†ï Í¥ÄÎ¶¨ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


# Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
def parse_date_from_request(request_text):
    """ÏöîÏ≤≠ ÌÖçÏä§Ìä∏ÏóêÏÑú ÎÇ†Ïßú ÌååÏã±"""
    try:
        korea_tz = pytz.timezone('Asia/Seoul')
        korea_now = datetime.now(korea_tz).date()

        if 'Ïò§Îäò' in request_text:
            return korea_now
        elif 'ÎÇ¥Ïùº' in request_text:
            return korea_now + timedelta(days=1)
        elif 'Î™®Î†à' in request_text or 'Î™®Îûò' in request_text:
            return korea_now + timedelta(days=2)
        elif 'Ïù¥Î≤à Ï£º' in request_text:
            days_until_friday = (4 - korea_now.weekday()) % 7
            days_until_friday = 7 if days_until_friday == 0 else days_until_friday
            return korea_now + timedelta(days=days_until_friday)
        elif 'Îã§Ïùå Ï£º' in request_text:
            return korea_now + timedelta(days=7)
        else:
            return korea_now + timedelta(days=1)
    except Exception as e:
        logger.error(f"ÎÇ†Ïßú ÌååÏã± Ïò§Î•ò: {e}")
        return datetime.now().date()

def parse_multiple_schedules_backend(request_text):
    """Î∞±ÏóîÎìúÏóêÏÑú Ïó¨Îü¨ ÏùºÏ†ï ÌååÏã±"""
    try:
        separators = [',', 'Ôºå', 'Í∑∏Î¶¨Í≥†', 'Î∞è', 'ÏôÄ', 'Í≥º']
        
        parts = [request_text]
        for sep in separators:
            new_parts = []
            for part in parts:
                new_parts.extend(part.split(sep))
            parts = new_parts
        
        cleaned_requests = []
        for part in parts:
            cleaned = part.strip()
            if cleaned and len(cleaned) > 2:
                cleaned_requests.append(cleaned)
        
        return cleaned_requests if len(cleaned_requests) > 1 else [request_text]
    except Exception as e:
        logger.error(f"ÏùºÏ†ï ÌååÏã± Ïò§Î•ò: {e}")
        return [request_text]


# ScheduleOptimizerBot ÌÅ¥ÎûòÏä§
class ScheduleOptimizerBot:
    """ÏùºÏ†ï ÏµúÏ†ÅÌôîÎ•º ÏúÑÌïú AI Î¥á ÌÅ¥ÎûòÏä§"""
    
    def __init__(self):
        logger.info("ScheduleOptimizerBot Ï¥àÍ∏∞Ìôî ÏãúÏûë")
        try:

            self.chatbots = {
                # OpenAI ÏµúÏã†/Í≤ΩÎüâ Ï±ÑÌåÖ Î™®Îç∏Î°ú ÍµêÏ≤¥
                'gpt': RealChatBot(OPENAI_API_KEY, 'gpt-4o-mini', 'openai'),
                # Anthropic Î™®Îç∏Î™Ö ÏµúÏã† alias Í∂åÏû•
                'claude': RealChatBot(ANTHROPIC_API_KEY, 'claude-3-5-haiku-latest', 'anthropic'),
                # GroqÎäî llama-3.1-8b-instantÎèÑ OK. mixtral Ïì∞Í≥† Ïã∂ÏúºÎ©¥ Î™®Îç∏Î™ÖÎßå Î≥ÄÍ≤Ω
                'mixtral': RealChatBot(GROQ_API_KEY, 'llama-3.1-8b-instant', 'groq'),
            }
            logger.info("ChatBot ÌÅ¥ÎûòÏä§ Î°úÎìú ÏÑ±Í≥µ")
        except ImportError as e:
            logger.warning(f"ChatBot ÌÅ¥ÎûòÏä§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {e}. ÎçîÎØ∏ ÌÅ¥ÎûòÏä§Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.")
            self.chatbots = {
                'gpt': DummyChatBot(),
                'claude': DummyChatBot(),
                'mixtral': DummyChatBot(),
            }
        except Exception as e:
            logger.error(f"ChatBot Ï¥àÍ∏∞Ìôî Ï§ë ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•ò: {e}")
            self.chatbots = {
                'gpt': DummyChatBot(),
                'claude': DummyChatBot(),
                'mixtral': DummyChatBot(),
            }
    
    def get_ai_suggestions(self, prompt, suggestion_type="schedule"):
        """Ïó¨Îü¨ AI Î™®Îç∏Î°úÎ∂ÄÌÑ∞ Ï†úÏïàÎ∞õÍ∏∞"""
        logger.info(f"AI Ï†úÏïà ÏöîÏ≤≠ ÏãúÏûë - ÌÉÄÏûÖ: {suggestion_type}")
        suggestions = {}
        
        for model_name, chatbot in self.chatbots.items():
            try:
                logger.info(f"{model_name} AI Î™®Îç∏ ÏöîÏ≤≠ ÏãúÏûë")
                if hasattr(chatbot, 'chat'):
                    response = chatbot.chat(prompt)
                    logger.info(f"{model_name} AI ÏùëÎãµ Í∏∏Ïù¥: {len(response) if response else 0}")
                else:
                    response = f"ÎçîÎØ∏ ÏùëÎãµ: {model_name}ÏóêÏÑú {suggestion_type} Î∂ÑÏÑù ÏôÑÎ£å"
                    logger.info(f"{model_name} ÎçîÎØ∏ ÏùëÎãµ ÏÇ¨Ïö©")
                suggestions[f"{model_name}_suggestion"] = response
            except Exception as e:
                logger.error(f"{model_name} AI ÏöîÏ≤≠ Ïã§Ìå®: {str(e)}")
                suggestions[f"{model_name}_suggestion"] = f"Ïò§Î•ò Î∞úÏÉù: {str(e)}"
        
        logger.info(f"AI Ï†úÏïà ÏôÑÎ£å: {len(suggestions)}Í∞ú ÏùëÎãµ")
        return suggestions
    
    def analyze_and_optimize_suggestions(self, suggestions, query, selected_models=['GPT', 'Claude', 'Mixtral']):
        """Ïó¨Îü¨ AI Ï†úÏïàÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÏµúÏ†ÅÌôîÎêú Í≤∞Í≥º ÏÉùÏÑ±"""
        logger.info("AI Ï†úÏïà Î∂ÑÏÑù Î∞è ÏµúÏ†ÅÌôî ÏãúÏûë")
        try:
            optimized = self._extract_first_valid_suggestion(suggestions)
            confidence = 0.85
            
            logger.info("AI Ï†úÏïà Î∂ÑÏÑù Î∞è ÏµúÏ†ÅÌôî ÏôÑÎ£å")
            return {
                "optimized_suggestion": optimized,
                "confidence_score": confidence,
                "ai_analysis": {
                    "analysis_summary": "AI Î™®Îç∏Îì§Ïùò Ï†úÏïàÏùÑ Ï¢ÖÌï© Î∂ÑÏÑùÌñàÏäµÎãàÎã§.",
                    "reasoning": "Ïó¨Îü¨ Î™®Îç∏Ïùò Í≥µÌÜµÏ†êÏùÑ Î∞îÌÉïÏúºÎ°ú ÏµúÏ†ÅÌôîÌñàÏäµÎãàÎã§.",
                    "models_used": selected_models
                },
                "individual_suggestions": self._parse_individual_suggestions(suggestions)
            }
            
        except Exception as e:
            logger.error(f"AI Î∂ÑÏÑù Ïò§Î•ò: {str(e)}")
            current_datetime = get_current_datetime()
            return {
                "optimized_suggestion": {
                    "title": "ÏÉà ÏùºÏ†ï",
                    "description": "AIÍ∞Ä Ï†úÏïàÌïú ÏùºÏ†ïÏûÖÎãàÎã§",
                    "suggested_date": current_datetime.strftime('%Y-%m-%d'),
                    "suggested_start_time": "09:00",
                    "suggested_end_time": "10:00",
                    "location": "",
                    "priority": "MEDIUM",
                    "attendees": [],
                    "reasoning": f"Î∂ÑÏÑù Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"
                },
                "confidence_score": 0.5,
                "ai_analysis": {
                    "analysis_summary": f"Î∂ÑÏÑù Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}",
                    "reasoning": "Í∏∞Î≥∏ Ï†úÏïàÏùÑ ÏÉùÏÑ±ÌñàÏäµÎãàÎã§.",
                    "models_used": []
                },
                "individual_suggestions": []
            }
    
    def _extract_first_valid_suggestion(self, suggestions):
        """Ï≤´ Î≤àÏß∏ Ïú†Ìö®Ìïú Ï†úÏïà Ï∂îÏ∂ú"""
        logger.info("Ïú†Ìö®Ìïú Ï†úÏïà Ï∂îÏ∂ú ÏãúÏûë")
        current_datetime = get_current_datetime()
        
        for key, suggestion in suggestions.items():
            try:
                logger.info(f"{key}ÏóêÏÑú JSON Ï∂îÏ∂ú ÏãúÎèÑ")
                json_match = re.search(r'\{.*\}', suggestion, re.DOTALL)
                if json_match:
                    parsed_json = json.loads(json_match.group())
                    logger.info(f"{key}ÏóêÏÑú JSON ÌååÏã± ÏÑ±Í≥µ")
                    
                    # ÌïÑÏàò ÌïÑÎìú ÌôïÏù∏ Î∞è Í∏∞Î≥∏Í∞í ÏÑ§Ï†ï
                    parsed_json.setdefault('suggested_date', current_datetime.strftime('%Y-%m-%d'))
                    parsed_json.setdefault('suggested_start_time', "09:00")
                    parsed_json.setdefault('suggested_end_time', "10:00")
                    parsed_json.setdefault('title', "ÏÉà ÏùºÏ†ï")
                    parsed_json.setdefault('description', "AIÍ∞Ä Ï†úÏïàÌïú ÏùºÏ†ïÏûÖÎãàÎã§")
                    parsed_json.setdefault('priority', "MEDIUM")
                    parsed_json.setdefault('attendees', [])
                    parsed_json.setdefault('location', "")
                    parsed_json.setdefault('reasoning', "AI Î∂ÑÏÑù Í≤∞Í≥ºÏûÖÎãàÎã§.")
                        
                    logger.info("Ïú†Ìö®Ìïú Ï†úÏïà Ï∂îÏ∂ú ÏÑ±Í≥µ")
                    return parsed_json
            except (json.JSONDecodeError, AttributeError, KeyError) as e:
                logger.warning(f"JSON ÌååÏã± Ïã§Ìå® ({key}): {str(e)}")
                continue
            except Exception as e:
                logger.error(f"ÏòàÏÉÅÏπò Î™ªÌïú ÌååÏã± Ïò§Î•ò ({key}): {str(e)}")
                continue
        
        # Î™®Îì† ÌååÏã±Ïù¥ Ïã§Ìå®Ìïú Í≤ΩÏö∞ Í∏∞Î≥∏Í∞í Î∞òÌôò
        logger.warning("Î™®Îì† AI ÏùëÎãµ ÌååÏã± Ïã§Ìå®, Í∏∞Î≥∏Í∞í Î∞òÌôò")
        return {
            "title": "ÏÉà ÏùºÏ†ï",
            "description": "AIÍ∞Ä Ï†úÏïàÌïú ÏùºÏ†ïÏûÖÎãàÎã§",
            "suggested_date": current_datetime.strftime('%Y-%m-%d'),
            "suggested_start_time": "09:00",
            "suggested_end_time": "10:00",
            "location": "",
            "priority": "MEDIUM",
            "attendees": [],
            "reasoning": "Ïó¨Îü¨ AI Î™®Îç∏Ïùò Ï†úÏïàÏùÑ Ï¢ÖÌï©Ìïú Í≤∞Í≥ºÏûÖÎãàÎã§."
        }
    
    def _parse_individual_suggestions(self, suggestions):
        """Í∞úÎ≥Ñ Ï†úÏïàÎì§ÏùÑ ÌååÏã±"""
        logger.info("Í∞úÎ≥Ñ Ï†úÏïà ÌååÏã± ÏãúÏûë")
        parsed = []
        current_datetime = get_current_datetime()
        
        for key, suggestion in suggestions.items():
            try:
                json_match = re.search(r'\{.*\}', suggestion, re.DOTALL)
                if json_match:
                    parsed_suggestion = json.loads(json_match.group())
                    parsed_suggestion['source'] = key.replace('_suggestion', '')
                    
                    # ÌïÑÏàò ÌïÑÎìú ÌôïÏù∏ Î∞è Í∏∞Î≥∏Í∞í ÏÑ§Ï†ï
                    parsed_suggestion.setdefault('suggested_date', current_datetime.strftime('%Y-%m-%d'))
                    parsed_suggestion.setdefault('suggested_start_time', "09:00")
                    parsed_suggestion.setdefault('suggested_end_time', "10:00")
                    parsed_suggestion.setdefault('title', "ÏÉà ÏùºÏ†ï")
                    parsed_suggestion.setdefault('description', f"{key.replace('_suggestion', '').upper()}Í∞Ä Ï†úÏïàÌïú ÏùºÏ†ïÏûÖÎãàÎã§")
                    parsed_suggestion.setdefault('priority', "MEDIUM")
                    parsed_suggestion.setdefault('attendees', [])
                    parsed_suggestion.setdefault('location', "")
                    parsed_suggestion.setdefault('reasoning', f"{key.replace('_suggestion', '').upper()} Î™®Îç∏Ïùò Î∂ÑÏÑù Í≤∞Í≥ºÏûÖÎãàÎã§.")
                        
                    parsed.append(parsed_suggestion)
                    logger.info(f"{key} Í∞úÎ≥Ñ Ï†úÏïà ÌååÏã± ÏÑ±Í≥µ")
            except (json.JSONDecodeError, AttributeError, KeyError) as e:
                logger.warning(f"Í∞úÎ≥Ñ Ï†úÏïà ÌååÏã± Ïã§Ìå® ({key}): {str(e)}")
                continue
            except Exception as e:
                logger.error(f"Í∞úÎ≥Ñ Ï†úÏïà ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•ò ({key}): {str(e)}")
                continue
        
        # ÌååÏã±Îêú Ï†úÏïàÏù¥ ÏóÜÏúºÎ©¥ Í∏∞Î≥∏ Ï†úÏïà ÏÉùÏÑ±
        if not parsed:
            logger.warning("Î™®Îì† Í∞úÎ≥Ñ Ï†úÏïà ÌååÏã± Ïã§Ìå®, Í∏∞Î≥∏ Ï†úÏïà ÏÉùÏÑ±")
            for model in ['gpt', 'claude', 'mixtral']:
                parsed.append({
                    "title": "ÏÉà ÏùºÏ†ï",
                    "description": f"{model.upper()}Í∞Ä Ï†úÏïàÌïú ÏùºÏ†ïÏûÖÎãàÎã§",
                    "suggested_date": current_datetime.strftime('%Y-%m-%d'),
                    "suggested_start_time": "09:00",
                    "suggested_end_time": "10:00",
                    "location": "",
                    "priority": "MEDIUM",
                    "attendees": [],
                    "reasoning": f"{model.upper()} Î™®Îç∏Ïùò Î∂ÑÏÑù Í≤∞Í≥ºÏûÖÎãàÎã§.",
                    "source": model
                })
        
        logger.info(f"Í∞úÎ≥Ñ Ï†úÏïà ÌååÏã± ÏôÑÎ£å: {len(parsed)}Í∞ú")
        return parsed
        
import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from datetime import datetime
from collections import Counter

from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient


# views.py - Í∂åÌïú ÏÑ§Ï†ï Ï∂îÍ∞Ä
# views.py - Î™®Îì† APIViewÏóê Í∂åÌïú ÏÑ§Ï†ï Ï∂îÍ∞Ä




# views.py - ÏßÑÌñâÎ•† Ï∂îÏ†Å Í∞úÏÑ† Î≤ÑÏ†Ñ
import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.permissions import AllowAny
from datetime import datetime, timedelta
from collections import Counter
import threading
import queue

from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient


# Ï†ÑÏó≠ ÏßÑÌñâÎ•† Ï∂îÏ†Å
analysis_progress_tracker = {}


#  
# views.py - Í≥†Í∏â Î∂ÑÏÑù Í∏∞Îä• Ï∂îÍ∞Ä

import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.permissions import AllowAny
from datetime import datetime, timedelta
from collections import Counter
import threading
import queue

from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient

# Ï†ÑÏó≠ ÏßÑÌñâÎ•† Ï∂îÏ†Å (Í∏∞Ï°¥Í≥º ÎèôÏùº)
analysis_progress_tracker = {}
import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.permissions import AllowAny
from datetime import datetime, timedelta
from collections import Counter
import threading
import queue

from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient

# Ï†ÑÏó≠ ÏßÑÌñâÎ•† Ï∂îÏ†Å (Í∏∞Ï°¥Í≥º ÎèôÏùº)
analysis_progress_tracker = {}

class AnalysisProgressTracker:
    """Î∂ÑÏÑù ÏßÑÌñâÎ•† Ï∂îÏ†Å ÌÅ¥ÎûòÏä§ - Í≥†Í∏â Î∂ÑÏÑù Îã®Í≥Ñ Ï∂îÍ∞Ä"""
    
    def __init__(self):
        self.progress_data = {}
    
    def start_tracking(self, video_id, total_frames=0, analysis_type='enhanced'):
        """Î∂ÑÏÑù Ï∂îÏ†Å ÏãúÏûë"""
        self.progress_data[video_id] = {
            'progress': 0,
            'currentStep': 'Î∂ÑÏÑù Ï§ÄÎπÑÏ§ë',
            'startTime': datetime.now().isoformat(),
            'processedFrames': 0,
            'totalFrames': total_frames,
            'estimatedTime': None,
            'analysisType': analysis_type,
            'steps': [],
            'currentFeature': '',
            'completedFeatures': [],
            'totalFeatures': self._get_total_features(analysis_type)
        }
    
    def update_progress(self, video_id, progress=None, step=None, processed_frames=None, current_feature=None):
        """ÏßÑÌñâÎ•† ÏóÖÎç∞Ïù¥Ìä∏ - Í≥†Í∏â Î∂ÑÏÑù Ï†ïÎ≥¥ Ìè¨Ìï®"""
        if video_id not in self.progress_data:
            return
        
        data = self.progress_data[video_id]
        
        if progress is not None:
            data['progress'] = min(100, max(0, progress))
        
        if step is not None:
            data['currentStep'] = step
            data['steps'].append({
                'step': step,
                'timestamp': datetime.now().isoformat()
            })
        
        if current_feature is not None:
            data['currentFeature'] = current_feature
            if current_feature not in data['completedFeatures']:
                data['completedFeatures'].append(current_feature)
        
        if processed_frames is not None:
            data['processedFrames'] = processed_frames
            
            # ÏßÑÌñâÎ•† ÏûêÎèô Í≥ÑÏÇ∞ (ÌîÑÎ†àÏûÑ Í∏∞Î∞ò + Í∏∞Îä• Í∏∞Î∞ò)
            if data['totalFrames'] > 0:
                frame_progress = (processed_frames / data['totalFrames']) * 80  # ÌîÑÎ†àÏûÑ Î∂ÑÏÑù 80%
                feature_progress = (len(data['completedFeatures']) / data['totalFeatures']) * 20  # ÌõÑÏ≤òÎ¶¨ 20%
                calculated_progress = frame_progress + feature_progress
                data['progress'] = min(100, calculated_progress)
        
        # ÏòàÏÉÅ ÏôÑÎ£å ÏãúÍ∞Ñ Í≥ÑÏÇ∞ (Í≥†Í∏â Î∂ÑÏÑù Í≥†Î†§)
        if data['progress'] > 5:
            elapsed = (datetime.now() - datetime.fromisoformat(data['startTime'])).total_seconds()
            
            # Î∂ÑÏÑù ÌÉÄÏûÖÎ≥Ñ ÏãúÍ∞Ñ Í∞ÄÏ§ëÏπò
            time_weights = {
                'basic': 1.0,
                'enhanced': 2.0,
                'comprehensive': 4.0,
                'custom': 2.5
            }
            
            weight = time_weights.get(data['analysisType'], 2.0)
            estimated_total = (elapsed / data['progress']) * 100 * weight
            remaining = estimated_total - elapsed
            data['estimatedTime'] = max(0, remaining)
    
    def _get_total_features(self, analysis_type):
        """Î∂ÑÏÑù ÌÉÄÏûÖÎ≥Ñ Ï¥ù Í∏∞Îä• Ïàò"""
        feature_counts = {
            'basic': 2,  # Í∞ùÏ≤¥Í∞êÏßÄ, Í∏∞Î≥∏Ï∫°ÏÖò
            'enhanced': 4,  # Í∞ùÏ≤¥Í∞êÏßÄ, CLIP, OCR, Í≥†Í∏âÏ∫°ÏÖò
            'comprehensive': 6,  # Î™®Îì† Í∏∞Îä•
            'custom': 4  # ÌèâÍ∑†Í∞í
        }
        return feature_counts.get(analysis_type, 4)
    
    def get_progress(self, video_id):
        """ÏßÑÌñâÎ•† Ï°∞Ìöå"""
        return self.progress_data.get(video_id, {})
    
    def finish_tracking(self, video_id, success=True):
        """Î∂ÑÏÑù ÏôÑÎ£å"""
        if video_id in self.progress_data:
            self.progress_data[video_id]['progress'] = 100
            self.progress_data[video_id]['currentStep'] = 'Î∂ÑÏÑù ÏôÑÎ£å' if success else 'Î∂ÑÏÑù Ïã§Ìå®'
            self.progress_data[video_id]['success'] = success
            # ÏôÑÎ£å ÌõÑ 10Î∂Ñ Îí§ Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú
            threading.Timer(600, lambda: self.progress_data.pop(video_id, None)).start()

# Ï†ÑÏó≠ Ìä∏ÎûòÏª§ Ïù∏Ïä§ÌÑ¥Ïä§
progress_tracker = AnalysisProgressTracker()

# views.py - EnhancedAnalyzeVideoView ÌÅ¥ÎûòÏä§ ÏôÑÏ†Ñ ÏàòÏ†ï
import threading
import time
import json
import cv2
from datetime import datetime
from django.conf import settings
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
# views.py ÏÉÅÎã® import Î∂ÄÎ∂Ñ - ÏàòÏ†ïÎê®

import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.permissions import AllowAny
from datetime import datetime, timedelta
from collections import Counter
import threading
import queue

# Î™®Îç∏ imports
from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient

# ‚úÖ Ï§ëÏöî: get_video_analyzer Ìï®Ïàò import Ï∂îÍ∞Ä
from .video_analyzer import get_video_analyzer, VideoAnalyzer

# ‚úÖ Ï∂îÍ∞Ä: Í∏∞ÌÉÄ ÌïÑÏöîÌïú Ìï®ÏàòÎì§ÎèÑ import
try:
    from .video_analyzer import (
        EnhancedVideoAnalyzer, 
        ColorAnalyzer, 
        SceneClassifier, 
        AdvancedSceneAnalyzer,
        log_once  # Î°úÍ∑∏ Ï§ëÎ≥µ Î∞©ÏßÄ Ìï®Ïàò
    )
    print("‚úÖ video_analyzer Î™®ÎìàÏóêÏÑú Î™®Îì† ÌÅ¥ÎûòÏä§ import ÏÑ±Í≥µ")
except ImportError as e:
    print(f"‚ö†Ô∏è video_analyzer import Î∂ÄÎ∂Ñ Ïã§Ìå®: {e}")
    # Fallback - Í∏∞Î≥∏ ÌÅ¥ÎûòÏä§Îßå import
    try:
        from .video_analyzer import get_video_analyzer, VideoAnalyzer, log_once
        print("‚úÖ video_analyzer Î™®ÎìàÏóêÏÑú Î™®Îì† ÌÅ¥ÎûòÏä§ import ÏÑ±Í≥µ")
    except ImportError as e:
        print(f"‚ö†Ô∏è video_analyzer import Î∂ÄÎ∂Ñ Ïã§Ìå®: {e}")
        get_video_analyzer = None
        VideoAnalyzer = None
        log_once = None

# views.py - Ïã§Ï†ú AI Î∂ÑÏÑùÏùÑ ÏÇ¨Ïö©ÌïòÎäî EnhancedAnalyzeVideoView

import os
import json
import time
import threading
from datetime import datetime
from django.http import JsonResponse
from django.views import View
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from django.conf import settings
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny

# ÎπÑÎîîÏò§ Î∂ÑÏÑùÍ∏∞ import
try:
    from .video_analyzer import get_video_analyzer, get_analyzer_status
    from .db_builder import get_video_rag_system
    VIDEO_ANALYZER_AVAILABLE = True
    print("‚úÖ video_analyzer Î™®Îìà import ÏÑ±Í≥µ")
except ImportError as e:
    VIDEO_ANALYZER_AVAILABLE = False
    print(f"‚ùå video_analyzer import Ïã§Ìå®: {e}")

# Django Î™®Îç∏ import
from .models import Video, VideoAnalysis, Scene, Frame

@method_decorator(csrf_exempt, name='dispatch')
class EnhancedAnalyzeVideoView(APIView):
    """Ïã§Ï†ú AI Î∂ÑÏÑùÏùÑ ÏÇ¨Ïö©ÌïòÎäî Í≥†Í∏â ÎπÑÎîîÏò§ Î∂ÑÏÑù ÏãúÏûë"""
    permission_classes = [AllowAny]
    
    def post(self, request, video_id):
        try:
            print(f"üöÄ Ïã§Ï†ú AI ÎπÑÎîîÏò§ Î∂ÑÏÑù ÏãúÏûë: video_id={video_id}")
            
            analysis_type = request.data.get('analysisType', 'enhanced')
            analysis_config = request.data.get('analysisConfig', {})
            enhanced_analysis = request.data.get('enhancedAnalysis', True)
            
            print(f"üìã Î∂ÑÏÑù ÏöîÏ≤≠ Ï†ïÎ≥¥:")
            print(f"  - ÎπÑÎîîÏò§ ID: {video_id}")
            print(f"  - Î∂ÑÏÑù ÌÉÄÏûÖ: {analysis_type}")
            print(f"  - Í≥†Í∏â Î∂ÑÏÑù: {enhanced_analysis}")
            print(f"  - Î∂ÑÏÑù ÏÑ§Ï†ï: {analysis_config}")
            
            # ÎπÑÎîîÏò§ Ï°¥Ïû¨ ÌôïÏù∏
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({
                    'error': 'Ìï¥Îãπ ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
                }, status=status.HTTP_404_NOT_FOUND)
            
            # Ïù¥ÎØ∏ Î∂ÑÏÑù Ï§ëÏù∏ÏßÄ ÌôïÏù∏
            if video.analysis_status == 'processing':
                return Response({
                    'error': 'Ïù¥ÎØ∏ Î∂ÑÏÑùÏù¥ ÏßÑÌñâ Ï§ëÏûÖÎãàÎã§.',
                    'current_status': video.analysis_status
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # AI Î∂ÑÏÑùÍ∏∞ ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä ÌôïÏù∏
            if not VIDEO_ANALYZER_AVAILABLE:
                return Response({
                    'error': 'AI Î∂ÑÏÑù Î™®ÎìàÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§. ÏÑúÎ≤Ñ ÏÑ§Ï†ïÏùÑ ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.',
                    'fallback': 'basic_analysis'
                }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
            
            # Î∂ÑÏÑùÍ∏∞ ÏÉÅÌÉú ÌôïÏù∏
            analyzer_status = get_analyzer_status()
            print(f"üîç Î∂ÑÏÑùÍ∏∞ ÏÉÅÌÉú: {analyzer_status}")
            
            # Î∂ÑÏÑù ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
            video.analysis_status = 'processing'
            video.save()
            
            print(f"‚úÖ ÎπÑÎîîÏò§ ÏÉÅÌÉúÎ•º 'processing'ÏúºÎ°ú Î≥ÄÍ≤Ω: {video.original_name}")
            
            # Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú Ïã§Ï†ú AI Î∂ÑÏÑù ÏãúÏûë
            analysis_thread = threading.Thread(
                target=self._run_real_ai_analysis,
                args=(video, analysis_type, analysis_config, enhanced_analysis),
                daemon=True
            )
            analysis_thread.start()
            
            print("üßµ Ïã§Ï†ú AI Î∂ÑÏÑù Ïä§Î†àÎìú ÏãúÏûëÎê®")
            
            return Response({
                'success': True,
                'message': f'{self._get_analysis_type_name(analysis_type)} AI Î∂ÑÏÑùÏù¥ ÏãúÏûëÎêòÏóàÏäµÎãàÎã§.',
                'video_id': video.id,
                'analysis_type': analysis_type,
                'enhanced_analysis': enhanced_analysis,
                'estimated_time': self._get_estimated_time_real(analysis_type),
                'status': 'processing',
                'ai_features': analyzer_status.get('features', {}),
                'analysis_method': 'real_ai_analysis'
            })
            
        except Exception as e:
            print(f"‚ùå AI Î∂ÑÏÑù ÏãúÏûë Ïò§Î•ò: {e}")
            import traceback
            print(f"üîç ÏÉÅÏÑ∏ Ïò§Î•ò: {traceback.format_exc()}")
            
            return Response({
                'error': f'AI Î∂ÑÏÑù ÏãúÏûë Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _run_real_ai_analysis(self, video, analysis_type, analysis_config, enhanced_analysis):
        """Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú Ïã§ÌñâÎêòÎäî Ïã§Ï†ú AI Î∂ÑÏÑù Ìï®Ïàò"""
        start_time = time.time()
        
        try:
            print(f"üöÄ ÎπÑÎîîÏò§ {video.id} Ïã§Ï†ú AI Î∂ÑÏÑù ÏãúÏûë - ÌÉÄÏûÖ: {analysis_type}")
            
            # 1. VideoAnalyzer Ïù∏Ïä§ÌÑ¥Ïä§ Í∞ÄÏ†∏Ïò§Í∏∞
            analyzer = get_video_analyzer()
            if not analyzer:
                raise Exception("VideoAnalyzer Ïù∏Ïä§ÌÑ¥Ïä§Î•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§")
            
            print(f"‚úÖ VideoAnalyzer Î°úÎìú ÏôÑÎ£å: {type(analyzer).__name__}")
            
            # 2. Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû• ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
            analysis_results_dir = os.path.join(settings.MEDIA_ROOT, 'analysis_results')
            os.makedirs(analysis_results_dir, exist_ok=True)
            
            # 3. JSON ÌååÏùºÎ™Ö ÏÉùÏÑ±
            timestamp = int(time.time())
            json_filename = f"real_analysis_{video.id}_{analysis_type}_{timestamp}.json"
            json_filepath = os.path.join(analysis_results_dir, json_filename)
            
            print(f"üìÅ Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû• Í≤ΩÎ°ú: {json_filepath}")
            
            # 4. ÏßÑÌñâÎ•† ÏΩúÎ∞± Ìï®Ïàò Ï†ïÏùò
            def progress_callback(progress, message):
                print(f"üìä Î∂ÑÏÑù ÏßÑÌñâÎ•†: {progress:.1f}% - {message}")
                # ÌïÑÏöîÏãú ÏõπÏÜåÏºìÏù¥ÎÇò Îã§Î•∏ Î∞©Î≤ïÏúºÎ°ú Ïã§ÏãúÍ∞Ñ ÏóÖÎç∞Ïù¥Ìä∏ Í∞ÄÎä•
            
            # 5. Ïã§Ï†ú AI Î∂ÑÏÑù ÏàòÌñâ
            print("üß† Ïã§Ï†ú AI Î∂ÑÏÑù ÏãúÏûë...")
            analysis_results = analyzer.analyze_video_comprehensive(
                video=video,
                analysis_type=analysis_type,
                progress_callback=progress_callback
            )
            
            if not analysis_results.get('success', False):
                raise Exception(f"AI Î∂ÑÏÑù Ïã§Ìå®: {analysis_results.get('error', 'Unknown error')}")
            
            print(f"‚úÖ AI Î∂ÑÏÑù ÏôÑÎ£å: {analysis_results.get('total_frames_analyzed', 0)}Í∞ú ÌîÑÎ†àÏûÑ Ï≤òÎ¶¨")
            
            # 6. Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä
            analysis_results['metadata'] = {
                'video_id': video.id,
                'video_name': video.original_name,
                'analysis_type': analysis_type,
                'analysis_config': analysis_config,
                'enhanced_analysis': enhanced_analysis,
                'json_file_path': json_filepath,
                'analysis_timestamp': datetime.now().isoformat(),
                'total_frames': getattr(video, 'total_frames', 0),
                'video_duration': getattr(video, 'duration', 0),
                'fps': getattr(video, 'fps', 30),
                'processing_time_seconds': time.time() - start_time,
                'analysis_method': 'real_ai_enhanced',
                'ai_features_used': analysis_results.get('analysis_config', {}).get('features_enabled', {})
            }
            
            # 7. JSON ÌååÏùº Ï†ÄÏû•
            try:
                with open(json_filepath, 'w', encoding='utf-8') as f:
                    json.dump(analysis_results, f, ensure_ascii=False, indent=2, default=str)
                print(f"‚úÖ Î∂ÑÏÑù Í≤∞Í≥º JSON Ï†ÄÏû• ÏôÑÎ£å: {json_filepath}")
            except Exception as json_error:
                print(f"‚ö†Ô∏è JSON Ï†ÄÏû• Ïã§Ìå®: {json_error}")
                # JSON Ï†ÄÏû• Ïã§Ìå®Ìï¥ÎèÑ DBÎäî Ï†ÄÏû•ÌïòÎèÑÎ°ù Í≥ÑÏÜç ÏßÑÌñâ
            
            # 8. Django Î™®Îç∏Ïóê Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû•
            self._save_analysis_to_db(video, analysis_results, enhanced_analysis, json_filepath)
            
            # 9. RAG ÏãúÏä§ÌÖúÏóê Î∂ÑÏÑù Í≤∞Í≥º Îì±Î°ù
            self._register_to_rag_system(video.id, json_filepath)
            
            # 10. ÏôÑÎ£å ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
            video.analysis_status = 'completed'
            video.is_analyzed = True
            video.save()
            
            processing_time = time.time() - start_time
            print(f"üéâ ÎπÑÎîîÏò§ {video.id} Ïã§Ï†ú AI Î∂ÑÏÑù ÏôÑÎ£å!")
            print(f"üìä Ï≤òÎ¶¨ ÏãúÍ∞Ñ: {processing_time:.1f}Ï¥à")
            print(f"üìä ÏµúÏ¢Ö ÌÜµÍ≥Ñ: {analysis_results.get('total_frames_analyzed', 0)}Í∞ú ÌîÑÎ†àÏûÑ Î∂ÑÏÑù")
            
        except Exception as e:
            print(f"‚ùå ÎπÑÎîîÏò§ {video.id} AI Î∂ÑÏÑù Ïã§Ìå®: {e}")
            import traceback
            print(f"üîç ÏÉÅÏÑ∏ Ïò§Î•ò:\n{traceback.format_exc()}")
            
            # Ïò§Î•ò ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
            try:
                video.analysis_status = 'failed'
                video.save()
            except Exception as save_error:
                print(f"‚ö†Ô∏è Ïò§Î•ò ÏÉÅÌÉú Ï†ÄÏû• Ïã§Ìå®: {save_error}")
    
    def _save_analysis_to_db(self, video, analysis_results, enhanced_analysis, json_filepath):
        """Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Django DBÏóê Ï†ÄÏû•"""
        try:
            print("üíæ Î∂ÑÏÑù Í≤∞Í≥ºÎ•º DBÏóê Ï†ÄÏû• Ï§ë...")
            
            video_summary = analysis_results.get('video_summary', {})
            frame_results = analysis_results.get('frame_results', [])
            analysis_config = analysis_results.get('analysis_config', {})
            metadata = analysis_results.get('metadata', {})
            
            # VideoAnalysis Í∞ùÏ≤¥ ÏÉùÏÑ±
            analysis = VideoAnalysis.objects.create(
                video=video,
                enhanced_analysis=enhanced_analysis,
                success_rate=95.0,  # Ïã§Ï†ú ÏÑ±Í≥µÎ•† Í≥ÑÏÇ∞ Î°úÏßÅ Ï∂îÍ∞Ä Í∞ÄÎä•
                processing_time_seconds=metadata.get('processing_time_seconds', 0),
                analysis_statistics={
                    'total_frames_analyzed': analysis_results.get('total_frames_analyzed', 0),
                    'unique_objects': len(video_summary.get('dominant_objects', [])),
                    'analysis_method': 'real_ai_enhanced',
                    'ai_features_used': analysis_config.get('features_enabled', {}),
                    'scene_types': video_summary.get('scene_types', []),
                    'text_extracted': bool(video_summary.get('text_content')),
                    'json_file_path': json_filepath,
                    'dominant_objects': video_summary.get('dominant_objects', []),
                    'analysis_quality_metrics': video_summary.get('analysis_quality_metrics', {}),
                    'processing_statistics': video_summary.get('processing_statistics', {})
                },
                caption_statistics={
                    'frames_with_caption': len([f for f in frame_results if f.get('final_caption')]),
                    'enhanced_captions': len([f for f in frame_results if f.get('enhanced_caption')]),
                    'text_content_length': len(video_summary.get('text_content', '')),
                    'average_confidence': video_summary.get('analysis_quality_metrics', {}).get('average_detection_confidence', 0.8)
                }
            )
            
            # Scene Í∞ùÏ≤¥Îì§ ÏÉùÏÑ± (ÌïòÏù¥ÎùºÏù¥Ìä∏ ÌîÑÎ†àÏûÑ Í∏∞Î∞ò)
            highlight_frames = video_summary.get('highlight_frames', [])
            scene_duration = video.duration / max(len(highlight_frames), 1) if video.duration > 0 else 1
            
            for i, highlight in enumerate(highlight_frames[:10]):  # ÏµúÎåÄ 10Í∞ú Ïî¨
                Scene.objects.create(
                    video=video,
                    scene_id=i + 1,
                    start_time=max(0, highlight.get('timestamp', 0) - scene_duration/2),
                    end_time=min(video.duration, highlight.get('timestamp', 0) + scene_duration/2),
                    duration=scene_duration,
                    frame_count=60,  # 2Ï¥à Î∂ÑÎüâ Í∞ÄÏ†ï
                    dominant_objects=video_summary.get('dominant_objects', [])[:5],
                    enhanced_captions_count=1 if highlight.get('object_count', 0) > 0 else 0
                )
            
            # Frame Í∞ùÏ≤¥Îì§ ÏÉùÏÑ± (Ï£ºÏöî ÌîÑÎ†àÏûÑÎì§Îßå)
            important_frames = [f for f in frame_results if f.get('final_caption') or len(f.get('objects', [])) > 0]
            for frame_data in important_frames[:50]:  # ÏµúÎåÄ 50Í∞ú ÌîÑÎ†àÏûÑ
                try:
                    Frame.objects.create(
                        video=video,
                        image_id=frame_data.get('image_id', 0),
                        timestamp=frame_data.get('timestamp', 0),
                        caption=frame_data.get('caption', ''),
                        enhanced_caption=frame_data.get('enhanced_caption', ''),
                        final_caption=frame_data.get('final_caption', ''),
                        detected_objects=frame_data.get('objects', []),
                        comprehensive_features=frame_data.get('comprehensive_features', {})
                    )
                except Exception as frame_error:
                    print(f"‚ö†Ô∏è ÌîÑÎ†àÏûÑ {frame_data.get('image_id', 'unknown')} Ï†ÄÏû• Ïã§Ìå®: {frame_error}")
                    continue
            
            print(f"‚úÖ DB Ï†ÄÏû• ÏôÑÎ£å: {len(important_frames)}Í∞ú ÌîÑÎ†àÏûÑ, {len(highlight_frames)}Í∞ú Ïî¨")
            
        except Exception as e:
            print(f"‚ùå DB Ï†ÄÏû• Ïã§Ìå®: {e}")
            import traceback
            print(f"üîç DB Ï†ÄÏû• Ïò§Î•ò ÏÉÅÏÑ∏:\n{traceback.format_exc()}")
    
    def _register_to_rag_system(self, video_id, json_filepath):
        """RAG ÏãúÏä§ÌÖúÏóê Î∂ÑÏÑù Í≤∞Í≥º Îì±Î°ù"""
        try:
            print(f"üîç RAG ÏãúÏä§ÌÖúÏóê ÎπÑÎîîÏò§ {video_id} Îì±Î°ù Ï§ë...")
            
            rag_system = get_video_rag_system()
            if not rag_system:
                print("‚ö†Ô∏è RAG ÏãúÏä§ÌÖúÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§")
                return
            
            success = rag_system.process_video_analysis_json(json_filepath, str(video_id))
            
            if success:
                print(f"‚úÖ RAG ÏãúÏä§ÌÖú Îì±Î°ù ÏôÑÎ£å: ÎπÑÎîîÏò§ {video_id}")
            else:
                print(f"‚ö†Ô∏è RAG ÏãúÏä§ÌÖú Îì±Î°ù Ïã§Ìå®: ÎπÑÎîîÏò§ {video_id}")
                
        except Exception as e:
            print(f"‚ùå RAG ÏãúÏä§ÌÖú Îì±Î°ù Ïò§Î•ò: {e}")
    
    def _get_analysis_type_name(self, analysis_type):
        """Î∂ÑÏÑù ÌÉÄÏûÖ Ïù¥Î¶Ñ Î∞òÌôò"""
        type_names = {
            'basic': 'Í∏∞Î≥∏ AI Î∂ÑÏÑù',
            'enhanced': 'Ìñ•ÏÉÅÎêú AI Î∂ÑÏÑù',
            'comprehensive': 'Ï¢ÖÌï© AI Î∂ÑÏÑù',
            'custom': 'ÏÇ¨Ïö©Ïûê Ï†ïÏùò AI Î∂ÑÏÑù'
        }
        return type_names.get(analysis_type, 'Ìñ•ÏÉÅÎêú AI Î∂ÑÏÑù')
    
    def _get_estimated_time_real(self, analysis_type):
        """Ïã§Ï†ú AI Î∂ÑÏÑù ÌÉÄÏûÖÎ≥Ñ ÏòàÏÉÅ ÏãúÍ∞Ñ"""
        time_estimates = {
            'basic': '5-15Î∂Ñ',
            'enhanced': '10-30Î∂Ñ', 
            'comprehensive': '20-60Î∂Ñ',
            'custom': 'ÏÉÅÌô©Ïóê Îî∞Îùº Îã§Î¶Ñ'
        }
        return time_estimates.get(analysis_type, '10-30Î∂Ñ')
    
    def get(self, request, video_id):
        """Î∂ÑÏÑù ÏÉÅÌÉú Ï°∞Ìöå"""
        try:
            video = Video.objects.get(id=video_id)
            
            analyzer_status = get_analyzer_status() if VIDEO_ANALYZER_AVAILABLE else {'status': 'unavailable'}
            
            return Response({
                'video_id': video.id,
                'video_name': video.original_name,
                'analysis_status': video.analysis_status,
                'is_analyzed': video.is_analyzed,
                'analyzer_available': VIDEO_ANALYZER_AVAILABLE,
                'analyzer_status': analyzer_status,
                'last_updated': video.updated_at.isoformat() if hasattr(video, 'updated_at') else None
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'Ìï¥Îãπ ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'ÏÉÅÌÉú Ï°∞Ìöå Ï§ë Ïò§Î•ò: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        
# üÜï ÏÉàÎ°úÏö¥ API ÏóîÎìúÌè¨Ïù∏Ìä∏ Ï∂îÍ∞Ä
class VideoQAAnalyticsView(APIView):
    """ÎπÑÎîîÏò§ QA Î∂ÑÏÑù Î∞è ÌÜµÍ≥Ñ Î∑∞"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        self.rag_system = get_video_rag_system()
        self.enhanced_qa = EnhancedVideoQASystem(self.rag_system, LLMClient())
    
    def get(self, request, video_id=None):
        """QA ÌÜµÍ≥Ñ Ï°∞Ìöå"""
        try:
            if video_id:
                # ÌäπÏ†ï ÎπÑÎîîÏò§Ïùò ÎåÄÌôî Ïª®ÌÖçÏä§Ìä∏ Ï°∞Ìöå
                context = self.enhanced_qa.get_conversation_context(str(video_id))
                
                # ÏßàÎ¨∏ Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ ÌÜµÍ≥Ñ
                category_stats = {}
                for conv in context:
                    category = conv.get('category', 'unknown')
                    category_stats[category] = category_stats.get(category, 0) + 1
                
                return Response({
                    'video_id': video_id,
                    'total_conversations': len(context),
                    'category_statistics': category_stats,
                    'recent_conversations': context[-5:],  # ÏµúÍ∑º 5Í∞ú
                    'success': True
                })
            else:
                # Ï†ÑÏ≤¥ ÏãúÏä§ÌÖú ÌÜµÍ≥Ñ
                total_videos = len(self.enhanced_qa.context_memory)
                total_conversations = sum(len(convs) for convs in self.enhanced_qa.context_memory.values())
                
                return Response({
                    'total_videos_with_conversations': total_videos,
                    'total_conversations': total_conversations,
                    'videos': list(self.enhanced_qa.context_memory.keys()),
                    'success': True
                })
                
        except Exception as e:
            return Response({
                'error': f'ÌÜµÍ≥Ñ Ï°∞Ìöå Ïã§Ìå®: {str(e)}',
                'success': False
            }, status=500)
    
    def delete(self, request, video_id=None):
        """ÎåÄÌôî Ïª®ÌÖçÏä§Ìä∏ ÏÇ≠Ï†ú"""
        try:
            if video_id:
                self.enhanced_qa.clear_context(str(video_id))
                return Response({
                    'message': f'ÎπÑÎîîÏò§ {video_id}Ïùò ÎåÄÌôî Ïª®ÌÖçÏä§Ìä∏Í∞Ä ÏÇ≠Ï†úÎêòÏóàÏäµÎãàÎã§.',
                    'success': True
                })
            else:
                self.enhanced_qa.clear_context()
                return Response({
                    'message': 'Î™®Îì† ÎåÄÌôî Ïª®ÌÖçÏä§Ìä∏Í∞Ä ÏÇ≠Ï†úÎêòÏóàÏäµÎãàÎã§.',
                    'success': True
                })
                
        except Exception as e:
            return Response({
                'error': f'Ïª®ÌÖçÏä§Ìä∏ ÏÇ≠Ï†ú Ïã§Ìå®: {str(e)}',
                'success': False
            }, status=500)


class VideoQAUtils:
    """ÎπÑÎîîÏò§ QA Í¥ÄÎ†® Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§"""
    
    @staticmethod
    def categorize_questions_batch(questions: List[str]) -> Dict[str, List[str]]:
        """ÏßàÎ¨∏Îì§ÏùÑ Î∞∞ÏπòÎ°ú Ïπ¥ÌÖåÍ≥†Î¶¨Ìôî"""
        categories = {
            'object_detection': [],
            'people_analysis': [],
            'scene_analysis': [],
            'action_analysis': [],
            'summary': [],
            'specific_search': [],
            'general': []
        }
        
        for question in questions:
            category = VideoQAUtils.classify_single_question(question)
            categories[category].append(question)
        
        return categories
    
    @staticmethod
    def classify_single_question(question: str) -> str:
        """Îã®Ïùº ÏßàÎ¨∏ Î∂ÑÎ•ò"""
        question_lower = question.lower()
        
        patterns = {
            'object_detection': ['Î¨¥ÏóáÏù¥', 'Î≠êÍ∞Ä', 'Í∞ùÏ≤¥', 'ÏÇ¨Î¨º', 'ÎÇòÏò§Îäî', 'Î≥¥Ïù¥Îäî'],
            'people_analysis': ['ÏÇ¨Îûå', 'Ïù∏Î¨º', 'ÏñºÍµ¥', 'ÏÑ±Î≥Ñ', 'ÎÇòÏù¥', 'Ïò∑'],
            'scene_analysis': ['Ïû•Î©¥', 'Î∞∞Í≤Ω', 'ÌôòÍ≤Ω', 'Ïû•ÏÜå', 'ÏúÑÏπò', 'ÏãúÍ∞Ñ'],
            'action_analysis': ['ÌñâÎèô', 'ÎèôÏûë', 'ÌïòÍ≥†Ïûà', 'ÏõÄÏßÅÏûÑ', 'ÌôúÎèô'],
            'summary': ['ÏöîÏïΩ', 'Ï†ïÎ¶¨', 'Ï†ÑÏ≤¥', 'ÎÇ¥Ïö©', 'Ï§ÑÍ±∞Î¶¨'],
            'specific_search': ['Ï∞æÏïÑ', 'Í≤ÄÏÉâ', 'Ïñ∏Ï†ú', 'Ïñ¥ÎîîÏÑú', 'Î™á Î≤àÏß∏']
        }
        
        for category, keywords in patterns.items():
            if any(keyword in question_lower for keyword in keywords):
                return category
        
        return 'general'
    
    @staticmethod
    def generate_question_suggestions(video_analysis_data: Dict) -> List[str]:
        """ÎπÑÎîîÏò§ Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú ÏßàÎ¨∏ Ï†úÏïà ÏÉùÏÑ±"""
        suggestions = []
        
        # Í∞ùÏ≤¥ Í∏∞Î∞ò ÏßàÎ¨∏
        if 'objects' in video_analysis_data:
            objects = video_analysis_data['objects'][:3]
            suggestions.extend([
                f"{obj}Í∞Ä Ïñ∏Ï†ú ÎÇòÏò§ÎÇòÏöî?" for obj in objects
            ])
        
        # Ïû•Î©¥ Í∏∞Î∞ò ÏßàÎ¨∏
        suggestions.extend([
            "ÎπÑÎîîÏò§Ïùò Ï£ºÏöî Ïû•Î©¥ÏùÑ ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî",
            "Ïñ¥Îñ§ ÏÇ¨ÎûåÎì§Ïù¥ ÎÇòÏò§ÎÇòÏöî?",
            "Ï£ºÏöî ÌñâÎèôÏù¥ÎÇò ÌôúÎèôÏùÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?",
            "Î∞∞Í≤ΩÏù¥ÎÇò Ïû•ÏÜåÎäî Ïñ¥ÎîîÏù∏Í∞ÄÏöî?"
        ])
        
        return suggestions[:5]  # ÏÉÅÏúÑ 5Í∞úÎßå Î∞òÌôò


# üÜï Ï∫êÏã± ÏãúÏä§ÌÖú (ÏÑ†ÌÉùÏÇ¨Ìï≠)
from django.core.cache import cache
from hashlib import md5

class QACache:
    """QA ÏùëÎãµ Ï∫êÏã± ÏãúÏä§ÌÖú"""
    
    @staticmethod
    def get_cache_key(video_id: str, question: str) -> str:
        """Ï∫êÏãú ÌÇ§ ÏÉùÏÑ±"""
        content = f"{video_id}:{question}"
        return f"qa_cache:{md5(content.encode()).hexdigest()}"
    
    @staticmethod
    def get_cached_response(video_id: str, question: str) -> Optional[Dict]:
        """Ï∫êÏãúÎêú ÏùëÎãµ Ï°∞Ìöå"""
        cache_key = QACache.get_cache_key(video_id, question)
        return cache.get(cache_key)
    
    @staticmethod
    def cache_response(video_id: str, question: str, response: Dict, timeout: int = 300):
        """ÏùëÎãµ Ï∫êÏã±"""
        cache_key = QACache.get_cache_key(video_id, question)
        cache.set(cache_key, response, timeout)
    
    @staticmethod
    def clear_video_cache(video_id: str):
        """ÌäπÏ†ï ÎπÑÎîîÏò§Ïùò Î™®Îì† Ï∫êÏãú ÏÇ≠Ï†ú"""
        # Ï∫êÏãú Ìå®ÌÑ¥ÏúºÎ°ú ÏÇ≠Ï†ú (Redis ÏÇ¨Ïö© Ïãú)
        pattern = f"qa_cache:*{video_id}*"
        # Íµ¨ÌòÑÏùÄ ÏÇ¨Ïö©ÌïòÎäî Ï∫êÏãú Î∞±ÏóîÎìúÏóê Îî∞Îùº Îã¨ÎùºÏßëÎãàÎã§.
        
# ÏÉàÎ°úÏö¥ Î∑∞ Ï∂îÍ∞Ä: AnalysisCapabilitiesView ÏôÑÏ†Ñ Íµ¨ÌòÑ
class AnalysisCapabilitiesView(APIView):
    """ÏãúÏä§ÌÖú Î∂ÑÏÑù Í∏∞Îä• ÏÉÅÌÉú ÌôïÏù∏ - ÏôÑÏ†Ñ Íµ¨ÌòÑ"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            print("üîç AnalysisCapabilitiesView: Î∂ÑÏÑù Í∏∞Îä• ÏÉÅÌÉú ÏöîÏ≤≠")
            
            # VideoAnalyzer ÏÉÅÌÉú ÌôïÏù∏
            analyzer_status = self._check_video_analyzer()
            
            # MultiLLM ÏÉÅÌÉú ÌôïÏù∏
            multi_llm_status = self._check_multi_llm_analyzer()
            
            # ÏãúÏä§ÌÖú Í∏∞Îä• ÏÉÅÌÉú
            capabilities = {
                'system_status': {
                    'analyzer_available': analyzer_status['available'],
                    'multi_llm_available': multi_llm_status['available'],
                    'device': analyzer_status.get('device', 'unknown'),
                    'timestamp': datetime.now().isoformat()
                },
                'core_features': {
                    'object_detection': {
                        'name': 'Í∞ùÏ≤¥ Í∞êÏßÄ',
                        'available': analyzer_status.get('yolo_available', False),
                        'description': 'YOLO Í∏∞Î∞ò Ïã§ÏãúÍ∞Ñ Í∞ùÏ≤¥ Í∞êÏßÄ',
                        'icon': 'üéØ'
                    },
                    'enhanced_captions': {
                        'name': 'Í≥†Í∏â Ï∫°ÏÖò ÏÉùÏÑ±',
                        'available': True,
                        'description': 'AI Í∏∞Î∞ò ÏÉÅÏÑ∏ Ï∫°ÏÖò ÏÉùÏÑ±',
                        'icon': 'üí¨'
                    }
                },
                'advanced_features': {
                    'clip_analysis': {
                        'name': 'CLIP Î∂ÑÏÑù',
                        'available': analyzer_status.get('clip_available', False),
                        'description': 'OpenAI CLIP Î™®Îç∏ Í∏∞Î∞ò Ïî¨ Ïù¥Ìï¥',
                        'icon': 'üñºÔ∏è'
                    },
                    'ocr_text_extraction': {
                        'name': 'OCR ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú',
                        'available': analyzer_status.get('ocr_available', False),
                        'description': 'EasyOCR Í∏∞Î∞ò Îã§Íµ≠Ïñ¥ ÌÖçÏä§Ìä∏ Ïù∏Ïãù',
                        'icon': 'üìù'
                    },
                    'vqa_analysis': {
                        'name': 'VQA ÏßàÎ¨∏ÎãµÎ≥Ä',
                        'available': analyzer_status.get('vqa_available', False),
                        'description': 'BLIP Î™®Îç∏ Í∏∞Î∞ò ÏãúÍ∞ÅÏ†Å ÏßàÎ¨∏ ÎãµÎ≥Ä',
                        'icon': '‚ùì'
                    },
                    'scene_graph': {
                        'name': 'Scene Graph',
                        'available': analyzer_status.get('scene_graph_available', False),
                        'description': 'NetworkX Í∏∞Î∞ò Í∞ùÏ≤¥ Í¥ÄÍ≥Ñ Î∂ÑÏÑù',
                        'icon': 'üï∏Ô∏è'
                    }
                },
                'multi_llm_features': {
                    'gpt4v': {
                        'name': 'GPT-4V',
                        'available': multi_llm_status.get('gpt4v_available', False),
                        'description': 'OpenAI GPT-4 Vision',
                        'icon': 'üü¢'
                    },
                    'claude': {
                        'name': 'Claude-3.5',
                        'available': multi_llm_status.get('claude_available', False),
                        'description': 'Anthropic Claude-3.5 Sonnet',
                        'icon': 'üü†'
                    },
                    'gemini': {
                        'name': 'Gemini Pro',
                        'available': multi_llm_status.get('gemini_available', False),
                        'description': 'Google Gemini Pro Vision',
                        'icon': 'üîµ'
                    },
                    'groq': {
                        'name': 'Groq Llama',
                        'available': multi_llm_status.get('groq_available', False),
                        'description': 'Groq Llama-3.1-70B',
                        'icon': '‚ö°'
                    }
                },
                'api_status': {
                    'openai_available': multi_llm_status.get('openai_api_key', False),
                    'anthropic_available': multi_llm_status.get('anthropic_api_key', False),
                    'google_available': multi_llm_status.get('google_api_key', False),
                    'groq_available': multi_llm_status.get('groq_api_key', False)
                }
            }
            
            # ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Í∏∞Îä• Ïàò Í≥ÑÏÇ∞
            total_features = (len(capabilities['core_features']) + 
                            len(capabilities['advanced_features']) + 
                            len(capabilities['multi_llm_features']))
            
            available_features = sum(1 for features in [
                capabilities['core_features'], 
                capabilities['advanced_features'],
                capabilities['multi_llm_features']
            ] for feature in features.values() if feature.get('available', False))
            
            capabilities['summary'] = {
                'total_features': total_features,
                'available_features': available_features,
                'availability_rate': (available_features / total_features * 100) if total_features > 0 else 0,
                'system_ready': analyzer_status['available'] and available_features > 0,
                'multi_llm_ready': multi_llm_status['available'] and multi_llm_status['model_count'] > 0
            }
            
            print(f"‚úÖ Î∂ÑÏÑù Í∏∞Îä• ÏÉÅÌÉú: {available_features}/{total_features} ÏÇ¨Ïö© Í∞ÄÎä•")
            
            return Response(capabilities)
            
        except Exception as e:
            print(f"‚ùå AnalysisCapabilitiesView Ïò§Î•ò: {e}")
            import traceback
            print(f"üîç ÏÉÅÏÑ∏ Ïò§Î•ò: {traceback.format_exc()}")
            
            return Response({
                'system_status': {
                    'analyzer_available': False,
                    'multi_llm_available': False,
                    'device': 'error',
                    'error': str(e)
                },
                'summary': {
                    'system_ready': False,
                    'error': str(e)
                }
            }, status=500)
    
    def _check_video_analyzer(self):
        """VideoAnalyzer ÏÉÅÌÉú ÌôïÏù∏"""
        try:
            analyzer = get_video_analyzer()
            return {
                'available': True,
                'device': getattr(analyzer, 'device', 'cpu'),
                'yolo_available': getattr(analyzer, 'model', None) is not None,
                'clip_available': getattr(analyzer, 'clip_available', False),
                'ocr_available': getattr(analyzer, 'ocr_available', False),
                'vqa_available': getattr(analyzer, 'vqa_available', False),
                'scene_graph_available': getattr(analyzer, 'scene_graph_available', False)
            }
        except Exception as e:
            print(f"‚ùå VideoAnalyzer ÏÉÅÌÉú ÌôïÏù∏ Ïã§Ìå®: {e}")
            return {'available': False, 'error': str(e)}
    
    def _check_multi_llm_analyzer(self):
        """MultiLLM ÏÉÅÌÉú ÌôïÏù∏"""
        try:
            multi_llm = get_multi_llm_analyzer()
            available_models = getattr(multi_llm, 'available_models', [])
            
            return {
                'available': len(available_models) > 0,
                'model_count': len(available_models),
                'available_models': available_models,
                'gpt4v_available': 'gpt-4v' in available_models,
                'claude_available': 'claude-3.5' in available_models,
                'gemini_available': 'gemini-pro' in available_models,
                'groq_available': 'groq-llama' in available_models,
                'openai_api_key': bool(os.getenv("OPENAI_API_KEY")),
                'anthropic_api_key': bool(os.getenv("ANTHROPIC_API_KEY")),
                'google_api_key': bool(os.getenv("GOOGLE_API_KEY")),
                'groq_api_key': bool(os.getenv("GROQ_API_KEY"))
            }
        except Exception as e:
            print(f"‚ùå MultiLLM ÏÉÅÌÉú ÌôïÏù∏ Ïã§Ìå®: {e}")
            return {'available': False, 'error': str(e)}


# ÏÉàÎ°úÏö¥ Î∑∞: MultiLLM Ï†ÑÏö© Ï±ÑÌåÖ Î∑∞
class MultiLLMChatView(APIView):
    """Î©ÄÌã∞ LLM Ï†ÑÏö© Ï±ÑÌåÖ Î∑∞"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        self.multi_llm_analyzer = get_multi_llm_analyzer()
    
    def post(self, request):
        try:
            user_query = request.data.get('message', '').strip()
            video_id = request.data.get('video_id')
            analysis_mode = request.data.get('analysis_mode', 'comparison')
            
            if not user_query:
                return Response({'error': 'Î©îÏãúÏßÄÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'}, status=400)
            
            # ÎπÑÎîîÏò§Í∞Ä ÏóÜÏñ¥ÎèÑ ÌÖçÏä§Ìä∏ Í∏∞Î∞òÏúºÎ°ú Ï≤òÎ¶¨ Í∞ÄÎä•
            video = None
            video_context = {}
            frame_images = []
            
            if video_id:
                try:
                    video = Video.objects.get(id=video_id)
                    video_context = self._prepare_video_context(video)
                    frame_images = self._extract_frames_safely(video)
                except Video.DoesNotExist:
                    pass  # ÎπÑÎîîÏò§ ÏóÜÏù¥ÎèÑ ÏßÑÌñâ
            
            # Î©ÄÌã∞ LLM Î∂ÑÏÑù Ïã§Ìñâ
            multi_responses = self.multi_llm_analyzer.analyze_video_multi_llm(
                frame_images, user_query, video_context
            )
            
            comparison_result = self.multi_llm_analyzer.compare_responses(multi_responses)
            
            return Response({
                'response_type': 'multi_llm_result',
                'query': user_query,
                'video_info': {'id': video.id, 'name': video.original_name} if video else None,
                'llm_responses': {
                    model: {
                        'response': resp.response_text,
                        'confidence': resp.confidence_score,
                        'processing_time': resp.processing_time,
                        'success': resp.success,
                        'error': resp.error
                    }
                    for model, resp in multi_responses.items()
                },
                'comparison_analysis': comparison_result['comparison'],
                'recommendation': comparison_result['comparison']['recommendation']
            })
            
        except Exception as e:
            print(f"‚ùå MultiLLM Ï±ÑÌåÖ Ïò§Î•ò: {e}")
            return Response({'error': str(e)}, status=500)
    
    def _prepare_video_context(self, video):
        """ÎπÑÎîîÏò§ Ïª®ÌÖçÏä§Ìä∏ Ï§ÄÎπÑ"""
        context = {
            'duration': video.duration,
            'filename': video.original_name
        }
        
        if hasattr(video, 'analysis') and video.analysis:
            try:
                stats = video.analysis.analysis_statistics
                context.update({
                    'detected_objects': stats.get('dominant_objects', []),
                    'scene_types': stats.get('scene_types', [])
                })
            except:
                pass
        
        return context
    
    def _extract_frames_safely(self, video):
        """ÏïàÏ†ÑÌïú ÌîÑÎ†àÏûÑ Ï∂îÏ∂ú"""
        try:
            # EnhancedVideoChatViewÏùò Î©îÏÑúÎìú Ïû¨ÏÇ¨Ïö©
            view = EnhancedVideoChatView()
            return view._extract_key_frames_for_llm(video, max_frames=2)
        except:
            return []


# LLM ÌÜµÍ≥Ñ Î∑∞ Ï∂îÍ∞Ä
class LLMStatsView(APIView):
    """LLM ÏÑ±Îä• ÌÜµÍ≥Ñ Î∑∞"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            # Í∞ÑÎã®Ìïú ÌÜµÍ≥Ñ Î∞òÌôò (Ïã§Ï†úÎ°úÎäî Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú ÏàòÏßë)
            stats = {
                'total_requests': 0,
                'model_usage': {
                    'gpt-4v': {'count': 0, 'avg_time': 0, 'success_rate': 0},
                    'claude-3.5': {'count': 0, 'avg_time': 0, 'success_rate': 0},
                    'gemini-pro': {'count': 0, 'avg_time': 0, 'success_rate': 0},
                    'groq-llama': {'count': 0, 'avg_time': 0, 'success_rate': 0}
                },
                'average_response_time': 0,
                'overall_success_rate': 0,
                'last_updated': datetime.now().isoformat()
            }
            
            return Response(stats)
            
        except Exception as e:
            return Response({'error': str(e)}, status=500)

class VideoListView(APIView):
    """ÎπÑÎîîÏò§ Î™©Î°ù Ï°∞Ìöå - Í≥†Í∏â Î∂ÑÏÑù Ï†ïÎ≥¥ Ìè¨Ìï®"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            print("üîç VideoListView: ÎπÑÎîîÏò§ Î™©Î°ù ÏöîÏ≤≠ (Í≥†Í∏â Î∂ÑÏÑù Ï†ïÎ≥¥ Ìè¨Ìï®)")
            videos = Video.objects.all()
            video_list = []
            
            for video in videos:
                video_data = {
                    'id': video.id,
                    'filename': video.filename,
                    'original_name': video.original_name,
                    'duration': video.duration,
                    'is_analyzed': video.is_analyzed,
                    'analysis_status': video.analysis_status,
                    'uploaded_at': video.uploaded_at,
                    'file_size': video.file_size
                }
                
                # Í≥†Í∏â Î∂ÑÏÑù Ï†ïÎ≥¥ Ï∂îÍ∞Ä
                if hasattr(video, 'analysis'):
                    analysis = video.analysis
                    stats = analysis.analysis_statistics
                    
                    video_data.update({
                        'enhanced_analysis': analysis.enhanced_analysis,
                        'success_rate': analysis.success_rate,
                        'processing_time': analysis.processing_time_seconds,
                        'analysis_type': stats.get('analysis_type', 'basic'),
                        'advanced_features_used': {
                            'clip': stats.get('clip_analysis', False),
                            'ocr': stats.get('text_extracted', False),
                            'vqa': stats.get('vqa_analysis', False),
                            'scene_graph': stats.get('scene_graph_analysis', False)
                        },
                        'scene_types': stats.get('scene_types', []),
                        'unique_objects': stats.get('unique_objects', 0)
                    })
                
                # ÏßÑÌñâÎ•† Ï†ïÎ≥¥ Ï∂îÍ∞Ä (Î∂ÑÏÑù Ï§ëÏù∏ Í≤ΩÏö∞)
                if video.analysis_status == 'processing':
                    progress_info = progress_tracker.get_progress(video.id)
                    if progress_info:
                        video_data['progress_info'] = progress_info
                
                video_list.append(video_data)
            
            print(f"‚úÖ VideoListView: {len(video_list)}Í∞ú ÎπÑÎîîÏò§ Î∞òÌôò (Í≥†Í∏â Î∂ÑÏÑù Ï†ïÎ≥¥ Ìè¨Ìï®)")
            return Response({
                'videos': video_list,
                'total_count': len(video_list),
                'analysis_capabilities': self._get_system_capabilities()
            })
            
        except Exception as e:
            print(f"‚ùå VideoListView Ïò§Î•ò: {e}")
            return Response({
                'error': f'ÎπÑÎîîÏò§ Î™©Î°ù Ï°∞Ìöå Ïò§Î•ò: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _get_system_capabilities(self):
        """ÏãúÏä§ÌÖú Î∂ÑÏÑù Í∏∞Îä• ÏÉÅÌÉú"""
        try:
            # ‚úÖ ÏàòÏ†ï: Ï†ÑÏó≠ VideoAnalyzer Ïù∏Ïä§ÌÑ¥Ïä§ ÏÇ¨Ïö©
            analyzer = get_video_analyzer()
            return {
                'clip_available': analyzer.clip_available,
                'ocr_available': analyzer.ocr_available,
                'vqa_available': analyzer.vqa_available,
                'scene_graph_available': analyzer.scene_graph_available
            }
        except:
            return {
                'clip_available': False,
                'ocr_available': False,
                'vqa_available': False,
                'scene_graph_available': False
            }

class AnalysisStatusView(APIView):
    """Î∂ÑÏÑù ÏÉÅÌÉú ÌôïÏù∏ - ÏßÑÌñâÎ•† Ï†ïÎ≥¥ Ìè¨Ìï®"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            response_data = {
                'status': video.analysis_status,
                'video_filename': video.filename,
                'is_analyzed': video.is_analyzed
            }
            
            # ÏßÑÌñâÎ•† Ï†ïÎ≥¥ Ï∂îÍ∞Ä
            if video.analysis_status == 'processing':
                progress_info = progress_tracker.get_progress(video.id)
                response_data.update(progress_info)
            
            # Î∂ÑÏÑù ÏôÑÎ£åÎêú Í≤ΩÏö∞ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï∂îÍ∞Ä
            if hasattr(video, 'analysis'):
                analysis = video.analysis
                response_data.update({
                    'enhanced_analysis': analysis.enhanced_analysis,
                    'success_rate': analysis.success_rate,
                    'processing_time': analysis.processing_time_seconds,
                    'stats': {
                        'objects': analysis.analysis_statistics.get('unique_objects', 0),
                        'scenes': Scene.objects.filter(video=video).count(),
                        'captions': analysis.caption_statistics.get('frames_with_caption', 0)
                    }
                })
            
            return Response(response_data)
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
import threading
import time
import json
import cv2
import os
import base64
from datetime import datetime
from django.conf import settings
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory, CostAnalysis
from .llm_client import LLMClient

# ‚úÖ ÏïàÏ†ÑÌïú import
try:
    from .video_analyzer import get_video_analyzer
except ImportError:
    print("‚ö†Ô∏è video_analyzer import Ïã§Ìå®")
    get_video_analyzer = None

try:
    from .multi_llm_service import get_multi_llm_analyzer
except ImportError:
    print("‚ö†Ô∏è multi_llm_service import Ïã§Ìå®")
    get_multi_llm_analyzer = None

# ‚úÖ ÏàòÏ†ïÎêú AnalyzeVideoView - URL ÌååÎùºÎØ∏ÌÑ∞ Ï≤òÎ¶¨
class AnalyzeVideoView(APIView):
    """Í∏∞Î≥∏ ÎπÑÎîîÏò§ Î∂ÑÏÑù ÏãúÏûë"""
    permission_classes = [AllowAny]
    
    def post(self, request, video_id):  # ‚úÖ video_id ÌååÎùºÎØ∏ÌÑ∞ Ï∂îÍ∞Ä
        try:
            print(f"üî¨ Í∏∞Î≥∏ ÎπÑÎîîÏò§ Î∂ÑÏÑù ÏãúÏûë: video_id={video_id}")
            
            enable_enhanced = request.data.get('enable_enhanced_analysis', False)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({
                    'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
                }, status=status.HTTP_404_NOT_FOUND)
            
            # Ïù¥ÎØ∏ Î∂ÑÏÑù Ï§ëÏù∏ÏßÄ ÌôïÏù∏
            if video.analysis_status == 'processing':
                return Response({
                    'error': 'Ïù¥ÎØ∏ Î∂ÑÏÑùÏù¥ ÏßÑÌñâ Ï§ëÏûÖÎãàÎã§.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # Î∂ÑÏÑù ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
            video.analysis_status = 'processing'
            video.save()
            
            # Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú Î∂ÑÏÑù ÏãúÏûë
            analysis_thread = threading.Thread(
                target=self._run_basic_analysis,
                args=(video, enable_enhanced),
                daemon=True
            )
            analysis_thread.start()
            
            return Response({
                'success': True,
                'message': 'Í∏∞Î≥∏ ÎπÑÎîîÏò§ Î∂ÑÏÑùÏù¥ ÏãúÏûëÎêòÏóàÏäµÎãàÎã§.',
                'video_id': video.id,
                'enhanced_analysis': enable_enhanced,
                'estimated_time': '5-10Î∂Ñ'
            })
            
        except Exception as e:
            print(f"‚ùå Í∏∞Î≥∏ Î∂ÑÏÑù ÏãúÏûë Ïò§Î•ò: {e}")
            return Response({
                'error': f'Î∂ÑÏÑù ÏãúÏûë Ï§ë Ïò§Î•ò: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _run_basic_analysis(self, video, enable_enhanced):
        """Î∞±Í∑∏ÎùºÏö¥Îìú Í∏∞Î≥∏ Î∂ÑÏÑù"""
        try:
            print(f"üî¨ Í∏∞Î≥∏ Î∂ÑÏÑù Ïã§Ìñâ: {video.original_name}")
            
            # Í∞ÑÎã®Ìïú Î∂ÑÏÑù ÏãúÎÆ¨Î†àÏù¥ÏÖò
            time.sleep(2)  # Ïã§Ï†úÎ°úÎäî Î∂ÑÏÑù Î°úÏßÅ ÏàòÌñâ
            
            # VideoAnalysis ÏÉùÏÑ±
            analysis = VideoAnalysis.objects.create(
                video=video,
                enhanced_analysis=enable_enhanced,
                success_rate=85.0,
                processing_time_seconds=120,
                analysis_statistics={
                    'analysis_type': 'basic',
                    'unique_objects': 8,
                    'total_detections': 45,
                    'scene_types': ['outdoor', 'urban']
                },
                caption_statistics={
                    'frames_with_caption': 25,
                    'average_confidence': 0.8
                }
            )
            
            # ÏôÑÎ£å ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
            video.analysis_status = 'completed'
            video.is_analyzed = True
            video.save()
            
            print(f"‚úÖ Í∏∞Î≥∏ Î∂ÑÏÑù ÏôÑÎ£å: {video.original_name}")
            
        except Exception as e:
            print(f"‚ùå Í∏∞Î≥∏ Î∂ÑÏÑù Ïã§Ìå®: {e}")
            video.analysis_status = 'failed'
            video.save()

class AnalysisProgressView(APIView):
    """Î∂ÑÏÑù ÏßÑÌñâÎ•† Ï†ÑÏö© API"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            progress_info = progress_tracker.get_progress(video_id)
            
            if not progress_info:
                return Response({
                    'error': 'ÏßÑÌñâ Ï§ëÏù∏ Î∂ÑÏÑùÏù¥ ÏóÜÏäµÎãàÎã§'
                }, status=status.HTTP_404_NOT_FOUND)
            
            return Response(progress_info)
            
        except Exception as e:
            return Response({
                'error': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


# Í∏∞Ï°¥Ïùò Îã§Î•∏ View ÌÅ¥ÎûòÏä§Îì§ÏùÄ Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ
class VideoUploadView(APIView):
    """ÎπÑÎîîÏò§ ÏóÖÎ°úÎìú"""
    permission_classes = [AllowAny]
    parser_classes = (MultiPartParser, FormParser)
    
    def post(self, request):
        try:
            if 'video' not in request.FILES:
                return Response({
                    'error': 'ÎπÑÎîîÏò§ ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            video_file = request.FILES['video']
            
            if not video_file.name.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.webm')):
                return Response({
                    'error': 'ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÌååÏùº ÌòïÏãùÏûÖÎãàÎã§'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # Generate unique filename
            timestamp = int(time.time())
            filename = f"upload_{timestamp}_{video_file.name}"
            
            # Save file
            file_path = default_storage.save(
                f'uploads/{filename}',
                ContentFile(video_file.read())
            )
            
            # Create Video model instance
            video = Video.objects.create(
                filename=filename,
                original_name=video_file.name,
                file_path=file_path,
                file_size=video_file.size,
                analysis_status='pending'
            )
            
            return Response({
                'success': True,
                'video_id': video.id,
                'filename': filename,
                'message': f'ÎπÑÎîîÏò§ "{video_file.name}"Ïù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏóÖÎ°úÎìúÎêòÏóàÏäµÎãàÎã§.'
            })
            
        except Exception as e:
            return Response({
                'error': f'ÏóÖÎ°úÎìú Ïò§Î•ò: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class APIStatusView(APIView):
    """API ÏÉÅÌÉú ÌôïÏù∏"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        # print("üîç APIStatusView: API ÏÉÅÌÉú ÏöîÏ≤≠ Î∞õÏùå")
        try:
            llm_client = LLMClient()
            status_info = llm_client.get_api_status()
            
            response_data = {
                'groq': status_info.get('groq', {'available': False}),
                'openai': status_info.get('openai', {'available': False}),
                'anthropic': status_info.get('anthropic', {'available': False}),
                'fallback_enabled': True,
                'timestamp': datetime.now().isoformat(),
                'server_status': 'running',
                'active_analyses': len([k for k, v in progress_tracker.progress_data.items() 
                                     if v.get('progress', 0) < 100])
            }
            
            # print(f"‚úÖ APIStatusView: ÏÉÅÌÉú Ï†ïÎ≥¥ Î∞òÌôò - {response_data}")
            return Response(response_data)
        except Exception as e:
            print(f"‚ùå APIStatusView Ïò§Î•ò: {e}")
            return Response({
                'error': str(e),
                'server_status': 'error'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)



class VideoChatView(APIView):
    """ÎπÑÎîîÏò§ Í¥ÄÎ†® Ï±ÑÌåÖ API - Í∏∞Ï°¥ ChatViewÏôÄ Íµ¨Î∂Ñ"""
    permission_classes = [AllowAny]  # üîß Í∂åÌïú ÏÑ§Ï†ï Ï∂îÍ∞Ä
    
    def __init__(self):
        super().__init__()
        self.llm_client = LLMClient()
        self.video_analyzer = VideoAnalyzer()
    
    def post(self, request):
        try:
            user_message = request.data.get('message', '').strip()
            video_id = request.data.get('video_id')
            
            if not user_message:
                return Response({'response': 'Î©îÏãúÏßÄÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'})
            
            print(f"üí¨ ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄ: {user_message}")
            
            # Get current video
            if video_id:
                try:
                    current_video = Video.objects.get(id=video_id)
                except Video.DoesNotExist:
                    current_video = Video.objects.filter(is_analyzed=True).first()
            else:
                current_video = Video.objects.filter(is_analyzed=True).first()
            
            if not current_video:
                return Response({
                    'response': 'Î∂ÑÏÑùÎêú ÎπÑÎîîÏò§Í∞Ä ÏóÜÏäµÎãàÎã§. ÎπÑÎîîÏò§Î•º ÏóÖÎ°úÎìúÌïòÍ≥† Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî.'
                })
            
            # Get video info
            video_info = self._get_video_info(current_video)
            
            # Determine if multi-LLM should be used
            use_multi_llm = "compare" in user_message.lower() or "ÎπÑÍµê" in user_message or "Î∂ÑÏÑù" in user_message
            
            # Handle different query types
            if self._is_search_query(user_message):
                return self._handle_search_query(user_message, current_video, video_info, use_multi_llm)
            
            elif self._is_highlight_query(user_message):
                return self._handle_highlight_query(user_message, current_video, video_info, use_multi_llm)
            
            elif self._is_summary_query(user_message):
                return self._handle_summary_query(user_message, current_video, video_info, use_multi_llm)
            
            elif self._is_info_query(user_message):
                return self._handle_info_query(user_message, current_video, video_info, use_multi_llm)
            
            else:
                # General conversation
                bot_response = self.llm_client.generate_smart_response(
                    user_query=user_message,
                    search_results=None,
                    video_info=video_info,
                    use_multi_llm=use_multi_llm
                )
                return Response({'response': bot_response})
                
        except Exception as e:
            print(f"‚ùå Chat error: {e}")
            error_response = self.llm_client.generate_smart_response(
                user_query="ÏãúÏä§ÌÖú Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. ÎèÑÏõÄÏùÑ ÏöîÏ≤≠Ìï©ÎãàÎã§.",
                search_results=None,
                video_info=None
            )
            return Response({'response': error_response})
    
    # ... Í∏∞Ï°¥ Î©îÏÑúÎìúÎì§ Ïú†ÏßÄ


class FrameView(APIView):
    """ÌîÑÎ†àÏûÑ Ïù¥ÎØ∏ÏßÄ Ï†úÍ≥µ"""
    permission_classes = [AllowAny]  # üîß Í∂åÌïú ÏÑ§Ï†ï Ï∂îÍ∞Ä
    
    def get(self, request, video_id, frame_number, frame_type='normal'):
        try:
            video = Video.objects.get(id=video_id)
            
            # Get video file path
            video_path = None
            possible_paths = [
                os.path.join(settings.VIDEO_FOLDER, video.filename),
                os.path.join(settings.UPLOAD_FOLDER, video.filename),
                video.file_path
            ]
            
            for path in possible_paths:
                if os.path.exists(path):
                    video_path = path
                    break
            
            if not video_path:
                return Response({
                    'error': 'ÎπÑÎîîÏò§ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§'
                }, status=status.HTTP_404_NOT_FOUND)
            
            # Extract frame
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                return Response({
                    'error': 'ÎπÑÎîîÏò§ ÌååÏùºÏùÑ Ïó¥ Ïàò ÏóÜÏäµÎãàÎã§'
                }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            cap.set(cv2.CAP_PROP_POS_FRAMES, max(0, frame_number - 1))
            ret, frame = cap.read()
            cap.release()
            
            if not ret:
                return Response({
                    'error': 'ÌîÑÎ†àÏûÑÏùÑ Ï∂îÏ∂úÌï† Ïàò ÏóÜÏäµÎãàÎã§'
                }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # Handle annotated frames
            if frame_type == 'annotated':
                target_class = request.GET.get('class', '').lower()
                frame = self._annotate_frame(frame, video, frame_number, target_class)
            
            # Resize frame if too large
            height, width = frame.shape[:2]
            if width > 800:
                ratio = 800 / width
                new_width = 800
                new_height = int(height * ratio)
                frame = cv2.resize(frame, (new_width, new_height))
            
            # Save temporary image
            temp_filename = f'frame_{video.id}_{frame_number}_{int(time.time())}.jpg'
            temp_path = os.path.join(settings.IMAGE_FOLDER, temp_filename)
            
            cv2.imwrite(temp_path, frame, [cv2.IMWRITE_JPEG_QUALITY, 90])
            
            return FileResponse(
                open(temp_path, 'rb'),
                content_type='image/jpeg',
                filename=temp_filename
            )
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class ScenesView(APIView):
    """Scene Î™©Î°ù Ï°∞Ìöå"""
    permission_classes = [AllowAny]  # üîß Í∂åÌïú ÏÑ§Ï†ï Ï∂îÍ∞Ä
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            scenes = Scene.objects.filter(video=video).order_by('scene_id')
            
            scene_list = []
            for scene in scenes:
                scene_data = {
                    'scene_id': scene.scene_id,
                    'start_time': scene.start_time,
                    'end_time': scene.end_time,
                    'duration': scene.duration,
                    'frame_count': scene.frame_count,
                    'dominant_objects': scene.dominant_objects,
                    'caption_type': 'enhanced' if scene.enhanced_captions_count > 0 else 'basic'
                }
                scene_list.append(scene_data)
            
            return Response({
                'scenes': scene_list,
                'total_scenes': len(scene_list),
                'analysis_type': 'enhanced' if hasattr(video, 'analysis') and video.analysis.enhanced_analysis else 'basic'
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        



import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.permissions import AllowAny
from datetime import datetime, timedelta
from collections import Counter
import threading
import queue

from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient


class AnalysisFeaturesView(APIView):
    """Î∂ÑÏÑù Í∏∞Îä•Î≥Ñ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï†úÍ≥µ"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            analyzer = VideoAnalyzer()
            
            features = {
                'object_detection': {
                    'name': 'Í∞ùÏ≤¥ Í∞êÏßÄ',
                    'description': 'YOLO Í∏∞Î∞ò Ïã§ÏãúÍ∞Ñ Í∞ùÏ≤¥ Í∞êÏßÄ Î∞è Î∂ÑÎ•ò',
                    'available': True,
                    'processing_time_factor': 1.0,
                    'icon': 'üéØ',
                    'details': 'ÎπÑÎîîÏò§ ÎÇ¥ ÏÇ¨Îûå, Ï∞®Îüâ, ÎèôÎ¨º Îì± Îã§ÏñëÌïú Í∞ùÏ≤¥Î•º Ï†ïÌôïÌïòÍ≤å Í∞êÏßÄÌï©ÎãàÎã§.'
                },
                'clip_analysis': {
                    'name': 'CLIP Ïî¨ Î∂ÑÏÑù',
                    'description': 'OpenAI CLIP Î™®Îç∏ÏùÑ ÌôúÏö©Ìïú Í≥†Í∏â Ïî¨ Ïù¥Ìï¥',
                    'available': analyzer.clip_available,
                    'processing_time_factor': 1.5,
                    'icon': 'üñºÔ∏è',
                    'details': 'Ïù¥ÎØ∏ÏßÄÏùò ÏùòÎØ∏Ï†Å Ïª®ÌÖçÏä§Ìä∏Î•º Ïù¥Ìï¥ÌïòÏó¨ Ïî¨ Î∂ÑÎ•ò Î∞è Î∂ÑÏÑùÏùÑ ÏàòÌñâÌï©ÎãàÎã§.'
                },
                'ocr': {
                    'name': 'OCR ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú',
                    'description': 'EasyOCRÏùÑ ÏÇ¨Ïö©Ìïú Îã§Íµ≠Ïñ¥ ÌÖçÏä§Ìä∏ Ïù∏Ïãù',
                    'available': analyzer.ocr_available,
                    'processing_time_factor': 1.2,
                    'icon': 'üìù',
                    'details': 'ÎπÑÎîîÏò§ ÎÇ¥ ÌïúÍ∏Ä, ÏòÅÎ¨∏ ÌÖçÏä§Ìä∏Î•º Ï†ïÌôïÌïòÍ≤å Ïù∏ÏãùÌïòÍ≥† Ï∂îÏ∂úÌï©ÎãàÎã§.'
                },
                'vqa': {
                    'name': 'VQA ÏßàÎ¨∏ÎãµÎ≥Ä',
                    'description': 'BLIP Î™®Îç∏ Í∏∞Î∞ò ÏãúÍ∞ÅÏ†Å ÏßàÎ¨∏ ÎãµÎ≥Ä',
                    'available': analyzer.vqa_available,
                    'processing_time_factor': 2.0,
                    'icon': '‚ùì',
                    'details': 'Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌïú ÏßàÎ¨∏ÏùÑ ÏÉùÏÑ±ÌïòÍ≥† ÎãµÎ≥ÄÌïòÏó¨ ÍπäÏù¥ ÏûàÎäî Î∂ÑÏÑùÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.'
                },
                'scene_graph': {
                    'name': 'Scene Graph',
                    'description': 'Í∞ùÏ≤¥Í∞Ñ Í¥ÄÍ≥Ñ Î∞è ÏÉÅÌò∏ÏûëÏö© Î∂ÑÏÑù',
                    'available': analyzer.scene_graph_available,
                    'processing_time_factor': 3.0,
                    'icon': 'üï∏Ô∏è',
                    'details': 'Í∞ùÏ≤¥Îì§ ÏÇ¨Ïù¥Ïùò Í¥ÄÍ≥ÑÏôÄ ÏÉÅÌò∏ÏûëÏö©ÏùÑ Î∂ÑÏÑùÌïòÏó¨ Î≥µÏû°Ìïú Ïî¨ÏùÑ Ïù¥Ìï¥Ìï©ÎãàÎã§.'
                },
                'enhanced_caption': {
                    'name': 'Í≥†Í∏â Ï∫°ÏÖò ÏÉùÏÑ±',
                    'description': 'Î™®Îì† Î∂ÑÏÑù Í≤∞Í≥ºÎ•º ÌÜµÌï©Ìïú ÏÉÅÏÑ∏ Ï∫°ÏÖò',
                    'available': True,
                    'processing_time_factor': 1.1,
                    'icon': 'üí¨',
                    'details': 'Ïó¨Îü¨ AI Î™®Îç∏Ïùò Í≤∞Í≥ºÎ•º Ï¢ÖÌï©ÌïòÏó¨ ÏÉÅÏÑ∏ÌïòÍ≥† Ï†ïÌôïÌïú Ï∫°ÏÖòÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.'
                }
            }
            
            return Response({
                'features': features,
                'device': analyzer.device,
                'total_available': sum(1 for f in features.values() if f['available']),
                'recommended_configs': {
                    'basic': ['object_detection', 'enhanced_caption'],
                    'enhanced': ['object_detection', 'clip_analysis', 'ocr', 'enhanced_caption'],
                    'comprehensive': list(features.keys())
                }
            })
            
        except Exception as e:
            return Response({
                'error': f'Î∂ÑÏÑù Í∏∞Îä• Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class AdvancedVideoSearchView(APIView):
    """Í≥†Í∏â ÎπÑÎîîÏò§ Í≤ÄÏÉâ API"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        self.video_analyzer = VideoAnalyzer()
        self.llm_client = LLMClient()
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            query = request.data.get('query', '').strip()
            search_options = request.data.get('search_options', {})
            
            if not query:
                return Response({
                    'error': 'Í≤ÄÏÉâÏñ¥Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            video = Video.objects.get(id=video_id)
            
            # Í≥†Í∏â Í≤ÄÏÉâ ÏàòÌñâ
            search_results = self.video_analyzer.search_comprehensive(video, query)
            
            # Í≥†Í∏â Î∂ÑÏÑù Í≤∞Í≥ºÍ∞Ä Ìè¨Ìï®Îêú ÌîÑÎ†àÏûÑÎì§Ïóê ÎåÄÌï¥ Ï∂îÍ∞Ä Ï†ïÎ≥¥ ÏàòÏßë
            enhanced_results = []
            for result in search_results[:10]:
                frame_id = result.get('frame_id')
                try:
                    frame = Frame.objects.get(video=video, image_id=frame_id)
                    enhanced_result = dict(result)
                    
                    # Í≥†Í∏â Î∂ÑÏÑù Í≤∞Í≥º Ï∂îÍ∞Ä
                    comprehensive_features = frame.comprehensive_features or {}
                    
                    if search_options.get('include_clip_analysis') and 'clip_features' in comprehensive_features:
                        enhanced_result['clip_analysis'] = comprehensive_features['clip_features']
                    
                    if search_options.get('include_ocr_text') and 'ocr_text' in comprehensive_features:
                        enhanced_result['ocr_text'] = comprehensive_features['ocr_text']
                    
                    if search_options.get('include_vqa_results') and 'vqa_results' in comprehensive_features:
                        enhanced_result['vqa_insights'] = comprehensive_features['vqa_results']
                    
                    if search_options.get('include_scene_graph') and 'scene_graph' in comprehensive_features:
                        enhanced_result['scene_graph'] = comprehensive_features['scene_graph']
                    
                    enhanced_results.append(enhanced_result)
                    
                except Frame.DoesNotExist:
                    enhanced_results.append(result)
            
            # AI Í∏∞Î∞ò Í≤ÄÏÉâ Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ±
            search_insights = self._generate_search_insights(query, enhanced_results, video)
            
            return Response({
                'search_results': enhanced_results,
                'query': query,
                'insights': search_insights,
                'total_matches': len(search_results),
                'search_type': 'advanced',
                'video_info': {
                    'id': video.id,
                    'name': video.original_name,
                    'analysis_type': getattr(video, 'analysis_type', 'basic')
                }
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'Í≥†Í∏â Í≤ÄÏÉâ Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _generate_search_insights(self, query, results, video):
        """Í≤ÄÏÉâ Í≤∞Í≥ºÏóê ÎåÄÌïú AI Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ±"""
        try:
            if not results:
                return "Í≤ÄÏÉâ Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§. Îã§Î•∏ Í≤ÄÏÉâÏñ¥Î•º ÏãúÎèÑÌï¥Î≥¥ÏÑ∏Ïöî."
            
            # Í≤ÄÏÉâ Í≤∞Í≥º ÏöîÏïΩ
            insights_prompt = f"""
            Í≤ÄÏÉâÏñ¥: "{query}"
            ÎπÑÎîîÏò§: {video.original_name}
            Í≤ÄÏÉâ Í≤∞Í≥º: {len(results)}Í∞ú Îß§Ïπ≠
            
            Ï£ºÏöî Î∞úÍ≤¨ÏÇ¨Ìï≠:
            {json.dumps(results[:3], ensure_ascii=False, indent=2)}
            
            Ïù¥ Í≤ÄÏÉâ Í≤∞Í≥ºÏóê ÎåÄÌïú Í∞ÑÎã®ÌïòÍ≥† Ïú†Ïö©Ìïú Ïù∏ÏÇ¨Ïù¥Ìä∏Î•º ÌïúÍµ≠Ïñ¥Î°ú Ï†úÍ≥µÌï¥Ï£ºÏÑ∏Ïöî.
            """
            
            insights = self.llm_client.generate_smart_response(
                user_query=insights_prompt,
                search_results=results[:5],
                video_info=f"ÎπÑÎîîÏò§: {video.original_name}",
                use_multi_llm=False
            )
            
            return insights
            
        except Exception as e:
            return f"Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ± Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"


class EnhancedFrameView(APIView):
    """Í≥†Í∏â Î∂ÑÏÑù Ï†ïÎ≥¥Í∞Ä Ìè¨Ìï®Îêú ÌîÑÎ†àÏûÑ Îç∞Ïù¥ÌÑ∞ Ï†úÍ≥µ"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id, frame_number):
        try:
            video = Video.objects.get(id=video_id)
            
            # ÌîÑÎ†àÏûÑ Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå
            try:
                frame = Frame.objects.get(video=video, image_id=frame_number)
                
                frame_data = {
                    'frame_id': frame.image_id,
                    'timestamp': frame.timestamp,
                    'caption': frame.caption,
                    'enhanced_caption': frame.enhanced_caption,
                    'final_caption': frame.final_caption,
                    'detected_objects': frame.detected_objects,
                    'comprehensive_features': frame.comprehensive_features,
                    'analysis_quality': frame.comprehensive_features.get('caption_quality', 'basic')
                }
                
                # Í≥†Í∏â Î∂ÑÏÑù Í≤∞Í≥º Î∂ÑÌï¥
                if frame.comprehensive_features:
                    features = frame.comprehensive_features
                    
                    frame_data['advanced_analysis'] = {
                        'clip_analysis': features.get('clip_features', {}),
                        'ocr_text': features.get('ocr_text', {}),
                        'vqa_results': features.get('vqa_results', {}),
                        'scene_graph': features.get('scene_graph', {}),
                        'scene_complexity': features.get('scene_complexity', 0)
                    }
                
                return Response(frame_data)
                
            except Frame.DoesNotExist:
                # ÌîÑÎ†àÏûÑ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏúºÎ©¥ Í∏∞Î≥∏ Ïù¥ÎØ∏ÏßÄÎßå Î∞òÌôò
                return Response({
                    'frame_id': frame_number,
                    'message': 'ÌîÑÎ†àÏûÑ Îç∞Ïù¥ÌÑ∞Îäî ÏóÜÏßÄÎßå Ïù¥ÎØ∏ÏßÄÎäî ÏÇ¨Ïö© Í∞ÄÎä•Ìï©ÎãàÎã§.',
                    'image_url': f'/frame/{video_id}/{frame_number}/'
                })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'ÌîÑÎ†àÏûÑ Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class EnhancedScenesView(APIView):
    """Í≥†Í∏â Î∂ÑÏÑù Ï†ïÎ≥¥Í∞Ä Ìè¨Ìï®Îêú Ïî¨ Îç∞Ïù¥ÌÑ∞ Ï†úÍ≥µ"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            scenes = Scene.objects.filter(video=video).order_by('scene_id')
            
            enhanced_scenes = []
            for scene in scenes:
                scene_data = {
                    'scene_id': scene.scene_id,
                    'start_time': scene.start_time,
                    'end_time': scene.end_time,
                    'duration': scene.duration,
                    'frame_count': scene.frame_count,
                    'dominant_objects': scene.dominant_objects,
                    'enhanced_captions_count': scene.enhanced_captions_count,
                    'caption_type': 'enhanced' if scene.enhanced_captions_count > 0 else 'basic'
                }
                
                # Ïî¨ ÎÇ¥ ÌîÑÎ†àÏûÑÎì§Ïùò Í≥†Í∏â Î∂ÑÏÑù Í≤∞Í≥º ÏßëÍ≥Ñ
                scene_frames = Frame.objects.filter(
                    video=video,
                    timestamp__gte=scene.start_time,
                    timestamp__lte=scene.end_time
                )
                
                if scene_frames.exists():
                    # Í≥†Í∏â Í∏∞Îä• ÏÇ¨Ïö© ÌÜµÍ≥Ñ
                    clip_count = sum(1 for f in scene_frames if f.comprehensive_features.get('clip_features'))
                    ocr_count = sum(1 for f in scene_frames if f.comprehensive_features.get('ocr_text', {}).get('texts'))
                    vqa_count = sum(1 for f in scene_frames if f.comprehensive_features.get('vqa_results'))
                    
                    scene_data['advanced_features'] = {
                        'clip_analysis_frames': clip_count,
                        'ocr_text_frames': ocr_count,
                        'vqa_analysis_frames': vqa_count,
                        'total_frames': scene_frames.count()
                    }
                    
                    # Ïî¨ Î≥µÏû°ÎèÑ ÌèâÍ∑†
                    complexities = [f.comprehensive_features.get('scene_complexity', 0) for f in scene_frames]
                    scene_data['average_complexity'] = sum(complexities) / len(complexities) if complexities else 0
                
                enhanced_scenes.append(scene_data)
            
            return Response({
                'scenes': enhanced_scenes,
                'total_scenes': len(enhanced_scenes),
                'analysis_type': 'enhanced' if any(s.get('advanced_features') for s in enhanced_scenes) else 'basic',
                'video_info': {
                    'id': video.id,
                    'name': video.original_name
                }
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'Í≥†Í∏â Ïî¨ Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class AnalysisResultsView(APIView):
    """Ï¢ÖÌï© Î∂ÑÏÑù Í≤∞Í≥º Ï†úÍ≥µ"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            if not video.is_analyzed:
                return Response({
                    'error': 'ÏïÑÏßÅ Î∂ÑÏÑùÏù¥ ÏôÑÎ£åÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            analysis = video.analysis
            scenes = Scene.objects.filter(video=video)
            frames = Frame.objects.filter(video=video)
            
            # Ï¢ÖÌï© Î∂ÑÏÑù Í≤∞Í≥º
            results = {
                'video_info': {
                    'id': video.id,
                    'name': video.original_name,
                    'duration': video.duration,
                    'analysis_type': analysis.analysis_statistics.get('analysis_type', 'basic'),
                    'processing_time': analysis.processing_time_seconds,
                    'success_rate': analysis.success_rate
                },
                'analysis_summary': {
                    'total_scenes': scenes.count(),
                    'total_frames_analyzed': frames.count(),
                    'unique_objects': analysis.analysis_statistics.get('unique_objects', 0),
                    'features_used': analysis.analysis_statistics.get('features_used', []),
                    'scene_types': analysis.analysis_statistics.get('scene_types', [])
                },
                'advanced_features': {
                    'clip_analysis': analysis.analysis_statistics.get('clip_analysis', False),
                    'ocr_text_extracted': analysis.analysis_statistics.get('text_extracted', False),
                    'vqa_analysis': analysis.analysis_statistics.get('vqa_analysis', False),
                    'scene_graph_analysis': analysis.analysis_statistics.get('scene_graph_analysis', False)
                },
                'content_insights': {
                    'dominant_objects': analysis.analysis_statistics.get('dominant_objects', []),
                    'text_content_length': analysis.caption_statistics.get('text_content_length', 0),
                    'enhanced_captions_count': analysis.caption_statistics.get('enhanced_captions', 0),
                    'average_confidence': analysis.caption_statistics.get('average_confidence', 0)
                }
            }
            
            # ÌîÑÎ†àÏûÑÎ≥Ñ Í≥†Í∏â Î∂ÑÏÑù ÌÜµÍ≥Ñ
            if frames.exists():
                clip_frames = sum(1 for f in frames if f.comprehensive_features.get('clip_features'))
                ocr_frames = sum(1 for f in frames if f.comprehensive_features.get('ocr_text', {}).get('texts'))
                vqa_frames = sum(1 for f in frames if f.comprehensive_features.get('vqa_results'))
                
                results['frame_statistics'] = {
                    'total_frames': frames.count(),
                    'clip_analyzed_frames': clip_frames,
                    'ocr_processed_frames': ocr_frames,
                    'vqa_analyzed_frames': vqa_frames,
                    'coverage': {
                        'clip': (clip_frames / frames.count()) * 100 if frames.count() > 0 else 0,
                        'ocr': (ocr_frames / frames.count()) * 100 if frames.count() > 0 else 0,
                        'vqa': (vqa_frames / frames.count()) * 100 if frames.count() > 0 else 0
                    }
                }
            
            return Response(results)
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'Î∂ÑÏÑù Í≤∞Í≥º Ï°∞Ìöå Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class AnalysisSummaryView(APIView):
    """Î∂ÑÏÑù Í≤∞Í≥º ÏöîÏïΩ Ï†úÍ≥µ"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        self.llm_client = LLMClient()
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            if not video.is_analyzed:
                return Response({
                    'error': 'ÏïÑÏßÅ Î∂ÑÏÑùÏù¥ ÏôÑÎ£åÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # Î∂ÑÏÑù Í≤∞Í≥º Îç∞Ïù¥ÌÑ∞ ÏàòÏßë
            analysis = video.analysis
            frames = Frame.objects.filter(video=video)[:10]  # ÏÉÅÏúÑ 10Í∞ú ÌîÑÎ†àÏûÑ
            
            # AI Í∏∞Î∞ò ÏöîÏïΩ ÏÉùÏÑ±
            summary_data = {
                'video_name': video.original_name,
                'analysis_type': analysis.analysis_statistics.get('analysis_type', 'basic'),
                'features_used': analysis.analysis_statistics.get('features_used', []),
                'dominant_objects': analysis.analysis_statistics.get('dominant_objects', []),
                'scene_types': analysis.analysis_statistics.get('scene_types', []),
                'processing_time': analysis.processing_time_seconds
            }
            
            # ÎåÄÌëú ÌîÑÎ†àÏûÑÎì§Ïùò Ï∫°ÏÖò ÏàòÏßë
            sample_captions = []
            for frame in frames:
                if frame.final_caption:
                    sample_captions.append(frame.final_caption)
            
            summary_prompt = f"""
            Îã§Ïùå ÎπÑÎîîÏò§ Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú ÏÉÅÏÑ∏ÌïòÍ≥† Ïú†Ïö©Ìïú ÏöîÏïΩÏùÑ ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî:
            
            ÎπÑÎîîÏò§: {video.original_name}
            Î∂ÑÏÑù Ïú†Ìòï: {summary_data['analysis_type']}
            ÏÇ¨Ïö©Îêú Í∏∞Îä•: {', '.join(summary_data['features_used'])}
            Ï£ºÏöî Í∞ùÏ≤¥: {', '.join(summary_data['dominant_objects'][:5])}
            Ïî¨ Ïú†Ìòï: {', '.join(summary_data['scene_types'][:3])}
            
            ÎåÄÌëú Ï∫°ÏÖòÎì§:
            {chr(10).join(sample_captions[:5])}
            
            Ïù¥ ÎπÑÎîîÏò§Ïùò Ï£ºÏöî ÎÇ¥Ïö©, ÌäπÏßï, ÌôúÏö© Î∞©ÏïàÏùÑ Ìè¨Ìï®ÌïòÏó¨ ÌïúÍµ≠Ïñ¥Î°ú ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî.
            """
            
            ai_summary = self.llm_client.generate_smart_response(
                user_query=summary_prompt,
                search_results=None,
                video_info=f"ÎπÑÎîîÏò§: {video.original_name}",
                use_multi_llm=True  # Í≥†ÌíàÏßà ÏöîÏïΩÏùÑ ÏúÑÌï¥ Îã§Ï§ë LLM ÏÇ¨Ïö©
            )
            
            return Response({
                'video_id': video.id,
                'video_name': video.original_name,
                'ai_summary': ai_summary,
                'analysis_data': summary_data,
                'key_insights': {
                    'total_objects': len(summary_data['dominant_objects']),
                    'scene_variety': len(summary_data['scene_types']),
                    'analysis_depth': len(summary_data['features_used']),
                    'processing_efficiency': f"{summary_data['processing_time']}Ï¥à"
                },
                'generated_at': datetime.now().isoformat()
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'ÏöîÏïΩ ÏÉùÏÑ± Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class AnalysisExportView(APIView):
    """Î∂ÑÏÑù Í≤∞Í≥º ÎÇ¥Î≥¥ÎÇ¥Í∏∞"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            if not video.is_analyzed:
                return Response({
                    'error': 'ÏïÑÏßÅ Î∂ÑÏÑùÏù¥ ÏôÑÎ£åÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            export_format = request.GET.get('format', 'json')
            
            # Ï†ÑÏ≤¥ Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ ÏàòÏßë
            analysis = video.analysis
            scenes = Scene.objects.filter(video=video)
            frames = Frame.objects.filter(video=video)
            
            export_data = {
                'export_info': {
                    'video_id': video.id,
                    'video_name': video.original_name,
                    'export_date': datetime.now().isoformat(),
                    'export_format': export_format
                },
                'video_metadata': {
                    'filename': video.filename,
                    'duration': video.duration,
                    'file_size': video.file_size,
                    'uploaded_at': video.uploaded_at.isoformat()
                },
                'analysis_metadata': {
                    'analysis_type': analysis.analysis_statistics.get('analysis_type', 'basic'),
                    'enhanced_analysis': analysis.enhanced_analysis,
                    'success_rate': analysis.success_rate,
                    'processing_time_seconds': analysis.processing_time_seconds,
                    'features_used': analysis.analysis_statistics.get('features_used', [])
                },
                'scenes': [
                    {
                        'scene_id': scene.scene_id,
                        'start_time': scene.start_time,
                        'end_time': scene.end_time,
                        'duration': scene.duration,
                        'frame_count': scene.frame_count,
                        'dominant_objects': scene.dominant_objects
                    }
                    for scene in scenes
                ],
                'frames': [
                    {
                        'frame_id': frame.image_id,
                        'timestamp': frame.timestamp,
                        'caption': frame.caption,
                        'enhanced_caption': frame.enhanced_caption,
                        'final_caption': frame.final_caption,
                        'detected_objects': frame.detected_objects,
                        'comprehensive_features': frame.comprehensive_features
                    }
                    for frame in frames
                ],
                'statistics': {
                    'total_scenes': scenes.count(),
                    'total_frames': frames.count(),
                    'unique_objects': analysis.analysis_statistics.get('unique_objects', 0),
                    'scene_types': analysis.analysis_statistics.get('scene_types', []),
                    'dominant_objects': analysis.analysis_statistics.get('dominant_objects', [])
                }
            }
            
            if export_format == 'json':
                response = JsonResponse(export_data, json_dumps_params={'ensure_ascii': False, 'indent': 2})
                response['Content-Disposition'] = f'attachment; filename="{video.original_name}_analysis.json"'
                return response
            
            elif export_format == 'csv':
                # CSV ÌòïÌÉúÎ°ú ÌîÑÎ†àÏûÑ Îç∞Ïù¥ÌÑ∞ ÎÇ¥Î≥¥ÎÇ¥Í∏∞
                import csv
                from io import StringIO
                
                output = StringIO()
                writer = csv.writer(output)
                
                # Ìó§Îçî
                writer.writerow(['frame_id', 'timestamp', 'caption', 'enhanced_caption', 'objects_count', 'scene_complexity'])
                
                # Îç∞Ïù¥ÌÑ∞
                for frame_data in export_data['frames']:
                    writer.writerow([
                        frame_data['frame_id'],
                        frame_data['timestamp'],
                        frame_data.get('caption', ''),
                        frame_data.get('enhanced_caption', ''),
                        len(frame_data.get('detected_objects', [])),
                        frame_data.get('comprehensive_features', {}).get('scene_complexity', 0)
                    ])
                
                response = HttpResponse(output.getvalue(), content_type='text/csv')
                response['Content-Disposition'] = f'attachment; filename="{video.original_name}_analysis.csv"'
                return response
            
            else:
                return Response({
                    'error': 'ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÎÇ¥Î≥¥ÎÇ¥Í∏∞ ÌòïÏãùÏûÖÎãàÎã§. json ÎòêÎäî csvÎ•º ÏÇ¨Ïö©Ìï¥Ï£ºÏÑ∏Ïöî.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'ÎÇ¥Î≥¥ÎÇ¥Í∏∞ Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


# Í≤ÄÏÉâ Í¥ÄÎ†® Î∑∞Îì§
class ObjectSearchView(APIView):
    """Í∞ùÏ≤¥Î≥Ñ Í≤ÄÏÉâ"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            object_type = request.GET.get('object', '')
            video_id = request.GET.get('video_id')
            
            if not object_type:
                return Response({
                    'error': 'Í≤ÄÏÉâÌï† Í∞ùÏ≤¥ ÌÉÄÏûÖÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # ÌäπÏ†ï ÎπÑÎîîÏò§ ÎòêÎäî Ï†ÑÏ≤¥ ÎπÑÎîîÏò§ÏóêÏÑú Í≤ÄÏÉâ
            if video_id:
                videos = Video.objects.filter(id=video_id, is_analyzed=True)
            else:
                videos = Video.objects.filter(is_analyzed=True)
            
            results = []
            for video in videos:
                frames = Frame.objects.filter(video=video)
                
                for frame in frames:
                    for obj in frame.detected_objects:
                        if object_type.lower() in obj.get('class', '').lower():
                            results.append({
                                'video_id': video.id,
                                'video_name': video.original_name,
                                'frame_id': frame.image_id,
                                'timestamp': frame.timestamp,
                                'object_class': obj.get('class'),
                                'confidence': obj.get('confidence'),
                                'caption': frame.final_caption or frame.caption
                            })
            
            return Response({
                'search_query': object_type,
                'results': results[:50],  # ÏµúÎåÄ 50Í∞ú Í≤∞Í≥º
                'total_matches': len(results)
            })
            
        except Exception as e:
            return Response({
                'error': f'Í∞ùÏ≤¥ Í≤ÄÏÉâ Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class TextSearchView(APIView):
    """ÌÖçÏä§Ìä∏ Í≤ÄÏÉâ (OCR Í≤∞Í≥º Í∏∞Î∞ò)"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            search_text = request.GET.get('text', '')
            video_id = request.GET.get('video_id')
            
            if not search_text:
                return Response({
                    'error': 'Í≤ÄÏÉâÌï† ÌÖçÏä§Ìä∏Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # ÌäπÏ†ï ÎπÑÎîîÏò§ ÎòêÎäî Ï†ÑÏ≤¥ ÎπÑÎîîÏò§ÏóêÏÑú Í≤ÄÏÉâ
            if video_id:
                videos = Video.objects.filter(id=video_id, is_analyzed=True)
            else:
                videos = Video.objects.filter(is_analyzed=True)
            
            results = []
            for video in videos:
                frames = Frame.objects.filter(video=video)
                
                for frame in frames:
                    ocr_data = frame.comprehensive_features.get('ocr_text', {})
                    if 'full_text' in ocr_data and search_text.lower() in ocr_data['full_text'].lower():
                        results.append({
                            'video_id': video.id,
                            'video_name': video.original_name,
                            'frame_id': frame.image_id,
                            'timestamp': frame.timestamp,
                            'extracted_text': ocr_data['full_text'],
                            'text_details': ocr_data.get('texts', []),
                            'caption': frame.final_caption or frame.caption
                        })
            
            return Response({
                'search_query': search_text,
                'results': results[:50],
                'total_matches': len(results)
            })
            
        except Exception as e:
            return Response({
                'error': f'ÌÖçÏä§Ìä∏ Í≤ÄÏÉâ Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class SceneSearchView(APIView):
    """Ïî¨ ÌÉÄÏûÖÎ≥Ñ Í≤ÄÏÉâ"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            scene_type = request.GET.get('scene', '')
            video_id = request.GET.get('video_id')
            
            if not scene_type:
                return Response({
                    'error': 'Í≤ÄÏÉâÌï† Ïî¨ ÌÉÄÏûÖÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # ÌäπÏ†ï ÎπÑÎîîÏò§ ÎòêÎäî Ï†ÑÏ≤¥ ÎπÑÎîîÏò§ÏóêÏÑú Í≤ÄÏÉâ
            if video_id:
                videos = Video.objects.filter(id=video_id, is_analyzed=True)
            else:
                videos = Video.objects.filter(is_analyzed=True)
            
            results = []
            for video in videos:
                if hasattr(video, 'analysis'):
                    scene_types = video.analysis.analysis_statistics.get('scene_types', [])
                    if any(scene_type.lower() in st.lower() for st in scene_types):
                        results.append({
                            'video_id': video.id,
                            'video_name': video.original_name,
                            'scene_types': scene_types,
                            'analysis_type': video.analysis.analysis_statistics.get('analysis_type', 'basic'),
                            'dominant_objects': video.analysis.analysis_statistics.get('dominant_objects', [])
                        })
            
            return Response({
                'search_query': scene_type,
                'results': results,
                'total_matches': len(results)
            })
            
        except Exception as e:
            return Response({
                'error': f'Ïî¨ Í≤ÄÏÉâ Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.shortcuts import get_object_or_404
from django.db import transaction
import json
import logging
import os
import time

logger = logging.getLogger(__name__)

@csrf_exempt
@require_http_methods(["DELETE"])
def delete_video(request, video_id):
    """Í∞úÏÑ†Îêú ÎπÑÎîîÏò§ ÏÇ≠Ï†ú - ÏÉÅÏÑ∏ Î°úÍπÖ Î∞è Í≤ÄÏ¶ù Ìè¨Ìï®"""
    
    logger.info(f"üóëÔ∏è ÎπÑÎîîÏò§ ÏÇ≠Ï†ú ÏöîÏ≤≠ ÏãúÏûë: ID={video_id}")
    
    try:
        # 1Îã®Í≥Ñ: ÎπÑÎîîÏò§ Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
        try:
            video = get_object_or_404(Video, id=video_id)
            logger.info(f"‚úÖ ÎπÑÎîîÏò§ Ï∞æÏùå: {video.original_name} (ÌååÏùº: {video.file_path})")
        except Video.DoesNotExist:
            logger.warning(f"‚ùå ÎπÑÎîîÏò§ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå: ID={video_id}")
            return JsonResponse({
                'error': 'Ìï¥Îãπ ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.',
                'video_id': video_id,
                'deleted': False
            }, status=404)
        
        # 2Îã®Í≥Ñ: ÏÇ≠Ï†ú Í∞ÄÎä• Ïó¨Î∂Ä ÌôïÏù∏
        if video.analysis_status == 'processing':
            logger.warning(f"‚ùå Î∂ÑÏÑù Ï§ëÏù∏ ÎπÑÎîîÏò§ ÏÇ≠Ï†ú ÏãúÎèÑ: ID={video_id}")
            return JsonResponse({
                'error': 'Î∂ÑÏÑù Ï§ëÏù∏ ÎπÑÎîîÏò§Îäî ÏÇ≠Ï†úÌï† Ïàò ÏóÜÏäµÎãàÎã§.',
                'video_id': video_id,
                'status': video.analysis_status,
                'deleted': False
            }, status=400)
        
        # 3Îã®Í≥Ñ: Ìä∏ÎûúÏû≠ÏÖòÏúºÎ°ú ÏïàÏ†ÑÌïú ÏÇ≠Ï†ú Ï≤òÎ¶¨
        video_info = {
            'id': video_id,
            'name': video.original_name,
            'file_path': video.file_path,
            'has_analysis': hasattr(video, 'analysis_results') and video.analysis_results.exists(),
            'has_scenes': hasattr(video, 'scenes') and video.scenes.exists()
        }
        
        with transaction.atomic():
            logger.info(f"üîÑ Ìä∏ÎûúÏû≠ÏÖò ÏãúÏûë: ÎπÑÎîîÏò§ {video_id} ÏÇ≠Ï†ú")
            
            # Í¥ÄÎ†® Îç∞Ïù¥ÌÑ∞ Î®ºÏ†Ä ÏÇ≠Ï†ú
            deleted_analysis_count = 0
            deleted_scenes_count = 0
            
            if hasattr(video, 'analysis_results'):
                deleted_analysis_count = video.analysis_results.count()
                video.analysis_results.all().delete()
                logger.info(f"üìä Î∂ÑÏÑù Í≤∞Í≥º ÏÇ≠Ï†ú: {deleted_analysis_count}Í∞ú")
            
            if hasattr(video, 'scenes'):
                deleted_scenes_count = video.scenes.count()
                video.scenes.all().delete()
                logger.info(f"üé¨ Ïî¨ Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú: {deleted_scenes_count}Í∞ú")
            
            # ÌååÏùº ÏãúÏä§ÌÖúÏóêÏÑú ÌååÏùº ÏÇ≠Ï†ú
            file_deleted = False
            if video.file_path and os.path.exists(video.file_path):
                try:
                    os.remove(video.file_path)
                    file_deleted = True
                    logger.info(f"üìÅ ÌååÏùº ÏÇ≠Ï†ú ÏÑ±Í≥µ: {video.file_path}")
                except Exception as file_error:
                    logger.error(f"‚ùå ÌååÏùº ÏÇ≠Ï†ú Ïã§Ìå®: {video.file_path} - {str(file_error)}")
                    # ÌååÏùº ÏÇ≠Ï†ú Ïã§Ìå®Ìï¥ÎèÑ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑúÎäî ÏÇ≠Ï†ú ÏßÑÌñâ
                    file_deleted = False
            else:
                logger.info(f"üìÅ ÏÇ≠Ï†úÌï† ÌååÏùº ÏóÜÏùå: {video.file_path}")
                file_deleted = True  # ÌååÏùºÏù¥ ÏóÜÏúºÎ©¥ ÏÇ≠Ï†úÎêú Í≤ÉÏúºÎ°ú Í∞ÑÏ£º
            
            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú ÎπÑÎîîÏò§ Î†àÏΩîÎìú ÏÇ≠Ï†ú
            video.delete()
            logger.info(f"üíæ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú ÎπÑÎîîÏò§ ÏÇ≠Ï†ú ÏôÑÎ£å: ID={video_id}")
            
            # Ìä∏ÎûúÏû≠ÏÖò Ïª§Î∞ã ÌõÑ Ïû†Ïãú ÎåÄÍ∏∞ (Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÎèôÍ∏∞Ìôî)
            time.sleep(0.1)
        
        # 4Îã®Í≥Ñ: ÏÇ≠Ï†ú Í≤ÄÏ¶ù
        try:
            verification_video = Video.objects.get(id=video_id)
            # ÎπÑÎîîÏò§Í∞Ä Ïó¨Ï†ÑÌûà Ï°¥Ïû¨ÌïòÎ©¥ Ïò§Î•ò
            logger.error(f"‚ùå ÏÇ≠Ï†ú Í≤ÄÏ¶ù Ïã§Ìå®: ÎπÑÎîîÏò§Í∞Ä Ïó¨Ï†ÑÌûà Ï°¥Ïû¨Ìï® ID={video_id}")
            return JsonResponse({
                'error': 'ÎπÑÎîîÏò§ ÏÇ≠Ï†úÏóê Ïã§Ìå®ÌñàÏäµÎãàÎã§. Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú Ï†úÍ±∞ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.',
                'video_id': video_id,
                'deleted': False,
                'verification_failed': True
            }, status=500)
        except Video.DoesNotExist:
            # ÎπÑÎîîÏò§Í∞Ä Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏúºÎ©¥ ÏÇ≠Ï†ú ÏÑ±Í≥µ
            logger.info(f"‚úÖ ÏÇ≠Ï†ú Í≤ÄÏ¶ù ÏÑ±Í≥µ: ÎπÑÎîîÏò§Í∞Ä ÏôÑÏ†ÑÌûà Ï†úÍ±∞Îê® ID={video_id}")
        
        # 5Îã®Í≥Ñ: ÏÑ±Í≥µ ÏùëÎãµ
        response_data = {
            'success': True,
            'message': f'ÎπÑÎîîÏò§ "{video_info["name"]}"Ïù¥(Í∞Ä) ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏÇ≠Ï†úÎêòÏóàÏäµÎãàÎã§.',
            'video_id': video_id,
            'deleted': True,
            'details': {
                'file_deleted': file_deleted,
                'analysis_results_deleted': deleted_analysis_count,
                'scenes_deleted': deleted_scenes_count,
                'file_path': video_info['file_path']
            }
        }
        
        logger.info(f"‚úÖ ÎπÑÎîîÏò§ ÏÇ≠Ï†ú ÏôÑÎ£å: {json.dumps(response_data, ensure_ascii=False)}")
        return JsonResponse(response_data)
        
    except Exception as e:
        logger.error(f"‚ùå ÎπÑÎîîÏò§ ÏÇ≠Ï†ú Ï§ë ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•ò: ID={video_id}, Ïò§Î•ò={str(e)}")
        return JsonResponse({
            'error': f'ÎπÑÎîîÏò§ ÏÇ≠Ï†ú Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}',
            'video_id': video_id,
            'deleted': False,
            'exception': str(e)
        }, status=500)

@csrf_exempt
@require_http_methods(["GET"])  
def video_detail(request, video_id):
    """ÎπÑÎîîÏò§ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï°∞Ìöå (Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏Ïö©)"""
    try:
        video = get_object_or_404(Video, id=video_id)
        return JsonResponse({
            'id': video.id,
            'original_name': video.original_name,
            'analysis_status': video.analysis_status,
            'exists': True
        })
    except Video.DoesNotExist:
        return JsonResponse({
            'error': 'Ìï¥Îãπ ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.',
            'video_id': video_id,
            'exists': False
        }, status=404)

# ÏÇ≠Ï†ú ÏÉÅÌÉú ÌôïÏù∏ÏùÑ ÏúÑÌïú Î≥ÑÎèÑ ÏóîÎìúÌè¨Ïù∏Ìä∏
@csrf_exempt
@require_http_methods(["GET"])
def check_video_exists(request, video_id):
    """ÎπÑÎîîÏò§ Ï°¥Ïû¨ Ïó¨Î∂ÄÎßå ÌôïÏù∏"""
    try:
        Video.objects.get(id=video_id)
        return JsonResponse({
            'exists': True,
            'video_id': video_id
        })
    except Video.DoesNotExist:
        return JsonResponse({
            'exists': False,
            'video_id': video_id
        })

# views.pyÏóê Ï∂îÍ∞ÄÌï† Î∞îÏö¥Îî© Î∞ïÏä§ Í∑∏Î¶¨Í∏∞ View ÌÅ¥ÎûòÏä§Îì§

class AdvancedVideoSearchView(APIView):
    """Í≥†Í∏â ÎπÑÎîîÏò§ Í≤ÄÏÉâ View - Î∞îÏö¥Îî© Î∞ïÏä§ Ï†ïÎ≥¥ Ìè¨Ìï®"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        self.video_analyzer = get_video_analyzer()
        self.llm_client = LLMClient()
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            query = request.data.get('query', '').strip()
            search_options = request.data.get('search_options', {})
            
            print(f"üîç Í≥†Í∏â ÎπÑÎîîÏò§ Í≤ÄÏÉâ: ÎπÑÎîîÏò§={video_id}, ÏøºÎ¶¨='{query}'")
            
            if not query:
                return Response({
                    'error': 'Í≤ÄÏÉâÏñ¥Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({
                    'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
                }, status=status.HTTP_404_NOT_FOUND)
            
            # Í≥†Í∏â Í≤ÄÏÉâ ÏàòÌñâ
            search_results = self._perform_advanced_search(video, query, search_options)
            
            # Î∞îÏö¥Îî© Î∞ïÏä§ Ï†ïÎ≥¥ Ï∂îÍ∞Ä
            enhanced_results = self._add_bbox_info(search_results, video)
            
            # AI Í∏∞Î∞ò Í≤ÄÏÉâ Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ±
            search_insights = self._generate_search_insights(query, enhanced_results, video)
            
            print(f"‚úÖ Í≥†Í∏â Í≤ÄÏÉâ ÏôÑÎ£å: {len(enhanced_results)}Í∞ú Í≤∞Í≥º")
            
            return Response({
                'search_results': enhanced_results,
                'query': query,
                'insights': search_insights,
                'total_matches': len(search_results),
                'search_type': 'advanced_search',
                'has_bbox_annotations': any(r.get('bbox_annotations') for r in enhanced_results),
                'video_info': {
                    'id': video.id,
                    'name': video.original_name,
                    'analysis_type': getattr(video, 'analysis_type', 'basic')
                }
            })
            
        except Exception as e:
            print(f"‚ùå Í≥†Í∏â ÎπÑÎîîÏò§ Í≤ÄÏÉâ Ïã§Ìå®: {e}")
            return Response({
                'error': f'Í≥†Í∏â Í≤ÄÏÉâ Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _perform_advanced_search(self, video, query, search_options):
        """Ïã§Ï†ú Í≥†Í∏â Í≤ÄÏÉâ ÏàòÌñâ"""
        try:
            # EnhancedVideoChatViewÏùò Í≤ÄÏÉâ Î°úÏßÅ Ïû¨ÏÇ¨Ïö©
            chat_view = EnhancedVideoChatView()
            video_info = chat_view._get_enhanced_video_info(video)
            
            # Í≤ÄÏÉâ ÏàòÌñâ
            response = chat_view._handle_enhanced_search(query, video, video_info)
            
            if hasattr(response, 'data') and 'search_results' in response.data:
                return response.data['search_results']
            else:
                return []
                
        except Exception as e:
            print(f"‚ùå Í≥†Í∏â Í≤ÄÏÉâ ÏàòÌñâ Ïò§Î•ò: {e}")
            return []
    
    def _add_bbox_info(self, search_results, video):
        """Í≤ÄÏÉâ Í≤∞Í≥ºÏóê Î∞îÏö¥Îî© Î∞ïÏä§ Ï†ïÎ≥¥ Ï∂îÍ∞Ä"""
        enhanced_results = []
        
        for result in search_results:
            enhanced_result = dict(result)
            
            # Î∞îÏö¥Îî© Î∞ïÏä§ Ïñ¥ÎÖ∏ÌÖåÏù¥ÏÖò Ï†ïÎ≥¥ ÌôïÏù∏ Î∞è Ï∂îÍ∞Ä
            if 'matches' in result:
                bbox_annotations = []
                for match in result['matches']:
                    if match.get('type') == 'object' and 'bbox' in match:
                        bbox_annotations.append({
                            'match': match['match'],
                            'confidence': match['confidence'],
                            'bbox': match['bbox'],
                            'colors': match.get('colors', []),
                            'color_description': match.get('color_description', '')
                        })
                
                enhanced_result['bbox_annotations'] = bbox_annotations
                
                # Î∞îÏö¥Îî© Î∞ïÏä§ Ïù¥ÎØ∏ÏßÄ URL Ï∂îÍ∞Ä
                if bbox_annotations:
                    bbox_url = f"/frame/{video.id}/{result['frame_id']}/bbox/"
                    enhanced_result['bbox_image_url'] = bbox_url
            
            enhanced_results.append(enhanced_result)
        
        return enhanced_results
    
    def _generate_search_insights(self, query, results, video):
        """Í≤ÄÏÉâ Í≤∞Í≥ºÏóê ÎåÄÌïú AI Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ±"""
        try:
            if not results:
                return "Í≤ÄÏÉâ Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§. Îã§Î•∏ Í≤ÄÏÉâÏñ¥Î•º ÏãúÎèÑÌï¥Î≥¥ÏÑ∏Ïöî."
            
            bbox_count = sum(1 for r in results if r.get('bbox_annotations'))
            total_objects = sum(len(r.get('bbox_annotations', [])) for r in results)
            
            insights_prompt = f"""
            Í≤ÄÏÉâÏñ¥: "{query}"
            ÎπÑÎîîÏò§: {video.original_name}
            Í≤ÄÏÉâ Í≤∞Í≥º: {len(results)}Í∞ú Îß§Ïπ≠
            Î∞îÏö¥Îî© Î∞ïÏä§ ÌëúÏãú Í∞ÄÎä•: {bbox_count}Í∞ú ÌîÑÎ†àÏûÑ
            Ï¥ù Í∞êÏßÄÎêú Í∞ùÏ≤¥: {total_objects}Í∞ú
            
            Ï£ºÏöî Î∞úÍ≤¨ÏÇ¨Ìï≠ÏùÑ Î∞îÌÉïÏúºÎ°ú Í∞ÑÎã®ÌïòÍ≥† Ïú†Ïö©Ìïú Ïù∏ÏÇ¨Ïù¥Ìä∏Î•º ÌïúÍµ≠Ïñ¥Î°ú Ï†úÍ≥µÌï¥Ï£ºÏÑ∏Ïöî.
            Î∞îÏö¥Îî© Î∞ïÏä§ ÌëúÏãú Í∏∞Îä•Ïóê ÎåÄÌïú ÏïàÎÇ¥ÎèÑ Ìè¨Ìï®Ìï¥Ï£ºÏÑ∏Ïöî.
            """
            
            insights = self.llm_client.generate_smart_response(
                user_query=insights_prompt,
                search_results=results[:3],
                video_info=f"ÎπÑÎîîÏò§: {video.original_name}",
                use_multi_llm=False
            )
            
            return insights
            
        except Exception as e:
            return f"Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ± Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"


# Í∏∞Ï°¥ FrameView ÌÅ¥ÎûòÏä§Ïóê Î∞îÏö¥Îî© Î∞ïÏä§ ÏòµÏÖò Ï∂îÍ∞Ä
class EnhancedFrameView(FrameView):
    """Í∏∞Ï°¥ FrameViewÎ•º ÌôïÏû•Ìïú Í≥†Í∏â ÌîÑÎ†àÏûÑ View"""
    
    def get(self, request, video_id, frame_number):
        try:
            # Î∞îÏö¥Îî© Î∞ïÏä§ ÌëúÏãú ÏòµÏÖò ÌôïÏù∏
            show_bbox = request.GET.get('bbox', '').lower() in ['true', '1', 'yes']
            
            if show_bbox:
                # Î∞îÏö¥Îî© Î∞ïÏä§Í∞Ä Ìè¨Ìï®Îêú Ïù¥ÎØ∏ÏßÄ Î∞òÌôò
                bbox_view = FrameWithBboxView()
                return bbox_view.get(request, video_id, frame_number)
            else:
                # Í∏∞Î≥∏ ÌîÑÎ†àÏûÑ Î∞òÌôò
                return super().get(request, video_id, frame_number)
                
        except Exception as e:
            print(f"‚ùå Í≥†Í∏â ÌîÑÎ†àÏûÑ Î∑∞ Ïò§Î•ò: {e}")
            return super().get(request, video_id, frame_number)

# chat/views.pyÏóê Îã§Ïùå ÌÅ¥ÎûòÏä§Î•º Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî

class AnalysisCapabilitiesView(APIView):
    """ÏãúÏä§ÌÖú Î∂ÑÏÑù Í∏∞Îä• ÏÉÅÌÉú ÌôïÏù∏"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            print("üîç AnalysisCapabilitiesView: Î∂ÑÏÑù Í∏∞Îä• ÏÉÅÌÉú ÏöîÏ≤≠")
            
            # VideoAnalyzer Ïù∏Ïä§ÌÑ¥Ïä§ Í∞ÄÏ†∏Ïò§Í∏∞
            try:
                analyzer = get_video_analyzer()
                analyzer_available = True
                print("‚úÖ VideoAnalyzer Ïù∏Ïä§ÌÑ¥Ïä§ Î°úÎî© ÏÑ±Í≥µ")
            except Exception as e:
                print(f"‚ö†Ô∏è VideoAnalyzer Î°úÎî© Ïã§Ìå®: {e}")
                analyzer = None
                analyzer_available = False
            
            # ÏãúÏä§ÌÖú Í∏∞Îä• ÏÉÅÌÉú ÌôïÏù∏
            capabilities = {
                'system_status': {
                    'analyzer_available': analyzer_available,
                    'device': getattr(analyzer, 'device', 'unknown') if analyzer else 'none',
                    'timestamp': datetime.now().isoformat()
                },
                'core_features': {
                    'object_detection': {
                        'name': 'Í∞ùÏ≤¥ Í∞êÏßÄ',
                        'available': analyzer.model is not None if analyzer else False,
                        'description': 'YOLO Í∏∞Î∞ò Ïã§ÏãúÍ∞Ñ Í∞ùÏ≤¥ Í∞êÏßÄ',
                        'icon': 'üéØ'
                    },
                    'enhanced_captions': {
                        'name': 'Í≥†Í∏â Ï∫°ÏÖò ÏÉùÏÑ±',
                        'available': True,
                        'description': 'AI Í∏∞Î∞ò ÏÉÅÏÑ∏ Ï∫°ÏÖò ÏÉùÏÑ±',
                        'icon': 'üí¨'
                    }
                },
                'advanced_features': {
                    'clip_analysis': {
                        'name': 'CLIP Î∂ÑÏÑù',
                        'available': getattr(analyzer, 'clip_available', False) if analyzer else False,
                        'description': 'OpenAI CLIP Î™®Îç∏ Í∏∞Î∞ò Ïî¨ Ïù¥Ìï¥',
                        'icon': 'üñºÔ∏è'
                    },
                    'ocr_text_extraction': {
                        'name': 'OCR ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú',
                        'available': getattr(analyzer, 'ocr_available', False) if analyzer else False,  
                        'description': 'EasyOCR Í∏∞Î∞ò Îã§Íµ≠Ïñ¥ ÌÖçÏä§Ìä∏ Ïù∏Ïãù',
                        'icon': 'üìù'
                    },
                    'vqa_analysis': {
                        'name': 'VQA ÏßàÎ¨∏ÎãµÎ≥Ä',
                        'available': getattr(analyzer, 'vqa_available', False) if analyzer else False,
                        'description': 'BLIP Î™®Îç∏ Í∏∞Î∞ò ÏãúÍ∞ÅÏ†Å ÏßàÎ¨∏ ÎãµÎ≥Ä',
                        'icon': '‚ùì'
                    },
                    'scene_graph': {
                        'name': 'Scene Graph',
                        'available': getattr(analyzer, 'scene_graph_available', False) if analyzer else False,
                        'description': 'NetworkX Í∏∞Î∞ò Í∞ùÏ≤¥ Í¥ÄÍ≥Ñ Î∂ÑÏÑù',
                        'icon': 'üï∏Ô∏è'
                    }
                },
                'api_status': {
                    'groq_available': True,  # LLMClientÏóêÏÑú ÌôïÏù∏ ÌïÑÏöî
                    'openai_available': True,
                    'anthropic_available': True
                }
            }
            
            # ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Í∏∞Îä• Ïàò Í≥ÑÏÇ∞
            total_features = len(capabilities['core_features']) + len(capabilities['advanced_features'])
            available_features = sum(1 for features in [capabilities['core_features'], capabilities['advanced_features']] 
                                   for feature in features.values() if feature.get('available', False))
            
            capabilities['summary'] = {
                'total_features': total_features,
                'available_features': available_features,
                'availability_rate': (available_features / total_features * 100) if total_features > 0 else 0,
                'system_ready': analyzer_available and available_features > 0
            }
            
            print(f"‚úÖ Î∂ÑÏÑù Í∏∞Îä• ÏÉÅÌÉú: {available_features}/{total_features} ÏÇ¨Ïö© Í∞ÄÎä•")
            
            return Response(capabilities)
            
        except Exception as e:
            print(f"‚ùå AnalysisCapabilitiesView Ïò§Î•ò: {e}")
            import traceback
            print(f"üîç ÏÉÅÏÑ∏ Ïò§Î•ò: {traceback.format_exc()}")
            
            # Ïò§Î•ò Î∞úÏÉùÏãú Í∏∞Î≥∏ ÏÉÅÌÉú Î∞òÌôò
            error_response = {
                'system_status': {
                    'analyzer_available': False,
                    'device': 'error',
                    'error': str(e)
                },
                'core_features': {},
                'advanced_features': {},
                'api_status': {},
                'summary': {
                    'total_features': 0,
                    'available_features': 0,
                    'availability_rate': 0,
                    'system_ready': False,
                    'error': str(e)
                }
            }
            
            return Response(error_response, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# views.pyÏóê Ï∂îÍ∞ÄÌï† Í≥†Í∏â Í≤ÄÏÉâ API ÌÅ¥ÎûòÏä§Îì§

class CrossVideoSearchView(APIView):
    """ÏòÅÏÉÅ Í∞Ñ Í≤ÄÏÉâ - Ïó¨Îü¨ ÎπÑÎîîÏò§ÏóêÏÑú Ï°∞Í±¥ Í≤ÄÏÉâ"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            query = request.data.get('query', '').strip()
            search_filters = request.data.get('filters', {})
            
            if not query:
                return Response({'error': 'Í≤ÄÏÉâÏñ¥Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'}, status=400)
            
            # ÏøºÎ¶¨ Î∂ÑÏÑù - ÎÇ†Ïî®, ÏãúÍ∞ÑÎåÄ, Ïû•ÏÜå Îì± Ï∂îÏ∂ú
            query_analysis = self._analyze_query(query)
            
            # Î∂ÑÏÑùÎêú ÎπÑÎîîÏò§Îì§ Ï§ëÏóêÏÑú Í≤ÄÏÉâ
            videos = Video.objects.filter(is_analyzed=True)
            matching_videos = []
            
            for video in videos:
                match_score = self._calculate_video_match_score(video, query_analysis, search_filters)
                if match_score > 0.3:  # ÏûÑÍ≥ÑÍ∞í
                    matching_videos.append({
                        'video_id': video.id,
                        'video_name': video.original_name,
                        'match_score': match_score,
                        'match_reasons': self._get_match_reasons(video, query_analysis),
                        'metadata': self._get_video_metadata(video),
                        'thumbnail_url': f'/api/frame/{video.id}/100/',
                    })
            
            # Ï†êÏàòÏàú Ï†ïÎ†¨
            matching_videos.sort(key=lambda x: x['match_score'], reverse=True)
            
            return Response({
                'query': query,
                'total_matches': len(matching_videos),
                'results': matching_videos[:20],  # ÏÉÅÏúÑ 20Í∞ú
                'query_analysis': query_analysis,
                'search_type': 'cross_video'
            })
            
        except Exception as e:
            return Response({'error': str(e)}, status=500)
    
    def _analyze_query(self, query):
        """ÏøºÎ¶¨ÏóêÏÑú ÎÇ†Ïî®, ÏãúÍ∞ÑÎåÄ, Ïû•ÏÜå Îì± Ï∂îÏ∂ú"""
        analysis = {
            'weather': None,
            'time_of_day': None,
            'location': None,
            'objects': [],
            'activities': []
        }
        
        query_lower = query.lower()
        
        # ÎÇ†Ïî® ÌÇ§ÏõåÎìú
        weather_keywords = {
            'ÎπÑ': 'rainy', 'ÎπÑÍ∞Ä': 'rainy', 'Ïö∞Ï≤ú': 'rainy',
            'ÎßëÏùÄ': 'sunny', 'ÌôîÏ∞ΩÌïú': 'sunny', 'ÌñáÎπõ': 'sunny',
            'ÌùêÎ¶∞': 'cloudy', 'Íµ¨Î¶Ñ': 'cloudy'
        }
        
        # ÏãúÍ∞ÑÎåÄ ÌÇ§ÏõåÎìú
        time_keywords = {
            'Î∞§': 'night', 'ÏïºÍ∞Ñ': 'night', 'Ï†ÄÎÖÅ': 'evening',
            'ÎÇÆ': 'day', 'Ïò§ÌõÑ': 'afternoon', 'ÏïÑÏπ®': 'morning'
        }
        
        # Ïû•ÏÜå ÌÇ§ÏõåÎìú
        location_keywords = {
            'Ïã§ÎÇ¥': 'indoor', 'Í±¥Î¨º': 'indoor', 'Î∞©': 'indoor',
            'Ïã§Ïô∏': 'outdoor', 'ÎèÑÎ°ú': 'outdoor', 'Í±∞Î¶¨': 'outdoor'
        }
        
        for keyword, value in weather_keywords.items():
            if keyword in query_lower:
                analysis['weather'] = value
                break
        
        for keyword, value in time_keywords.items():
            if keyword in query_lower:
                analysis['time_of_day'] = value
                break
                
        for keyword, value in location_keywords.items():
            if keyword in query_lower:
                analysis['location'] = value
                break
        
        return analysis
    
    def _calculate_video_match_score(self, video, query_analysis, filters):
        """ÎπÑÎîîÏò§ÏôÄ ÏøºÎ¶¨ Í∞ÑÏùò Îß§Ïπ≠ Ï†êÏàò Í≥ÑÏÇ∞"""
        score = 0.0
        
        try:
            # Î∂ÑÏÑù Í≤∞Í≥ºÍ∞Ä ÏûàÎäî Í≤ΩÏö∞
            if hasattr(video, 'analysis'):
                stats = video.analysis.analysis_statistics
                scene_types = stats.get('scene_types', [])
                
                # ÎÇ†Ïî® Îß§Ïπ≠
                if query_analysis['weather']:
                    weather_scenes = [s for s in scene_types if query_analysis['weather'] in s.lower()]
                    if weather_scenes:
                        score += 0.4
                
                # ÏãúÍ∞ÑÎåÄ Îß§Ïπ≠
                if query_analysis['time_of_day']:
                    time_scenes = [s for s in scene_types if query_analysis['time_of_day'] in s.lower()]
                    if time_scenes:
                        score += 0.3
                
                # Ïû•ÏÜå Îß§Ïπ≠
                if query_analysis['location']:
                    location_scenes = [s for s in scene_types if query_analysis['location'] in s.lower()]
                    if location_scenes:
                        score += 0.3
            
            return min(score, 1.0)
            
        except Exception:
            return 0.0
    
    def _get_match_reasons(self, video, query_analysis):
        """Îß§Ïπ≠ Ïù¥Ïú† ÏÉùÏÑ±"""
        reasons = []
        
        if query_analysis['weather']:
            reasons.append(f"{query_analysis['weather']} ÎÇ†Ïî® Ï°∞Í±¥")
        if query_analysis['time_of_day']:
            reasons.append(f"{query_analysis['time_of_day']} ÏãúÍ∞ÑÎåÄ")
        if query_analysis['location']:
            reasons.append(f"{query_analysis['location']} ÌôòÍ≤Ω")
            
        return reasons
    
    def _get_video_metadata(self, video):
        """ÎπÑÎîîÏò§ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î∞òÌôò"""
        metadata = {
            'duration': video.duration,
            'file_size': video.file_size,
            'uploaded_at': video.uploaded_at.isoformat(),
            'analysis_type': 'basic'
        }
        
        if hasattr(video, 'analysis'):
            stats = video.analysis.analysis_statistics
            metadata.update({
                'analysis_type': stats.get('analysis_type', 'basic'),
                'scene_types': stats.get('scene_types', []),
                'dominant_objects': stats.get('dominant_objects', [])
            })
        
        return metadata

# views.py - Í≥†Í∏â Í≤ÄÏÉâ Í¥ÄÎ†® Î∑∞ ÏàòÏ†ïÎêú Î≤ÑÏ†Ñ
# views.py - IntraVideoTrackingView Ìñ•ÏÉÅÎêú Î≤ÑÏ†Ñ (ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞ ÏßÄÏõê)

@method_decorator(csrf_exempt, name='dispatch')
class IntraVideoTrackingView(APIView):
    """ÏòÅÏÉÅ ÎÇ¥ Í∞ùÏ≤¥ Ï∂îÏ†Å - Ìñ•ÏÉÅÎêú Î≤ÑÏ†Ñ (ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞ ÏßÄÏõê)"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            tracking_target = request.data.get('tracking_target', '').strip()
            time_range = request.data.get('time_range', {})
            
            logger.info(f"üéØ Í∞ùÏ≤¥ Ï∂îÏ†Å ÏöîÏ≤≠: ÎπÑÎîîÏò§={video_id}, ÎåÄÏÉÅ='{tracking_target}', ÏãúÍ∞ÑÎ≤îÏúÑ={time_range}")
            
            if not video_id or not tracking_target:
                return Response({'error': 'ÎπÑÎîîÏò§ IDÏôÄ Ï∂îÏ†Å ÎåÄÏÉÅÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.'}, status=400)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'}, status=404)
            
            # Frame Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏ Î∞è ÏÉùÏÑ±
            self._ensure_frame_data(video)
            
            # ÌÉÄÍ≤ü Î∂ÑÏÑù (ÏÉâÏÉÅ, Í∞ùÏ≤¥ ÌÉÄÏûÖ Îì± Ï∂îÏ∂ú)
            target_analysis = self._analyze_tracking_target(tracking_target)
            logger.info(f"üìã ÌÉÄÍ≤ü Î∂ÑÏÑù Í≤∞Í≥º: {target_analysis}")
            
            # ÌîÑÎ†àÏûÑÎ≥Ñ Ï∂îÏ†Å Í≤∞Í≥º
            tracking_results = self._perform_object_tracking(video, target_analysis, time_range)
            
            logger.info(f"‚úÖ Í∞ùÏ≤¥ Ï∂îÏ†Å ÏôÑÎ£å: {len(tracking_results)}Í∞ú Í≤∞Í≥º")
            
            # Í≤∞Í≥ºÍ∞Ä ÏóÜÏúºÎ©¥ Îçî Í¥ÄÎåÄÌïú Í≤ÄÏÉâ ÏàòÌñâ
            if not tracking_results:
                logger.info("üîÑ Í¥ÄÎåÄÌïú Í≤ÄÏÉâ Î™®ÎìúÎ°ú Ïû¨ÏãúÎèÑ...")
                tracking_results = self._perform_lenient_tracking(video, target_analysis, time_range)
            
            return Response({
                'video_id': video_id,
                'tracking_target': tracking_target,
                'target_analysis': target_analysis,
                'tracking_results': tracking_results,
                'total_detections': len(tracking_results),
                'search_type': 'object_tracking'
            })
            
        except Exception as e:
            logger.error(f"‚ùå Í∞ùÏ≤¥ Ï∂îÏ†Å Ïò§Î•ò: {e}")
            import traceback
            logger.error(f"üîç ÏÉÅÏÑ∏ Ïò§Î•ò: {traceback.format_exc()}")
            return Response({'error': str(e)}, status=500)
    
    def _ensure_frame_data(self, video):
        """Frame Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏ Î∞è ÏÉùÏÑ±"""
        try:
            frame_count = video.frames.count()
            if frame_count == 0:
                logger.warning(f"‚ö†Ô∏è ÎπÑÎîîÏò§ {video.original_name}Ïóê Frame Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§. ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.")
                from .models import create_dummy_frame_data
                create_dummy_frame_data(video, frame_count=30)
                logger.info(f"‚úÖ ÎçîÎØ∏ Frame Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± ÏôÑÎ£å: 30Í∞ú")
                return True
            else:
                logger.info(f"üìä Í∏∞Ï°¥ Frame Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏: {frame_count}Í∞ú")
                return False
        except Exception as e:
            logger.error(f"‚ùå Frame Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏ Ïã§Ìå®: {e}")
            return False
    
    def _analyze_tracking_target(self, target):
        """Ï∂îÏ†Å ÎåÄÏÉÅ Î∂ÑÏÑù - Ìñ•ÏÉÅÎêú Î≤ÑÏ†Ñ"""
        analysis = {
            'object_type': None,
            'colors': [],
            'gender': None,
            'clothing': [],
            'keywords': target.lower().split(),
            'original_target': target
        }
        
        target_lower = target.lower()
        
        # Í∞ùÏ≤¥ ÌÉÄÏûÖ Îß§Ìïë ÌôïÏû•
        object_mappings = {
            ('ÏÇ¨Îûå', 'ÎÇ®ÏÑ±', 'Ïó¨ÏÑ±', 'Ïù∏Î¨º'): 'person',
            ('Í∞ÄÎ∞©', 'handbag'): 'handbag',  # Ï∂îÍ∞Ä!
            ('tv', 'Ìã∞ÎπÑ', 'ÌÖîÎ†àÎπÑÏ†Ñ'): 'tv',
            ('ÏùòÏûê', 'chair'): 'chair',
            ('Ï∞®', 'ÏûêÎèôÏ∞®', 'Ï∞®Îüâ', 'ÏäπÏö©Ï∞®'): 'car',
            ('ÏûêÏ†ÑÍ±∞', 'bicycle'): 'bicycle',
            ('Í∞ú', 'Í∞ïÏïÑÏßÄ', 'Î©çÎ©çÏù¥'): 'dog',
            ('Í≥†ÏñëÏù¥', 'ÎÉ•Ïù¥'): 'cat',
            ('ÎÖ∏Ìä∏Î∂Å', 'Ïª¥Ìì®ÌÑ∞', 'laptop'): 'laptop',
            ('Ìï∏ÎìúÌè∞', 'Ìú¥ÎåÄÌè∞', 'Ìè∞'): 'cell_phone'
        }
        
        
        for keywords, obj_type in object_mappings.items():
            if any(keyword in target_lower for keyword in keywords):
                analysis['object_type'] = obj_type
                break
        
        # ÏÉâÏÉÅ Ï∂îÏ∂ú ÌôïÏû•
        color_keywords = {
            'Îπ®Í∞Ñ': 'red', 'Îπ®Í∞ï': 'red', 'Ï†ÅÏÉâ': 'red',
            'Ï£ºÌô©': 'orange', 'Ïò§Î†åÏßÄ': 'orange',
            'ÎÖ∏ÎûÄ': 'yellow', 'ÎÖ∏Îûë': 'yellow', 'Ìô©ÏÉâ': 'yellow',
            'Ï¥àÎ°ù': 'green', 'ÎÖπÏÉâ': 'green',
            'ÌååÎûÄ': 'blue', 'ÌååÎûë': 'blue', 'Ï≤≠ÏÉâ': 'blue',
            'Î≥¥Îùº': 'purple', 'ÏûêÏ£º': 'purple',
            'Í≤ÄÏùÄ': 'black', 'Í≤ÄÏ†ï': 'black',
            'Ìù∞': 'white', 'ÌïòÏñÄ': 'white', 'Î∞±ÏÉâ': 'white',
            'ÌöåÏÉâ': 'gray', 'Í∑∏Î†àÏù¥': 'gray',
            'ÌïëÌÅ¨': 'pink','Î∂ÑÌôç': 'pink',
            'Í∞àÏÉâ': 'brown', 'Î∏åÎùºÏö¥': 'brown',
        }
        
        for keyword, color in color_keywords.items():
            if keyword in target_lower:
                analysis['colors'].append(color)
        
        # ÏÑ±Î≥Ñ Î∞è ÏùòÏÉÅ Ï†ïÎ≥¥
        if any(word in target_lower for word in ['ÎÇ®ÏÑ±', 'ÎÇ®Ïûê', 'ÏïÑÏ†ÄÏî®']):
            analysis['gender'] = 'male'
        elif any(word in target_lower for word in ['Ïó¨ÏÑ±', 'Ïó¨Ïûê', 'ÏïÑÏ£ºÎ®∏Îãà']):
            analysis['gender'] = 'female'
        
        if any(word in target_lower for word in ['ÏÉÅÏùò', 'Ìã∞ÏÖîÏ∏†', 'ÏÖîÏ∏†', 'Ïò∑']):
            analysis['clothing'].append('top')
        if any(word in target_lower for word in ['Î™®Ïûê', 'Ï∫°', 'Ìñá']):
            analysis['clothing'].append('hat')
        
        return analysis
    
    def _perform_object_tracking(self, video, target_analysis, time_range):
        """Ïã§Ï†ú Í∞ùÏ≤¥ Ï∂îÏ†Å ÏàòÌñâ - Ìñ•ÏÉÅÎêú Î≤ÑÏ†Ñ"""
        tracking_results = []
        
        try:
            # Frame Î™®Îç∏ÏóêÏÑú Ìï¥Îãπ ÎπÑÎîîÏò§Ïùò ÌîÑÎ†àÏûÑÎì§ Í∞ÄÏ†∏Ïò§Í∏∞
            frames_query = Frame.objects.filter(video=video).order_by('timestamp')
            
            # ÏãúÍ∞Ñ Î≤îÏúÑ ÌïÑÌÑ∞ÎßÅ
            if time_range.get('start') and time_range.get('end'):
                start_time = self._parse_time_to_seconds(time_range['start'])
                end_time = self._parse_time_to_seconds(time_range['end'])
                frames_query = frames_query.filter(timestamp__gte=start_time, timestamp__lte=end_time)
                logger.info(f"‚è∞ ÏãúÍ∞Ñ ÌïÑÌÑ∞ÎßÅ: {start_time}s ~ {end_time}s")
            
            frames = list(frames_query)
            logger.info(f"üìä Î∂ÑÏÑùÌï† ÌîÑÎ†àÏûÑ Ïàò: {len(frames)}Í∞ú")
            
            if not frames:
                logger.warning("‚ö†Ô∏è Î∂ÑÏÑùÌï† ÌîÑÎ†àÏûÑÏù¥ ÏóÜÏäµÎãàÎã§.")
                return []
            
            for frame in frames:
                try:
                    matches = self._find_matching_objects(frame, target_analysis)
                    for match in matches:
                        tracking_results.append({
                            'frame_id': frame.image_id,
                            'timestamp': frame.timestamp,
                            'confidence': match['confidence'],
                            'bbox': match['bbox'],
                            'description': match['description'],
                            'tracking_id': match.get('tracking_id', f"obj_{frame.image_id}"),
                            'match_reasons': match['match_reasons']
                        })
                except Exception as frame_error:
                    logger.warning(f"‚ö†Ô∏è ÌîÑÎ†àÏûÑ {frame.image_id} Ï≤òÎ¶¨ Ïã§Ìå®: {frame_error}")
                    continue
            
            # ÏãúÍ∞ÑÏàú Ï†ïÎ†¨
            tracking_results.sort(key=lambda x: x['timestamp'])
            
            return tracking_results
            
        except Exception as e:
            logger.error(f"‚ùå Ï∂îÏ†Å ÏàòÌñâ Ïò§Î•ò: {e}")
            return []
        
    def _perform_lenient_tracking(self, video, target_analysis, time_range):
        try:
            frames_query = Frame.objects.filter(video=video).order_by('timestamp')
            if time_range.get('start') and time_range.get('end'):
                start_time = self._parse_time_to_seconds(time_range['start'])
                end_time = self._parse_time_to_seconds(time_range['end'])
                frames_query = frames_query.filter(timestamp__gte=start_time, timestamp__lte=end_time)
                
            tracking_results = []
            for frame in frames_query:
                try:
                    detected_objects = self._get_detected_objects(frame)
                    for obj in detected_objects:
                        match_score = 0.0
                        match_reasons = []
                        
                        # Í∞ùÏ≤¥ ÌÉÄÏûÖ (ÌïÑÏàò)
                        if target_analysis.get('object_type'):
                            if obj['class'] == target_analysis['object_type']:
                                match_score += 0.3
                                match_reasons.append(f"{obj['class']} Í∞ùÏ≤¥ ÌÉÄÏûÖ Îß§Ïπ≠")
                            else:
                                continue  # Í∞ùÏ≤¥ ÌÉÄÏûÖÏù¥ Îã§Î•¥Î©¥ Í±¥ÎÑàÎõ∞Í∏∞
                        
                        # ÏÉâÏÉÅ (Í¥ÄÎåÄÌïòÏßÄÎßå Ïó¨Ï†ÑÌûà ÏÑ†Î≥ÑÏ†Å)
                        color_matched = False
                        if target_analysis.get('colors'):
                            for color in target_analysis['colors']:
                                obj_color_desc = obj['color_description'].lower()
                                if color == 'black':
                                    if 'black' in obj_color_desc:
                                        if 'mixed' not in obj_color_desc:
                                            match_score += 0.3  # ÏàúÏàò black
                                        else:
                                            match_score += 0.1  # black-mixed
                                        match_reasons.append(f"{color} ÏÉâÏÉÅ Îß§Ïπ≠")
                                        color_matched = True
                                        break
                                else:
                                    if color in obj_color_desc or color in [str(c).lower() for c in obj['colors']]:
                                        match_score += 0.2
                                        match_reasons.append(f"{color} ÏÉâÏÉÅ Îß§Ïπ≠")
                                        color_matched = True
                                        break
                            
                            if not color_matched:
                                continue  # ÏÉâÏÉÅÏù¥ ÏßÄÏ†ïÎêòÏóàÎäîÎç∞ Îß§Ïπ≠ÎêòÏßÄ ÏïäÏúºÎ©¥ Ï†úÏô∏
                        
                        # ÌÇ§ÏõåÎìú Îß§Ïπ≠
                        for keyword in target_analysis.get('keywords', []):
                            if keyword in obj['class'] and keyword not in ['ÏÇ¨Îûå', 'Ïò∑', 'ÏûÖÏùÄ']:
                                match_score += 0.1
                                match_reasons.append(f"ÌÇ§ÏõåÎìú '{keyword}' Îß§Ïπ≠")
                        
                        # Í¥ÄÎåÄÌïú Í≤ÄÏÉâÏóêÏÑúÎèÑ ÏµúÏÜå Ï†êÏàò Ïú†ÏßÄ
                        if match_score >= 0.3:
                            tracking_results.append({
                                'frame_id': frame.image_id,
                                'timestamp': frame.timestamp,
                                'confidence': min(match_score, obj['confidence'] or 0.5),
                                'bbox': obj['bbox'],
                                'description': self._generate_match_description(obj, target_analysis),
                                'tracking_id': obj.get('track_id') or f"obj_{frame.image_id}",
                                'match_reasons': match_reasons
                            })
                except Exception:
                    continue
                    
            tracking_results.sort(key=lambda x: x['timestamp'])
            logger.info(f"üîç Í¥ÄÎåÄÌïú Í≤ÄÏÉâ Í≤∞Í≥º: {len(tracking_results)}Í∞ú")
            return tracking_results
        except Exception as e:
            logger.error(f"‚ùå Í¥ÄÎåÄÌïú Ï∂îÏ†Å Ïò§Î•ò: {e}")
            return []
    def _get_detected_objects(self, frame):
        """
        Îã§ÏñëÌïú Ï†ÄÏû• Ïä§ÌÇ§ÎßàÎ•º Ìò∏ÌôòÌï¥ÏÑú Í∞ùÏ≤¥ Î¶¨Ïä§Ìä∏Î•º Î∞òÌôòÌïúÎã§.
        Ïö∞ÏÑ†ÏàúÏúÑ:
        1) frame.detected_objects
        2) frame.comprehensive_features['objects']
        3) frame.yolo_objects / frame.detections / frame.objects
        Î¨∏ÏûêÏó¥(JSON)Î°ú Ï†ÄÏû•Îêú Í≤ΩÏö∞ ÌååÏã± ÏãúÎèÑ.
        Í∞Å Í∞ùÏ≤¥Îäî ÏµúÏÜåÌïú {'class','bbox','confidence'} ÌÇ§Î•º Í∞ñÎèÑÎ°ù Ï†ïÍ∑úÌôî.
        """
        import json

        candidates = []

        # 1) detected_objects
        if hasattr(frame, 'detected_objects') and frame.detected_objects:
            candidates.append(frame.detected_objects)

        # 2) comprehensive_features.objects
        if hasattr(frame, 'comprehensive_features') and frame.comprehensive_features:
            objs = None
            if isinstance(frame.comprehensive_features, dict):
                objs = frame.comprehensive_features.get('objects') \
                or frame.comprehensive_features.get('detections')
            elif isinstance(frame.comprehensive_features, str):
                try:
                    cf = json.loads(frame.comprehensive_features)
                    objs = (cf or {}).get('objects') or (cf or {}).get('detections')
                except Exception:
                    pass
            if objs:
                candidates.append(objs)

        # 3) Í∏∞ÌÉÄ ÌïÑÎìúÎì§
        for attr in ('yolo_objects', 'detections', 'objects'):
            if hasattr(frame, attr) and getattr(frame, attr):
                candidates.append(getattr(frame, attr))

        # Ï≤´ Î≤àÏß∏ Ïú†Ìö® ÌõÑÎ≥¥ ÏÑ†ÌÉù
        detected = None
        for c in candidates:
            try:
                if isinstance(c, str):
                    c = json.loads(c)
                if isinstance(c, dict):           # {'objects': [...]} ÌòïÌÉú ÏßÄÏõê
                    c = c.get('objects') or c.get('detections')
                if isinstance(c, list):
                    detected = c
                    break
            except Exception:
                continue

        if not isinstance(detected, list):
            return []

        # Ï†ïÍ∑úÌôî
        norm = []
        for o in detected:
            if not isinstance(o, dict):
                continue
            cls = (o.get('class') or o.get('label') or o.get('name') or '').lower()
            bbox = o.get('bbox') or o.get('box') or o.get('xyxy') or []
            conf = float(o.get('confidence') or o.get('score') or 0.0)
            colors = o.get('colors') or o.get('color') or []
            if isinstance(colors, str):
                colors = [colors]
            color_desc = (o.get('color_description') or o.get('dominant_color') or 'unknown')
            track_id = o.get('track_id') or o.get('id')

            norm.append({
                'class': cls,
                'bbox': bbox,
                'confidence': conf,
                'colors': colors,
                'color_description': str(color_desc).lower(),
                'track_id': track_id,
                # ÏõêÎ≥∏ÎèÑ Í∞ôÏù¥ Î≥¥Í¥Ä(ÎîîÎ≤ÑÍ∑∏/ÌôïÏû•Ïö©)
                '_raw': o,
            })
        return norm

    def _find_matching_objects(self, frame, target_analysis):
        matches = []
        try:
            detected_objects = self._get_detected_objects(frame)
            if not detected_objects:
                return matches
                
            for obj in detected_objects:
                match_score = 0.0
                match_reasons = []
                
                # Í∞ùÏ≤¥ ÌÉÄÏûÖ Îß§Ïπ≠ (ÌïÑÏàò)
                if target_analysis.get('object_type') and obj['class'] == target_analysis['object_type']:
                    match_score += 0.4
                    match_reasons.append(f"{target_analysis['object_type']} Í∞ùÏ≤¥ Îß§Ïπ≠")
                elif target_analysis.get('object_type') and obj['class'] != target_analysis['object_type']:
                    # Í∞ùÏ≤¥ ÌÉÄÏûÖÏù¥ Îã§Î•¥Î©¥ Í±¥ÎÑàÎõ∞Í∏∞
                    continue
                
                # ÏÉâÏÉÅ Îß§Ïπ≠ (Îçî ÏóÑÍ≤©ÌïòÍ≤å)
                color_matched = False
                if target_analysis.get('colors'):
                    target_colors = target_analysis['colors']
                    obj_color_desc = obj['color_description'].lower()
                    obj_colors = [str(c).lower() for c in obj['colors']]
                    
                    for target_color in target_colors:
                        # Ï†ïÌôïÌïú ÏÉâÏÉÅ Îß§Ïπ≠ Ïö∞ÏÑ†
                        if target_color == 'black':
                            if ('black' in obj_color_desc and 'mixed' not in obj_color_desc) or \
                            'black' in obj_colors:
                                match_score += 0.5  # Ï†ïÌôïÌïú ÏÉâÏÉÅ Îß§Ïπ≠ ÎÜíÏùÄ Ï†êÏàò
                                match_reasons.append(f"Ï†ïÌôïÌïú {target_color} ÏÉâÏÉÅ Îß§Ïπ≠")
                                color_matched = True
                                break
                            elif 'black' in obj_color_desc:  # black-mixed Îì±
                                match_score += 0.2  # Î∂ÄÎ∂Ñ Îß§Ïπ≠ ÎÇÆÏùÄ Ï†êÏàò
                                match_reasons.append(f"Î∂ÄÎ∂Ñ {target_color} ÏÉâÏÉÅ Îß§Ïπ≠")
                                color_matched = True
                        else:
                            # Îã§Î•∏ ÏÉâÏÉÅÎì§ÎèÑ ÎπÑÏä∑Ìïú Î°úÏßÅ
                            if target_color in obj_color_desc and 'mixed' not in obj_color_desc:
                                match_score += 0.5
                                match_reasons.append(f"Ï†ïÌôïÌïú {target_color} ÏÉâÏÉÅ Îß§Ïπ≠")
                                color_matched = True
                                break
                            elif target_color in obj_color_desc or target_color in obj_colors:
                                match_score += 0.2
                                match_reasons.append(f"Î∂ÄÎ∂Ñ {target_color} ÏÉâÏÉÅ Îß§Ïπ≠")
                                color_matched = True
                    
                    # ÏÉâÏÉÅÏù¥ ÏßÄÏ†ïÎêòÏóàÎäîÎç∞ Îß§Ïπ≠ÎêòÏßÄ ÏïäÏúºÎ©¥ Ï†úÏô∏
                    if not color_matched:
                        continue
                
                # ÌÇ§ÏõåÎìú Îß§Ïπ≠ (Î≥¥Ï°∞)
                for keyword in target_analysis.get('keywords', []):
                    if keyword in obj['class'] and keyword not in ['ÏÇ¨Îûå', 'Ïò∑', 'ÏûÖÏùÄ']:
                        match_score += 0.1
                        match_reasons.append(f"ÌÇ§ÏõåÎìú '{keyword}' Îß§Ïπ≠")
                
                # ÏµúÏÜå Ï†êÏàò Í∏∞Ï§Ä ÏÉÅÌñ• Ï°∞Ï†ï
                if match_score >= 0.4:  # 0.3ÏóêÏÑú 0.4Î°ú ÏÉÅÌñ•
                    matches.append({
                        'confidence': min(match_score, obj['confidence'] or 0.5),
                        'bbox': obj['bbox'],
                        'description': self._generate_match_description(obj, target_analysis),
                        'match_reasons': match_reasons,
                        'tracking_id': obj.get('track_id') or f"obj_{frame.image_id}",
                    })
            return matches
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Í∞ùÏ≤¥ Îß§Ïπ≠ Ïò§Î•ò: {e}")
            return []

    
    def _generate_match_description(self, obj, target_analysis):
        """Îß§Ïπ≠ ÏÑ§Î™Ö ÏÉùÏÑ± - Ìñ•ÏÉÅÎêú Î≤ÑÏ†Ñ"""
        desc_parts = []
        
        # ÏÉâÏÉÅ Ï†ïÎ≥¥
        color_desc = obj.get('color_description', '')
        if color_desc and color_desc != 'unknown':
            desc_parts.append(color_desc)
        
        # Í∞ùÏ≤¥ ÌÅ¥ÎûòÏä§
        obj_class = obj.get('class', 'Í∞ùÏ≤¥')
        desc_parts.append(obj_class)
        
        # ÏÑ±Î≥Ñ Ï†ïÎ≥¥ (ÏûàÎäî Í≤ΩÏö∞)
        if target_analysis.get('gender'):
            desc_parts.append(f"({target_analysis['gender']})")
        
        # ÏùòÏÉÅ Ï†ïÎ≥¥ (ÏûàÎäî Í≤ΩÏö∞)
        if target_analysis.get('clothing'):
            clothing_desc = ', '.join(target_analysis['clothing'])
            desc_parts.append(f"[{clothing_desc}]")
        
        description = ' '.join(desc_parts) + ' Í∞êÏßÄ'
        
        return description
    
    def _parse_time_to_seconds(self, time_str):
        """ÏãúÍ∞Ñ Î¨∏ÏûêÏó¥ÏùÑ Ï¥àÎ°ú Î≥ÄÌôò - Ìñ•ÏÉÅÎêú Î≤ÑÏ†Ñ"""
        try:
            if not time_str:
                return 0
            
            time_str = str(time_str).strip()
            
            if ':' in time_str:
                parts = time_str.split(':')
                minutes = int(parts[0])
                seconds = int(parts[1]) if len(parts) > 1 else 0
                return minutes * 60 + seconds
            else:
                # ÏàúÏàò Ïà´ÏûêÏù∏ Í≤ΩÏö∞
                return int(float(time_str))
        except (ValueError, TypeError) as e:
            logger.warning(f"‚ö†Ô∏è ÏãúÍ∞Ñ ÌååÏã± Ïã§Ìå®: {time_str} -> {e}")
            return 0

@method_decorator(csrf_exempt, name='dispatch')
class TimeBasedAnalysisView(APIView):
    """ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù - ÏàòÏ†ïÎêú Î≤ÑÏ†Ñ"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            time_range = request.data.get('time_range', {})
            analysis_type = request.data.get('analysis_type', 'ÏÑ±ÎπÑ Î∂ÑÌè¨')
            
            logger.info(f"üìä ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù ÏöîÏ≤≠: ÎπÑÎîîÏò§={video_id}, ÏãúÍ∞ÑÎ≤îÏúÑ={time_range}, ÌÉÄÏûÖ='{analysis_type}'")
            
            if not video_id or not time_range.get('start') or not time_range.get('end'):
                return Response({'error': 'ÎπÑÎîîÏò§ IDÏôÄ ÏãúÍ∞Ñ Î≤îÏúÑÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.'}, status=400)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'}, status=404)
            
            # ÏãúÍ∞Ñ Î≤îÏúÑ ÌååÏã±
            start_time = self._parse_time_to_seconds(time_range['start'])
            end_time = self._parse_time_to_seconds(time_range['end'])
            
            logger.info(f"‚è∞ Î∂ÑÏÑù ÏãúÍ∞Ñ: {start_time}Ï¥à ~ {end_time}Ï¥à")
            
            # Ìï¥Îãπ ÏãúÍ∞ÑÎåÄÏùò ÌîÑÎ†àÏûÑÎì§ Î∂ÑÏÑù
            analysis_result = self._perform_time_based_analysis(
                video, start_time, end_time, analysis_type
            )
            
            logger.info(f"‚úÖ ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù ÏôÑÎ£å")
            
            return Response({
                'video_id': video_id,
                'time_range': time_range,
                'analysis_type': analysis_type,
                'result': analysis_result,
                'search_type': 'time_analysis'
            })
            
        except Exception as e:
            logger.error(f"‚ùå ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù Ïò§Î•ò: {e}")
            return Response({'error': str(e)}, status=500)
    
    def _perform_time_based_analysis(self, video, start_time, end_time, analysis_type):
        """ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù ÏàòÌñâ"""
        
        # Ìï¥Îãπ ÏãúÍ∞ÑÎåÄ ÌîÑÎ†àÏûÑÎì§ Í∞ÄÏ†∏Ïò§Í∏∞
        frames = Frame.objects.filter(
            video=video,
            timestamp__gte=start_time,
            timestamp__lte=end_time
        ).order_by('timestamp')
        
        frame_list = list(frames)
        logger.info(f"üìä Î∂ÑÏÑù ÎåÄÏÉÅ ÌîÑÎ†àÏûÑ: {len(frame_list)}Í∞ú")
        
        if 'ÏÑ±ÎπÑ' in analysis_type or 'ÏÇ¨Îûå' in analysis_type:
            return self._analyze_gender_distribution(frame_list, start_time, end_time)
        elif 'Ï∞®Îüâ' in analysis_type or 'ÍµêÌÜµ' in analysis_type:
            return self._analyze_vehicle_distribution(frame_list, start_time, end_time)
        else:
            return self._analyze_general_statistics(frame_list, start_time, end_time)
    
    def _analyze_gender_distribution(self, frames, start_time, end_time):
        """ÏÑ±ÎπÑ Î∂ÑÏÑù"""
        person_detections = []
        
        for frame in frames:
            if not hasattr(frame, 'detected_objects') or not frame.detected_objects:
                continue
                
            for obj in frame.detected_objects:
                if obj.get('class') == 'person':
                    person_detections.append({
                        'timestamp': frame.timestamp,
                        'confidence': obj.get('confidence', 0.5),
                        'bbox': obj.get('bbox', []),
                        'colors': obj.get('colors', []),
                        'color_description': obj.get('color_description', '')
                    })
        
        # ÏÑ±Î≥Ñ Ï∂îÏ†ï (Í∞ÑÎã®Ìïú Ìú¥Î¶¨Ïä§Ìã± - Ïã§Ï†úÎ°úÎäî Îçî Ï†ïÍµêÌïú AI Î™®Îç∏ ÌïÑÏöî)
        male_count = 0
        female_count = 0
        
        for detection in person_detections:
            # ÏÉâÏÉÅ Í∏∞Î∞ò Í∞ÑÎã®Ìïú ÏÑ±Î≥Ñ Ï∂îÏ†ï
            colors = detection['color_description'].lower()
            if 'blue' in colors or 'black' in colors or 'gray' in colors:
                male_count += 1
            elif 'pink' in colors or 'red' in colors:
                female_count += 1
            else:
                # 50:50ÏúºÎ°ú Î∂ÑÎ∞∞
                if len(person_detections) % 2 == 0:
                    male_count += 1
                else:
                    female_count += 1
        
        total_persons = male_count + female_count
        
        # ÏùòÏÉÅ ÏÉâÏÉÅ Î∂ÑÌè¨
        clothing_colors = {}
        for detection in person_detections:
            color = detection['color_description']
            if color and color != 'unknown':
                clothing_colors[color] = clothing_colors.get(color, 0) + 1
        
        # ÌîºÌÅ¨ ÏãúÍ∞ÑÎåÄ Î∂ÑÏÑù
        time_distribution = {}
        for detection in person_detections:
            time_bucket = int(detection['timestamp'] // 30) * 30  # 30Ï¥à Îã®ÏúÑ
            time_distribution[time_bucket] = time_distribution.get(time_bucket, 0) + 1
        
        peak_times = sorted(time_distribution.items(), key=lambda x: x[1], reverse=True)[:2]
        peak_time_strings = [f"{self._seconds_to_time_string(t[0])}-{self._seconds_to_time_string(t[0]+30)}" 
                           for t in peak_times]
        
        return {
            'total_persons': total_persons,
            'male_count': male_count,
            'female_count': female_count,
            'gender_ratio': {
                'male': round((male_count / total_persons * 100), 1) if total_persons > 0 else 0,
                'female': round((female_count / total_persons * 100), 1) if total_persons > 0 else 0
            },
            'clothing_colors': dict(sorted(clothing_colors.items(), key=lambda x: x[1], reverse=True)),
            'peak_times': peak_time_strings,
            'movement_patterns': 'left_to_right_dominant',  # Í∞ÑÎã®Ìïú ÏòàÏãú
            'analysis_period': f"{self._seconds_to_time_string(start_time)} - {self._seconds_to_time_string(end_time)}"
        }
    
    def _analyze_vehicle_distribution(self, frames, start_time, end_time):
        """Ï∞®Îüâ Î∂ÑÌè¨ Î∂ÑÏÑù"""
        vehicles = []
        
        for frame in frames:
            if not hasattr(frame, 'detected_objects') or not frame.detected_objects:
                continue
                
            for obj in frame.detected_objects:
                if obj.get('class') in ['car', 'truck', 'bus', 'motorcycle']:
                    vehicles.append({
                        'type': obj.get('class'),
                        'timestamp': frame.timestamp,
                        'confidence': obj.get('confidence', 0.5)
                    })
        
        vehicle_types = {}
        for v in vehicles:
            vehicle_types[v['type']] = vehicle_types.get(v['type'], 0) + 1
        
        duration_minutes = (end_time - start_time) / 60
        
        return {
            'total_vehicles': len(vehicles),
            'vehicle_types': vehicle_types,
            'average_per_minute': round(len(vehicles) / max(1, duration_minutes), 1),
            'analysis_period': f"{self._seconds_to_time_string(start_time)} - {self._seconds_to_time_string(end_time)}"
        }
    
    def _analyze_general_statistics(self, frames, start_time, end_time):
        """ÏùºÎ∞ò ÌÜµÍ≥Ñ Î∂ÑÏÑù"""
        all_objects = []
        
        for frame in frames:
            if hasattr(frame, 'detected_objects') and frame.detected_objects:
                all_objects.extend(frame.detected_objects)
        
        object_counts = {}
        for obj in all_objects:
            obj_class = obj.get('class', 'unknown')
            object_counts[obj_class] = object_counts.get(obj_class, 0) + 1
        
        return {
            'total_objects': len(all_objects),
            'object_distribution': dict(sorted(object_counts.items(), key=lambda x: x[1], reverse=True)),
            'frames_analyzed': len(frames),
            'average_objects_per_frame': round(len(all_objects) / max(1, len(frames)), 1),
            'analysis_period': f"{self._seconds_to_time_string(start_time)} - {self._seconds_to_time_string(end_time)}"
        }
    
    def _parse_time_to_seconds(self, time_str):
        """ÏãúÍ∞Ñ Î¨∏ÏûêÏó¥ÏùÑ Ï¥àÎ°ú Î≥ÄÌôò"""
        try:
            if ':' in time_str:
                parts = time_str.split(':')
                minutes = int(parts[0])
                seconds = int(parts[1]) if len(parts) > 1 else 0
                return minutes * 60 + seconds
            else:
                return int(time_str)
        except:
            return 0
    
    def _seconds_to_time_string(self, seconds):
        """Ï¥àÎ•º ÏãúÍ∞Ñ Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò"""
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes}:{secs:02d}"


@method_decorator(csrf_exempt, name='dispatch')
class CrossVideoSearchView(APIView):
    """ÏòÅÏÉÅ Í∞Ñ Í≤ÄÏÉâ - ÏàòÏ†ïÎêú Î≤ÑÏ†Ñ"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            query = request.data.get('query', '').strip()
            search_filters = request.data.get('filters', {})
            
            logger.info(f"üîç ÌÅ¨Î°úÏä§ ÎπÑÎîîÏò§ Í≤ÄÏÉâ ÏöîÏ≤≠: '{query}'")
            
            if not query:
                return Response({'error': 'Í≤ÄÏÉâÏñ¥Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'}, status=400)
            
            # ÏøºÎ¶¨ Î∂ÑÏÑù
            query_analysis = self._analyze_query(query)
            
            # Î∂ÑÏÑùÎêú ÎπÑÎîîÏò§Îì§ Ï§ëÏóêÏÑú Í≤ÄÏÉâ
            videos = Video.objects.filter(is_analyzed=True)
            matching_videos = []
            
            for video in videos:
                match_score = self._calculate_video_match_score(video, query_analysis, search_filters)
                if match_score > 0.3:  # ÏûÑÍ≥ÑÍ∞í
                    matching_videos.append({
                        'video_id': video.id,
                        'video_name': video.original_name,
                        'match_score': match_score,
                        'match_reasons': self._get_match_reasons(video, query_analysis),
                        'metadata': self._get_video_metadata(video),
                        'thumbnail_url': f'/frame/{video.id}/100/',
                    })
            
            # Ï†êÏàòÏàú Ï†ïÎ†¨
            matching_videos.sort(key=lambda x: x['match_score'], reverse=True)
            
            logger.info(f"‚úÖ ÌÅ¨Î°úÏä§ ÎπÑÎîîÏò§ Í≤ÄÏÉâ ÏôÑÎ£å: {len(matching_videos)}Í∞ú Í≤∞Í≥º")
            
            return Response({
                'query': query,
                'total_matches': len(matching_videos),
                'results': matching_videos[:20],  # ÏÉÅÏúÑ 20Í∞ú
                'query_analysis': query_analysis,
                'search_type': 'cross_video'
            })
            
        except Exception as e:
            logger.error(f"‚ùå ÌÅ¨Î°úÏä§ ÎπÑÎîîÏò§ Í≤ÄÏÉâ Ïò§Î•ò: {e}")
            return Response({'error': str(e)}, status=500)
    
    def _analyze_query(self, query):
        """ÏøºÎ¶¨ÏóêÏÑú ÎÇ†Ïî®, ÏãúÍ∞ÑÎåÄ, Ïû•ÏÜå Îì± Ï∂îÏ∂ú"""
        analysis = {
            'weather': None,
            'time_of_day': None,
            'location': None,
            'objects': [],
            'activities': []
        }
        
        query_lower = query.lower()
        
        # ÎÇ†Ïî® ÌÇ§ÏõåÎìú
        weather_keywords = {
            'ÎπÑ': 'rainy', 'ÎπÑÍ∞Ä': 'rainy', 'Ïö∞Ï≤ú': 'rainy',
            'ÎßëÏùÄ': 'sunny', 'ÌôîÏ∞ΩÌïú': 'sunny', 'ÌñáÎπõ': 'sunny',
            'ÌùêÎ¶∞': 'cloudy', 'Íµ¨Î¶Ñ': 'cloudy'
        }
        
        # ÏãúÍ∞ÑÎåÄ ÌÇ§ÏõåÎìú
        time_keywords = {
            'Î∞§': 'night', 'ÏïºÍ∞Ñ': 'night', 'Ï†ÄÎÖÅ': 'evening',
            'ÎÇÆ': 'day', 'Ïò§ÌõÑ': 'afternoon', 'ÏïÑÏπ®': 'morning'
        }
        
        # Ïû•ÏÜå ÌÇ§ÏõåÎìú
        location_keywords = {
            'Ïã§ÎÇ¥': 'indoor', 'Í±¥Î¨º': 'indoor', 'Î∞©': 'indoor',
            'Ïã§Ïô∏': 'outdoor', 'ÎèÑÎ°ú': 'outdoor', 'Í±∞Î¶¨': 'outdoor'
        }
        
        for keyword, value in weather_keywords.items():
            if keyword in query_lower:
                analysis['weather'] = value
                break
        
        for keyword, value in time_keywords.items():
            if keyword in query_lower:
                analysis['time_of_day'] = value
                break
                
        for keyword, value in location_keywords.items():
            if keyword in query_lower:
                analysis['location'] = value
                break
        
        return analysis
    
    def _calculate_video_match_score(self, video, query_analysis, filters):
        """ÎπÑÎîîÏò§ÏôÄ ÏøºÎ¶¨ Í∞ÑÏùò Îß§Ïπ≠ Ï†êÏàò Í≥ÑÏÇ∞"""
        score = 0.0
        
        try:
            # VideoAnalysisÏóêÏÑú Î∂ÑÏÑù Í≤∞Í≥ºÍ∞Ä ÏûàÎäî Í≤ΩÏö∞
            if hasattr(video, 'analysis'):
                analysis = video.analysis
                stats = analysis.analysis_statistics
                scene_types = stats.get('scene_types', [])
                
                # ÎÇ†Ïî® Îß§Ïπ≠
                if query_analysis['weather']:
                    weather_scenes = [s for s in scene_types if query_analysis['weather'] in s.lower()]
                    if weather_scenes:
                        score += 0.4
                
                # ÏãúÍ∞ÑÎåÄ Îß§Ïπ≠
                if query_analysis['time_of_day']:
                    time_scenes = [s for s in scene_types if query_analysis['time_of_day'] in s.lower()]
                    if time_scenes:
                        score += 0.3
                
                # Ïû•ÏÜå Îß§Ïπ≠
                if query_analysis['location']:
                    location_scenes = [s for s in scene_types if query_analysis['location'] in s.lower()]
                    if location_scenes:
                        score += 0.3
            
            return min(score, 1.0)
            
        except Exception:
            return 0.0
    
    def _get_match_reasons(self, video, query_analysis):
        """Îß§Ïπ≠ Ïù¥Ïú† ÏÉùÏÑ±"""
        reasons = []
        
        if query_analysis['weather']:
            reasons.append(f"{query_analysis['weather']} ÎÇ†Ïî® Ï°∞Í±¥")
        if query_analysis['time_of_day']:
            reasons.append(f"{query_analysis['time_of_day']} ÏãúÍ∞ÑÎåÄ")
        if query_analysis['location']:
            reasons.append(f"{query_analysis['location']} ÌôòÍ≤Ω")
            
        return reasons
    
    def _get_video_metadata(self, video):
        """ÎπÑÎîîÏò§ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î∞òÌôò"""
        metadata = {
            'duration': video.duration,
            'file_size': video.file_size,
            'uploaded_at': video.uploaded_at.isoformat(),
            'analysis_type': 'basic'
        }
        
        if hasattr(video, 'analysis'):
            stats = video.analysis.analysis_statistics
            metadata.update({
                'analysis_type': stats.get('analysis_type', 'basic'),
                'scene_types': stats.get('scene_types', []),
                'dominant_objects': stats.get('dominant_objects', [])
            })
        
        return metadata


class AdvancedSearchAutoView(APIView):
    """ÌÜµÌï© Í≥†Í∏â Í≤ÄÏÉâ - ÏûêÎèô ÌÉÄÏûÖ Í∞êÏßÄ"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            query = request.data.get('query', '').strip()
            video_id = request.data.get('video_id')
            time_range = request.data.get('time_range', {})
            options = request.data.get('options', {})
            
            if not query:
                return Response({'error': 'Í≤ÄÏÉâÏñ¥Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'}, status=400)
            
            # Í≤ÄÏÉâ ÌÉÄÏûÖ ÏûêÎèô Í∞êÏßÄ
            search_type = self._detect_search_type(query, video_id, time_range, options)
            
            # Ìï¥Îãπ Í≤ÄÏÉâ ÌÉÄÏûÖÏóê Îî∞Îùº Ï†ÅÏ†àÌïú View Ìò∏Ï∂ú
            if search_type == 'cross-video':
                view = CrossVideoSearchView()
                return view.post(request)
            elif search_type == 'object-tracking':
                view = IntraVideoTrackingView()
                return view.post(request)
            elif search_type == 'time-analysis':
                view = TimeBasedAnalysisView()
                return view.post(request)
            else:
                # Í∏∞Î≥∏ Í≤ÄÏÉâÏúºÎ°ú fallback
                view = EnhancedVideoChatView()
                return view.post(request)
                
        except Exception as e:
            return Response({'error': str(e)}, status=500)
    
    def _detect_search_type(self, query, video_id, time_range, options):
        """Í≤ÄÏÉâ ÌÉÄÏûÖ ÏûêÎèô Í∞êÏßÄ Î°úÏßÅ"""
        query_lower = query.lower()
        
        # ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù ÌÇ§ÏõåÎìú
        time_analysis_keywords = [
            'ÏÑ±ÎπÑ', 'Î∂ÑÌè¨', 'ÌÜµÍ≥Ñ', 'ÏãúÍ∞ÑÎåÄ', 'Íµ¨Í∞Ñ', 'ÏÇ¨Ïù¥', 
            'Î™áÎ™Ö', 'ÏñºÎßàÎÇò', 'ÌèâÍ∑†', 'ÎπÑÏú®', 'Ìå®ÌÑ¥', 'Î∂ÑÏÑù'
        ]
        
        # Í∞ùÏ≤¥ Ï∂îÏ†Å ÌÇ§ÏõåÎìú
        tracking_keywords = [
            'Ï∂îÏ†Å', 'Îî∞ÎùºÍ∞Ä', 'Ïù¥Îèô', 'Í≤ΩÎ°ú', 'ÏßÄÎÇòÍ∞Ñ', 
            'ÏÉÅÏùò', 'Î™®Ïûê', 'ÏÉâÍπî', 'Ïò∑', 'ÏÇ¨Îûå', 'Ï∞®Îüâ'
        ]
        
        # ÏòÅÏÉÅ Í∞Ñ Í≤ÄÏÉâ ÌÇ§ÏõåÎìú
        cross_video_keywords = [
            'Ï¥¨ÏòÅÎêú', 'ÏòÅÏÉÅ', 'ÎπÑÎîîÏò§', 'Ï∞æÏïÑ', 'ÎπÑÍ∞Ä', 'Î∞§', 
            'ÎÇÆ', 'Ïã§ÎÇ¥', 'Ïã§Ïô∏', 'Ïû•ÏÜå', 'ÎÇ†Ïî®'
        ]
        
        # ÏãúÍ∞Ñ Î≤îÏúÑÍ∞Ä ÏûàÍ≥† Î∂ÑÏÑù ÌÇ§ÏõåÎìúÍ∞Ä ÏûàÏúºÎ©¥ ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù
        if (time_range.get('start') and time_range.get('end')) or \
           any(keyword in query_lower for keyword in time_analysis_keywords):
            return 'time-analysis'
        
        # ÌäπÏ†ï ÎπÑÎîîÏò§ IDÍ∞Ä ÏûàÍ≥† Ï∂îÏ†Å ÌÇ§ÏõåÎìúÍ∞Ä ÏûàÏúºÎ©¥ Í∞ùÏ≤¥ Ï∂îÏ†Å
        if video_id and any(keyword in query_lower for keyword in tracking_keywords):
            return 'object-tracking'
        
        # ÌÅ¨Î°úÏä§ ÎπÑÎîîÏò§ ÌÇ§ÏõåÎìúÍ∞Ä ÏûàÏúºÎ©¥ ÏòÅÏÉÅ Í∞Ñ Í≤ÄÏÉâ
        if any(keyword in query_lower for keyword in cross_video_keywords):
            return 'cross-video'
        
        # Í∏∞Î≥∏Í∞í: ÎπÑÎîîÏò§ IDÍ∞Ä ÏûàÏúºÎ©¥ Ï∂îÏ†Å, ÏóÜÏúºÎ©¥ ÌÅ¨Î°úÏä§ ÎπÑÎîîÏò§
        return 'object-tracking' if video_id else 'cross-video'


# views.pyÏóê Ï∂îÍ∞ÄÌï† ÎàÑÎùΩÎêú ViewÎì§


# ‚úÖ LLMStatsView Ï∂îÍ∞Ä
class LLMStatsView(APIView):
    """LLM ÏÑ±Îä• ÌÜµÍ≥Ñ Î∑∞"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            # Í∞ÑÎã®Ìïú ÌÜµÍ≥Ñ Î∞òÌôò (Ïã§Ï†úÎ°úÎäî Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú ÏàòÏßë)
            stats = {
                'total_requests': 0,
                'model_usage': {
                    'gpt-4v': {'count': 0, 'avg_time': 0, 'success_rate': 0},
                    'claude-3.5': {'count': 0, 'avg_time': 0, 'success_rate': 0},
                    'gemini-pro': {'count': 0, 'avg_time': 0, 'success_rate': 0},
                    'groq-llama': {'count': 0, 'avg_time': 0, 'success_rate': 0}
                },
                'average_response_time': 0,
                'overall_success_rate': 0,
                'last_updated': datetime.now().isoformat()
            }
            
            return Response(stats)
            
        except Exception as e:
            return Response({'error': str(e)}, status=500)


# ‚úÖ cleanup_storage Ìï®Ïàò Ï∂îÍ∞Ä
@csrf_exempt
@require_http_methods(["POST"])
def cleanup_storage(request):
    """Ï†ÄÏû• Í≥µÍ∞Ñ Ï†ïÎ¶¨"""
    try:
        print("üßπ Ï†ÄÏû• Í≥µÍ∞Ñ Ï†ïÎ¶¨ ÏöîÏ≤≠")
        
        from django.conf import settings
        
        cleaned_files = []
        total_size_freed = 0
        
        # ÏûÑÏãú ÌååÏùºÎì§ Ï†ïÎ¶¨ (ÏòàÏãú)
        temp_dirs = [
            os.path.join(settings.MEDIA_ROOT, 'temp'),
            os.path.join(settings.MEDIA_ROOT, 'analysis_temp'),
            '/tmp/video_analysis'
        ]
        
        for temp_dir in temp_dirs:
            if os.path.exists(temp_dir):
                try:
                    # ÏûÑÏãú ÎîîÎ†âÌÜ†Î¶¨ ÎÇ¥Ïö© Ï†ïÎ¶¨
                    for filename in os.listdir(temp_dir):
                        file_path = os.path.join(temp_dir, filename)
                        if os.path.isfile(file_path):
                            file_size = os.path.getsize(file_path)
                            os.remove(file_path)
                            cleaned_files.append(filename)
                            total_size_freed += file_size
                except Exception as e:
                    print(f"‚ö†Ô∏è ÏûÑÏãú ÎîîÎ†âÌÜ†Î¶¨ Ï†ïÎ¶¨ Ïã§Ìå®: {temp_dir} - {e}")
        
        # Ïò§ÎûòÎêú Î∂ÑÏÑù Í≤∞Í≥º ÌååÏùºÎì§ Ï†ïÎ¶¨ (ÏÑ†ÌÉùÏÇ¨Ìï≠)
        analysis_results_dir = os.path.join(settings.MEDIA_ROOT, 'analysis_results')
        if os.path.exists(analysis_results_dir):
            import time
            current_time = time.time()
            for filename in os.listdir(analysis_results_dir):
                file_path = os.path.join(analysis_results_dir, filename)
                if os.path.isfile(file_path):
                    # 7Ïùº Ïù¥ÏÉÅ Îêú ÌååÏùºÎì§ ÏÇ≠Ï†ú
                    if current_time - os.path.getmtime(file_path) > 7 * 24 * 3600:
                        try:
                            file_size = os.path.getsize(file_path)
                            os.remove(file_path)
                            cleaned_files.append(filename)
                            total_size_freed += file_size
                        except Exception as e:
                            print(f"‚ö†Ô∏è Ïò§ÎûòÎêú ÌååÏùº ÏÇ≠Ï†ú Ïã§Ìå®: {filename} - {e}")
        
        result = {
            'success': True,
            'message': f'Ï†ÄÏû• Í≥µÍ∞Ñ Ï†ïÎ¶¨ ÏôÑÎ£å: {len(cleaned_files)}Í∞ú ÌååÏùº ÏÇ≠Ï†ú',
            'details': {
                'files_cleaned': len(cleaned_files),
                'size_freed_bytes': total_size_freed,
                'size_freed_mb': round(total_size_freed / (1024 * 1024), 2),
                'cleaned_files': cleaned_files[:10],  # Ï≤òÏùå 10Í∞úÎßå ÌëúÏãú
                'timestamp': datetime.now().isoformat()
            }
        }
        
        print(f"‚úÖ Ï†ÄÏû• Í≥µÍ∞Ñ Ï†ïÎ¶¨ ÏôÑÎ£å: {result}")
        return JsonResponse(result)
        
    except Exception as e:
        print(f"‚ùå Ï†ÄÏû• Í≥µÍ∞Ñ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
        return JsonResponse({
            'success': False,
            'error': str(e),
            'message': 'Ï†ÄÏû• Í≥µÍ∞Ñ Ï†ïÎ¶¨ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.'
        }, status=500)


# ‚úÖ ÎàÑÎùΩÎêú Í∏∞ÌÉÄ Ïú†Ìã∏Î¶¨Ìã∞ Î∑∞Îì§

@csrf_exempt  
@require_http_methods(["GET"])
def check_video_exists(request, video_id):
    """ÎπÑÎîîÏò§ Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏"""
    try:
        Video.objects.get(id=video_id)
        return JsonResponse({
            'exists': True,
            'video_id': video_id
        })
    except Video.DoesNotExist:
        return JsonResponse({
            'exists': False,
            'video_id': video_id
        })


# ‚úÖ FrameWithBboxView - Î∞îÏö¥Îî© Î∞ïÏä§Í∞Ä ÏûàÎäî ÌîÑÎ†àÏûÑ Î∑∞
class FrameWithBboxView(APIView):
    permission_classes = [AllowAny]
    
    def get(self, request, video_id, frame_number):
        try:
            print(f"üñºÔ∏è Î∞îÏö¥Îî© Î∞ïÏä§ ÌîÑÎ†àÏûÑ ÏöîÏ≤≠: ÎπÑÎîîÏò§={video_id}, ÌîÑÎ†àÏûÑ={frame_number}")
            
            video = Video.objects.get(id=video_id)
            frame = Frame.objects.get(video=video, image_id=frame_number)
            
            # ÎîîÎ≤ÑÍπÖ: detected_objects ÌôïÏù∏
            print(f"üîç Frame {frame_number} detected_objects: {frame.detected_objects}")
            
            if not frame.detected_objects:
                print("‚ö†Ô∏è detected_objectsÍ∞Ä ÏóÜÏäµÎãàÎã§")
                # ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Î∞òÌôò
                return self._get_original_frame(video, frame_number)
            
            # detected_objects ÌååÏã±
            detected_objects = frame.detected_objects
            if isinstance(detected_objects, str):
                import json
                detected_objects = json.loads(detected_objects)
            
            if not isinstance(detected_objects, list):
                detected_objects = detected_objects.get('objects', []) if isinstance(detected_objects, dict) else []
            
            print(f"üì¶ ÌååÏã±Îêú Í∞ùÏ≤¥ Ïàò: {len(detected_objects)}")
            
            # Ïù¥ÎØ∏ÏßÄ Î°úÎìú Î∞è Î∞ïÏä§ Í∑∏Î¶¨Í∏∞
            image_data = self._draw_bboxes_on_frame(video, frame_number, detected_objects)
            
            return HttpResponse(image_data, content_type='image/jpeg')
            
        except Video.DoesNotExist:
            return HttpResponse(status=404)
        except Frame.DoesNotExist:
            print(f"‚ö†Ô∏è Frame {frame_number} not found")
            return HttpResponse(status=404)
        except Exception as e:
            print(f"‚ùå Î∞ïÏä§ Í∑∏Î¶¨Í∏∞ Ïã§Ìå®: {e}")
            import traceback
            print(traceback.format_exc())
            return HttpResponse(status=500)
    def _draw_bboxes_on_frame(self, video, frame_number, detected_objects):
        """ÌîÑÎ†àÏûÑÏóê Î∞îÏö¥Îî© Î∞ïÏä§ Í∑∏Î¶¨Í∏∞"""
        try:
            from PIL import Image, ImageDraw, ImageFont
            import cv2
            import io
            import numpy as np
            import os
            
            # üîß ÏàòÏ†ï: file.path ÎåÄÏã† file_path ÌïÑÎìú ÏÇ¨Ïö©
            video_path = video.file_path
            
            # ÌååÏùº Í≤ΩÎ°úÍ∞Ä Ï†àÎåÄ Í≤ΩÎ°úÍ∞Ä ÏïÑÎãå Í≤ΩÏö∞ Ï≤òÎ¶¨
            if not os.path.isabs(video_path):
                from django.conf import settings
                # MEDIA_ROOTÎÇò Ï†ÅÏ†àÌïú base pathÏôÄ Í≤∞Ìï©
                video_path = os.path.join(settings.MEDIA_ROOT, video_path)
            
            print(f"üé• ÎπÑÎîîÏò§ ÌååÏùº Í≤ΩÎ°ú: {video_path}")
            
            # ÌååÏùº Ï°¥Ïû¨ ÌôïÏù∏
            if not os.path.exists(video_path):
                print(f"‚ö†Ô∏è ÎπÑÎîîÏò§ ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå: {video_path}")
                return self._create_dummy_image_with_boxes(frame_number, detected_objects)
            
            cap = cv2.VideoCapture(video_path)
            
            if not cap.isOpened():
                print(f"‚ö†Ô∏è ÎπÑÎîîÏò§ ÌååÏùº Ïó¥Í∏∞ Ïã§Ìå®: {video_path}")
                cap.release()
                return self._create_dummy_image_with_boxes(frame_number, detected_objects)
            
            # ÌîÑÎ†àÏûÑ Î≤àÌò∏Î°ú Ïù¥Îèô (0-based index)
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number - 1)
            ret, frame = cap.read()
            cap.release()
            
            if not ret:
                print(f"‚ö†Ô∏è ÌîÑÎ†àÏûÑ {frame_number} ÏùΩÍ∏∞ Ïã§Ìå®, ÎçîÎØ∏ Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±")
                return self._create_dummy_image_with_boxes(frame_number, detected_objects)
            
            # OpenCV Ïù¥ÎØ∏ÏßÄÎ•º PIL Ïù¥ÎØ∏ÏßÄÎ°ú Î≥ÄÌôò
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(frame_rgb)
            
            # Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Í∞ÄÏ†∏Ïò§Í∏∞
            img_width, img_height = image.size
            print(f"üñºÔ∏è Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞: {img_width}x{img_height}")
            
            draw = ImageDraw.Draw(image)
            
            # Î∞îÏö¥Îî© Î∞ïÏä§ Í∑∏Î¶¨Í∏∞
            colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange', 'cyan', 'magenta']
            
            for i, obj in enumerate(detected_objects):
                bbox = obj.get('bbox', [])
                obj_class = obj.get('class', 'object')
                confidence = obj.get('confidence', 0)
                track_id = obj.get('track_id', '')
                color_description = obj.get('color_description', '')
                
                if len(bbox) == 4:
                    # Ï†ïÍ∑úÌôîÎêú Ï¢åÌëúÎ•º ÌîΩÏÖÄ Ï¢åÌëúÎ°ú Î≥ÄÌôò
                    x1_norm, y1_norm, x2_norm, y2_norm = bbox
                    
                    x1 = int(x1_norm * img_width)
                    y1 = int(y1_norm * img_height)
                    x2 = int(x2_norm * img_width)
                    y2 = int(y2_norm * img_height)
                    
                    # Ï¢åÌëú Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨
                    x1 = max(0, min(x1, img_width))
                    y1 = max(0, min(y1, img_height))
                    x2 = max(0, min(x2, img_width))
                    y2 = max(0, min(y2, img_height))
                    
                    color = colors[i % len(colors)]
                    
                    # Î∞îÏö¥Îî© Î∞ïÏä§ Í∑∏Î¶¨Í∏∞
                    draw.rectangle([x1, y1, x2, y2], outline=color, width=3)
                    
                    # Î†àÏù¥Î∏î Í∑∏Î¶¨Í∏∞
                    label_parts = [obj_class]
                    if track_id:
                        label_parts.append(f"ID:{track_id}")
                    if color_description:
                        label_parts.append(color_description)
                    label_parts.append(f"{confidence:.2f}")
                    
                    label = " | ".join(label_parts)
                    
                    # Î†àÏù¥Î∏î Î∞∞Í≤Ω Ï∂îÍ∞Ä (Í∞ÄÎèÖÏÑ± Ìñ•ÏÉÅ)
                    label_bbox = draw.textbbox((x1, y1-20), label)
                    draw.rectangle(label_bbox, fill=color, outline=color)
                    draw.text((x1, y1-20), label, fill='white')
            
            # Ïù¥ÎØ∏ÏßÄÎ•º Î∞îÏù¥Ìä∏Î°ú Î≥ÄÌôò
            buffer = io.BytesIO()
            image.save(buffer, format='JPEG', quality=90)
            print(f"‚úÖ Î∞ïÏä§Í∞Ä Í∑∏Î†§ÏßÑ Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ± ÏôÑÎ£å (Í∞ùÏ≤¥ Ïàò: {len(detected_objects)})")
            return buffer.getvalue()
            
        except Exception as e:
            print(f"‚ùå ÌîÑÎ†àÏûÑ Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
            import traceback
            print(traceback.format_exc())
            
            # Ìè¥Î∞±: ÎçîÎØ∏ Ïù¥ÎØ∏ÏßÄ Î∞òÌôò
            return self._create_dummy_image_with_boxes(frame_number, detected_objects)

    def _create_dummy_image_with_boxes(self, frame_number, detected_objects):
        """ÎçîÎØ∏ Ïù¥ÎØ∏ÏßÄÏóê Î∞îÏö¥Îî© Î∞ïÏä§ Ï†ïÎ≥¥ ÌëúÏãú"""
        try:
            from PIL import Image, ImageDraw, ImageFont
            import io
            
            # ÎçîÎØ∏ Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±
            image = Image.new('RGB', (640, 480), color='lightgray')
            draw = ImageDraw.Draw(image)
            
            # Ï†úÎ™© Í∑∏Î¶¨Í∏∞
            draw.text((10, 10), f"Frame {frame_number} - Video File Not Found", fill='black')
            
            # Í∞êÏßÄÎêú Í∞ùÏ≤¥ Ï†ïÎ≥¥ ÌëúÏãú
            y_offset = 40
            for i, obj in enumerate(detected_objects):
                obj_class = obj.get('class', 'object')
                confidence = obj.get('confidence', 0)
                track_id = obj.get('track_id', '')
                color_desc = obj.get('color_description', '')
                
                info_text = f"{i+1}. {obj_class}"
                if track_id:
                    info_text += f" (ID:{track_id})"
                if color_desc:
                    info_text += f" - {color_desc}"
                info_text += f" ({confidence:.2f})"
                
                draw.text((10, y_offset), info_text, fill='black')
                y_offset += 20
                
                if y_offset > 450:  # Ïù¥ÎØ∏ÏßÄ Í≤ΩÍ≥Ñ ÎÇ¥ÏóêÏÑú ÌëúÏãú
                    break
            
            # Î∞îÏù¥Ìä∏Î°ú Î≥ÄÌôò
            buffer = io.BytesIO()
            image.save(buffer, format='JPEG', quality=90)
            return buffer.getvalue()
            
        except Exception as e:
            print(f"‚ùå ÎçîÎØ∏ Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±ÎèÑ Ïã§Ìå®: {e}")
            # ÏµúÌõÑÏùò ÏàòÎã®: Í∞ÑÎã®Ìïú Ïò§Î•ò Ïù¥ÎØ∏ÏßÄ
            try:
                image = Image.new('RGB', (320, 240), color='red')
                draw = ImageDraw.Draw(image)
                draw.text((10, 10), "Error", fill='white')
                buffer = io.BytesIO()
                image.save(buffer, format='JPEG', quality=50)
                return buffer.getvalue()
            except:
                raise Exception("Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ± ÏôÑÏ†Ñ Ïã§Ìå®")

    def _get_original_frame(self, video, frame_number):
        """ÏõêÎ≥∏ ÌîÑÎ†àÏûÑ Î∞òÌôò"""
        try:
            import cv2
            import io
            from PIL import Image
            import os
            
            # üîß ÏàòÏ†ï: file.path ÎåÄÏã† file_path ÌïÑÎìú ÏÇ¨Ïö©
            video_path = video.file_path
            
            # ÌååÏùº Í≤ΩÎ°úÍ∞Ä Ï†àÎåÄ Í≤ΩÎ°úÍ∞Ä ÏïÑÎãå Í≤ΩÏö∞ Ï≤òÎ¶¨
            if not os.path.isabs(video_path):
                from django.conf import settings
                video_path = os.path.join(settings.MEDIA_ROOT, video_path)
            
            if not os.path.exists(video_path):
                # ÌååÏùºÏù¥ ÏóÜÏúºÎ©¥ ÎçîÎØ∏ Ïù¥ÎØ∏ÏßÄ Î∞òÌôò
                image = Image.new('RGB', (640, 480), color='lightgray')
                draw = ImageDraw.Draw(image)
                draw.text((10, 10), f"Frame {frame_number} - No Detections", fill='black')
                
                buffer = io.BytesIO()
                image.save(buffer, format='JPEG', quality=90)
                return HttpResponse(buffer.getvalue(), content_type='image/jpeg')
            
            cap = cv2.VideoCapture(video_path)
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number - 1)
            ret, frame = cap.read()
            cap.release()
            
            if ret:
                # OpenCV Ïù¥ÎØ∏ÏßÄÎ•º JPEGÎ°ú Ïù∏ÏΩîÎî©
                _, buffer = cv2.imencode('.jpg', frame)
                return HttpResponse(buffer.tobytes(), content_type='image/jpeg')
            else:
                # ÌîÑÎ†àÏûÑ ÏùΩÍ∏∞ Ïã§Ìå®Ïãú ÎçîÎØ∏ Ïù¥ÎØ∏ÏßÄ
                image = Image.new('RGB', (640, 480), color='lightgray')
                buffer = io.BytesIO()
                image.save(buffer, format='JPEG', quality=90)
                return HttpResponse(buffer.getvalue(), content_type='image/jpeg')
                
        except Exception as e:
            print(f"‚ùå ÏõêÎ≥∏ ÌîÑÎ†àÏûÑ Î°úÎìú Ïã§Ìå®: {e}")
            # ÏµúÌõÑÏùò ÏàòÎã®
            image = Image.new('RGB', (320, 240), color='red')
            buffer = io.BytesIO()
            image.save(buffer, format='JPEG', quality=50)
            return HttpResponse(buffer.getvalue(), content_type='image/jpeg')
# ‚úÖ EnhancedFrameView - Í≥†Í∏â ÌîÑÎ†àÏûÑ Î∑∞  
class EnhancedFrameView(FrameView):
    """Í∏∞Ï°¥ FrameViewÎ•º ÌôïÏû•Ìïú Í≥†Í∏â ÌîÑÎ†àÏûÑ View"""
    
    def get(self, request, video_id, frame_number):
        try:
            # Î∞îÏö¥Îî© Î∞ïÏä§ ÌëúÏãú ÏòµÏÖò ÌôïÏù∏
            show_bbox = request.GET.get('bbox', '').lower() in ['true', '1', 'yes']
            
            if show_bbox:
                # Î∞îÏö¥Îî© Î∞ïÏä§Í∞Ä Ìè¨Ìï®Îêú Ïù¥ÎØ∏ÏßÄ Î∞òÌôò
                bbox_view = FrameWithBboxView()
                return bbox_view.get(request, video_id, frame_number)
            else:
                # Í∏∞Î≥∏ ÌîÑÎ†àÏûÑ Î∞òÌôò
                return super().get(request, video_id, frame_number)
                
        except Exception as e:
            print(f"‚ùå Í≥†Í∏â ÌîÑÎ†àÏûÑ Î∑∞ Ïò§Î•ò: {e}")
            return super().get(request, video_id, frame_number)


from django.db.models import Sum, Count, Avg
from django.utils import timezone
from datetime import timedelta

class CostManagementView(APIView):
    """ÎπÑÏö© Í¥ÄÎ¶¨ Î∞è Î∂ÑÏÑù Î∑∞"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            # Ï†ÑÏ≤¥ ÎπÑÏö© ÌÜµÍ≥Ñ
            total_videos = Video.objects.count()
            analyzed_videos = Video.objects.filter(image_analysis_completed=True).count()
            
            # Ï¥ù ÎπÑÏö© Í≥ÑÏÇ∞
            total_cost = 0
            total_chats = 0
            
            for video in Video.objects.all():
                cost_info = video.get_analysis_cost_summary()
                total_cost += cost_info['estimated_cost']
                total_chats += video.total_chat_count
            
            # ÏµúÍ∑º 7Ïùº ÎπÑÏö© Ï∂îÏù¥
            end_date = timezone.now().date()
            start_date = end_date - timedelta(days=7)
            
            daily_costs = []
            for i in range(7):
                date = start_date + timedelta(days=i)
                daily_analysis = CostAnalysis.get_daily_summary(date)
                daily_costs.append({
                    'date': date.isoformat(),
                    'cost': daily_analysis.estimated_total_cost if daily_analysis else 0.0,
                    'api_calls': daily_analysis.total_api_calls if daily_analysis else 0
                })
            
            # Î™®Îç∏Î≥Ñ ÏÇ¨Ïö©Îüâ ÌÜµÍ≥Ñ
            model_stats = {}
            for video in Video.objects.filter(image_analysis_completed=True):
                models_used = video.api_cost_tracking.get('models_used', [])
                for model in models_used:
                    model_stats[model] = model_stats.get(model, 0) + 1
            
            # Ìö®Ïú®ÏÑ± Î©îÌä∏Î¶≠
            avg_cost_per_video = total_cost / max(analyzed_videos, 1)
            avg_cost_per_chat = total_cost / max(total_chats, 1)
            
            # Ï†àÏïΩ Ï∂îÏ†ï
            without_optimization_cost = analyzed_videos * 0.10  # Îß§Î≤à Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑùÌñàÎã§Î©¥
            current_cost = total_cost
            savings = max(0, without_optimization_cost - current_cost)
            savings_percentage = (savings / without_optimization_cost * 100) if without_optimization_cost > 0 else 0
            
            return Response({
                'summary': {
                    'total_videos': total_videos,
                    'analyzed_videos': analyzed_videos,
                    'total_cost_usd': round(total_cost, 4),
                    'total_chats': total_chats,
                    'avg_cost_per_video': round(avg_cost_per_video, 4),
                    'avg_cost_per_chat': round(avg_cost_per_chat, 4)
                },
                'optimization_impact': {
                    'estimated_savings_usd': round(savings, 4),
                    'savings_percentage': round(savings_percentage, 1),
                    'optimization_strategy': 'first_chat_only_image_analysis'
                },
                'daily_trend': daily_costs,
                'model_usage': model_stats,
                'recommendations': self._get_cost_recommendations(total_cost, analyzed_videos, model_stats)
            })
            
        except Exception as e:
            return Response({'error': str(e)}, status=500)
    
    def _get_cost_recommendations(self, total_cost, analyzed_videos, model_stats):
        """ÎπÑÏö© ÏµúÏ†ÅÌôî Ï∂îÏ≤úÏÇ¨Ìï≠"""
        recommendations = []
        
        if total_cost > 5.0:  # $5 Ïù¥ÏÉÅÏù∏ Í≤ΩÏö∞
            recommendations.append({
                'type': 'cost_alert',
                'message': 'Ï¥ù ÎπÑÏö©Ïù¥ $5Î•º Ï¥àÍ≥ºÌñàÏäµÎãàÎã§. ÏÇ¨Ïö©ÎüâÏùÑ Î™®ÎãàÌÑ∞ÎßÅÌïòÏÑ∏Ïöî.',
                'priority': 'high'
            })
        
        if 'gpt-4' in str(model_stats):
            recommendations.append({
                'type': 'model_optimization',
                'message': 'GPT-4 ÎåÄÏã† GPT-4o-mini ÏÇ¨Ïö©ÏùÑ Í≥†Î†§Ìï¥Î≥¥ÏÑ∏Ïöî. ÎπÑÏö©ÏùÑ 90% Ï†àÏïΩÌï† Ïàò ÏûàÏäµÎãàÎã§.',
                'priority': 'medium'
            })
        
        if analyzed_videos > 50:
            recommendations.append({
                'type': 'usage_optimization',
                'message': 'ÎßéÏùÄ ÎπÑÎîîÏò§Î•º Î∂ÑÏÑùÌñàÏäµÎãàÎã§. RAG ÏãúÏä§ÌÖúÏùÑ ÌôúÏö©Ìï¥ ÎãµÎ≥Ä ÌíàÏßàÏùÑ ÎÜíÏù¥ÏÑ∏Ïöî.',
                'priority': 'low'
            })
        
        return recommendations


class VideoAnalysisStatusView(APIView):
    """ÎπÑÎîîÏò§Î≥Ñ Î∂ÑÏÑù ÏÉÅÌÉú Î∞è ÎπÑÏö© Ï†ïÎ≥¥"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            videos_data = []
            
            for video in Video.objects.all().order_by('-uploaded_at'):
                cost_summary = video.get_analysis_cost_summary()
                
                videos_data.append({
                    'id': video.id,
                    'name': video.original_name,
                    'duration': video.duration,
                    'uploaded_at': video.uploaded_at.isoformat(),
                    'image_analysis_completed': video.image_analysis_completed,
                    'image_analysis_date': video.image_analysis_date.isoformat() if video.image_analysis_date else None,
                    'total_chats': video.total_chat_count,
                    'cost_summary': cost_summary,
                    'has_json_analysis': bool(video.chat_analysis_json_path and os.path.exists(video.chat_analysis_json_path)),
                    'json_path': video.chat_analysis_json_path,
                    'analysis_status': video.analysis_status
                })
            
            return Response({
                'videos': videos_data,
                'total_count': len(videos_data),
                'summary': {
                    'with_image_analysis': sum(1 for v in videos_data if v['image_analysis_completed']),
                    'total_cost': sum(v['cost_summary']['estimated_cost'] for v in videos_data),
                    'total_chats': sum(v['total_chats'] for v in videos_data)
                }
            })
            
        except Exception as e:
            return Response({'error': str(e)}, status=500)


class ResetVideoAnalysisView(APIView):
    """ÎπÑÎîîÏò§ Î∂ÑÏÑù ÏÉÅÌÉú Î¶¨ÏÖã (ÌÖåÏä§Ìä∏/Í¥ÄÎ¶¨Ïö©)"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            reset_costs = request.data.get('reset_costs', False)
            
            if not video_id:
                return Response({'error': 'video_idÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.'}, status=400)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'}, status=404)
            
            # Î∂ÑÏÑù ÏÉÅÌÉú Î¶¨ÏÖã
            video.image_analysis_completed = False
            video.image_analysis_date = None
            
            # JSON ÌååÏùº ÏÇ≠Ï†ú
            if video.chat_analysis_json_path and os.path.exists(video.chat_analysis_json_path):
                try:
                    os.remove(video.chat_analysis_json_path)
                    print(f"‚úÖ JSON ÌååÏùº ÏÇ≠Ï†ú: {video.chat_analysis_json_path}")
                except Exception as e:
                    print(f"‚ö†Ô∏è JSON ÌååÏùº ÏÇ≠Ï†ú Ïã§Ìå®: {e}")
            
            video.chat_analysis_json_path = ''
            
            # ÎπÑÏö© Ï†ïÎ≥¥ Î¶¨ÏÖã (ÏòµÏÖò)
            if reset_costs:
                video.total_chat_count = 0
                video.api_cost_tracking = {}
            
            video.save()
            
            # VideoAnalysisÏóêÏÑú Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Ï†ïÎ≥¥ Ï†úÍ±∞
            if hasattr(video, 'analysis'):
                analysis = video.analysis
                stats = analysis.analysis_statistics
                stats.pop('image_analysis_completed', None)
                stats.pop('image_analysis_date', None)
                stats.pop('json_file_path', None)
                analysis.analysis_statistics = stats
                analysis.save()
            
            return Response({
                'success': True,
                'message': f'ÎπÑÎîîÏò§ "{video.original_name}"Ïùò Î∂ÑÏÑù ÏÉÅÌÉúÍ∞Ä Î¶¨ÏÖãÎêòÏóàÏäµÎãàÎã§.',
                'video_id': video_id,
                'reset_costs': reset_costs
            })
            
        except Exception as e:
            return Response({'error': str(e)}, status=500)


class CostOptimizationTipsView(APIView):
    """ÎπÑÏö© ÏµúÏ†ÅÌôî ÌåÅ Î∞è Í∞ÄÏù¥Îìú"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        return Response({
            'optimization_strategies': {
                'current_implementation': {
                    'name': 'Ï≤´ Ï±ÑÌåÖ Ï†ÑÏö© Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù',
                    'description': 'Ï≤´ Î≤àÏß∏ Ï±ÑÌåÖÏóêÏÑúÎßå Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑùÏùÑ ÏàòÌñâÌïòÍ≥†, Ïù¥ÌõÑ Ï±ÑÌåÖÏùÄ Ï†ÄÏû•Îêú Í≤∞Í≥º ÌôúÏö©',
                    'cost_reduction': '80-90%',
                    'status': 'active'
                },
                'additional_optimizations': [
                    {
                        'name': 'Î™®Îç∏ ÏÑ†ÌÉù ÏµúÏ†ÅÌôî',
                        'description': 'GPT-4 ÎåÄÏã† GPT-4o-mini ÏÇ¨Ïö© (ÏÑ±Îä• 90% Ïú†ÏßÄ, ÎπÑÏö© 90% Ï†àÏïΩ)',
                        'impact': 'high'
                    },
                    {
                        'name': 'Ïù¥ÎØ∏ÏßÄ ÌíàÏßà Ï°∞Ï†ï',
                        'description': 'Ïù¥ÎØ∏ÏßÄ Ìï¥ÏÉÅÎèÑÎ•º 800pxÎ°ú Ï†úÌïúÌïòÍ≥† JPEG ÌíàÏßàÏùÑ 70%Î°ú ÏÑ§Ï†ï',
                        'impact': 'medium'
                    },
                    {
                        'name': 'RAG ÏãúÏä§ÌÖú ÌôúÏö©',
                        'description': 'Ï†ÄÏû•Îêú Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Î≤°ÌÑ∞ DBÏóê Ï†ÄÏû•ÌïòÏó¨ Í≤ÄÏÉâ ÌíàÏßà Ìñ•ÏÉÅ',
                        'impact': 'medium'
                    },
                    {
                        'name': 'Î∞∞Ïπò Ï≤òÎ¶¨',
                        'description': 'Ïó¨Îü¨ ÎπÑÎîîÏò§Î•º Ìïú Î≤àÏóê Ï≤òÎ¶¨ÌïòÏó¨ API Ìò∏Ï∂ú ÏµúÏ†ÅÌôî',
                        'impact': 'low'
                    }
                ]
            },
            'cost_estimation': {
                'without_optimization': {
                    'per_chat': '$0.05-0.15',
                    'description': 'Îß§ Ï±ÑÌåÖÎßàÎã§ Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù ÏàòÌñâ'
                },
                'with_optimization': {
                    'first_chat': '$0.05-0.15', 
                    'subsequent_chats': '$0.001-0.005',
                    'description': 'Ï≤´ Ï±ÑÌåÖÎßå Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù, Ïù¥ÌõÑÎäî ÌÖçÏä§Ìä∏Îßå'
                }
            },
            'monitoring_tips': [
                'ÏùºÏùº API ÏÇ¨Ïö©Îüâ Î™®ÎãàÌÑ∞ÎßÅ',
                'Î™®Îç∏Î≥Ñ ÎπÑÏö© Ìö®Ïú®ÏÑ± Ï∂îÏ†Å',
                'ÏÇ¨Ïö©ÏûêÎ≥Ñ Ï±ÑÌåÖ Ìå®ÌÑ¥ Î∂ÑÏÑù',
                'ÏõîÎ≥Ñ ÎπÑÏö© ÏòàÏÇ∞ ÏÑ§Ï†ï'
            ]
        })

from django.core.management.base import BaseCommand
from django.utils import timezone
from datetime import timedelta
from chat.models import Video, CostAnalysis

class Command(BaseCommand):
    help = 'ÎπÑÏö© Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Î∞è ÏóÖÎç∞Ïù¥Ìä∏'
    
    def add_arguments(self, parser):
        parser.add_argument('--days', type=int, default=7, help='Î∂ÑÏÑùÌï† ÏùºÏàò')
        parser.add_argument('--update-monthly', action='store_true', help='ÏõîÎ≥Ñ ÏßëÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏')
    
    def handle(self, *args, **options):
        days = options['days']
        end_date = timezone.now().date()
        start_date = end_date - timedelta(days=days)
        
        self.stdout.write(f"üìä {days}ÏùºÍ∞Ñ ÎπÑÏö© Î∂ÑÏÑù Ï§ë...")
        
        for i in range(days):
            date = start_date + timedelta(days=i)
            
            # Ìï¥Îãπ ÎÇ†ÏßúÏùò ÎπÑÎîîÏò§Îì§ Î∂ÑÏÑù
            videos_on_date = Video.objects.filter(
                image_analysis_date__date=date
            )
            
            if videos_on_date.exists():
                total_cost = sum(
                    video.get_analysis_cost_summary()['estimated_cost'] 
                    for video in videos_on_date
                )
                
                total_calls = sum(
                    video.api_cost_tracking.get('total_api_calls', 0)
                    for video in videos_on_date
                )
                
                image_calls = sum(
                    video.api_cost_tracking.get('image_analysis_calls', 0)
                    for video in videos_on_date
                )
                
                # CostAnalysis Î†àÏΩîÎìú ÏÉùÏÑ±/ÏóÖÎç∞Ïù¥Ìä∏
                cost_analysis, created = CostAnalysis.objects.get_or_create(
                    date=date,
                    period_type='daily',
                    defaults={
                        'total_api_calls': total_calls,
                        'image_analysis_calls': image_calls,
                        'text_only_calls': total_calls - image_calls,
                        'estimated_total_cost': total_cost
                    }
                )
                
                if not created:
                    cost_analysis.total_api_calls = total_calls
                    cost_analysis.image_analysis_calls = image_calls
                    cost_analysis.text_only_calls = total_calls - image_calls
                    cost_analysis.estimated_total_cost = total_cost
                    cost_analysis.save()
                
                status = "ÏÉùÏÑ±Îê®" if created else "ÏóÖÎç∞Ïù¥Ìä∏Îê®"
                self.stdout.write(f"  {date}: ${total_cost:.4f} ({total_calls}Ìöå Ìò∏Ï∂ú) - {status}")
        
        self.stdout.write(self.style.SUCCESS("‚úÖ ÎπÑÏö© Î∂ÑÏÑù ÏôÑÎ£å"))


# chat/views.py - ÎπÑÏö© Ï†àÏïΩÌòï ÎπÑÎîîÏò§ Ï±ÑÌåÖ ÏãúÏä§ÌÖú

import threading
import time
import json
import cv2
import os
import base64
from datetime import datetime
from django.conf import settings
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient
from .video_analyzer import get_video_analyzer
from .multi_llm_service import get_multi_llm_analyzer
from .db_builder import get_video_rag_system

class CostEffectiveVideoChatView(APIView):
    """ÎπÑÏö© Ï†àÏïΩÌòï ÎπÑÎîîÏò§ Ï±ÑÌåÖ - Ï≤´ Ï±ÑÌåÖÏóêÎßå Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù, Ïù¥ÌõÑ JSON Í∏∞Î∞ò ÎãµÎ≥Ä"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        self.video_analyzer = get_video_analyzer()
        self.llm_client = LLMClient()
        self.multi_llm_analyzer = get_multi_llm_analyzer()
        self.rag_system = get_video_rag_system()
    
    def post(self, request):
        try:
            user_query = request.data.get('message', '').strip()
            video_id = request.data.get('video_id')
            analysis_mode = request.data.get('analysis_mode', 'single')
            use_multi_llm = request.data.get('use_multi_llm', analysis_mode != 'single')
            
            print(f"ü§ñ Ï±ÑÌåÖ ÏöîÏ≤≠: '{user_query}' (ÎπÑÎîîÏò§: {video_id}, Î™®Îìú: {analysis_mode})")
            
            if not user_query:
                return Response({'error': 'Î©îÏãúÏßÄÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'}, status=400)
            
            if not video_id:
                return Response({'error': 'ÎπÑÎîîÏò§ IDÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.'}, status=400)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'}, status=404)
            
            # üî• ÌïµÏã¨: Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Ïù¥Î†• ÌôïÏù∏
            image_analysis_status = self._check_image_analysis_status(video)
            
            if image_analysis_status['needs_analysis']:
                print("üñºÔ∏è Ï≤´ Î≤àÏß∏ Ï±ÑÌåÖ - Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù ÏàòÌñâ")
                response = self._handle_first_chat_with_analysis(
                    user_query, video, analysis_mode, use_multi_llm
                )
            else:
                print("üìÑ Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù ÏôÑÎ£åÎê® - JSON Í∏∞Î∞ò ÎãµÎ≥Ä")
                response = self._handle_subsequent_chat_with_json(
                    user_query, video, image_analysis_status['json_path']
                )
            
            # Ï±ÑÌåÖ Ïù¥Î†• Ï†ÄÏû•
            self._save_chat_history(video, user_query, response.get('response', ''))
            
            return Response(response)
            
        except Exception as e:
            print(f"‚ùå Ï±ÑÌåÖ Ï≤òÎ¶¨ Ïò§Î•ò: {e}")
            import traceback
            print(f"üîç ÏÉÅÏÑ∏ Ïò§Î•ò:\n{traceback.format_exc()}")
            return Response({
                'error': f'Ï±ÑÌåÖ Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {str(e)}',
                'response': 'Ï£ÑÏÜ°Ìï©ÎãàÎã§. ÌòÑÏû¨ ÏÑúÎπÑÏä§Ïóê Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.',
                'response_type': 'error'
            }, status=500)
    
    def _check_image_analysis_status(self, video):
        """Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù ÏàòÌñâ Ïó¨Î∂Ä ÌôïÏù∏"""
        try:
            # 1. Video Î™®Îç∏Ïùò image_analysis_completed ÌïÑÎìú ÌôïÏù∏
            if video.image_analysis_completed:
                json_path = video.chat_analysis_json_path
                if json_path and os.path.exists(json_path):
                    return {
                        'needs_analysis': False,
                        'json_path': json_path,
                        'analysis_date': video.image_analysis_date
                    }
            
            # 2. VideoAnalysis Î™®Îç∏ÏóêÏÑú Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Ïó¨Î∂Ä ÌôïÏù∏
            if hasattr(video, 'analysis'):
                analysis = video.analysis
                stats = analysis.analysis_statistics
                
                # Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑùÏù¥ ÏàòÌñâÎêòÏóàÎäîÏßÄ ÌôïÏù∏
                if stats.get('image_analysis_completed', False):
                    json_path = stats.get('json_file_path')
                    if json_path and os.path.exists(json_path):
                        return {
                            'needs_analysis': False,
                            'json_path': json_path,
                            'analysis_date': analysis.created_at
                        }
            
            # 3. JSON ÌååÏùº Ï°¥Ïû¨ Ïó¨Î∂ÄÎ°ú ÌôïÏù∏
            analysis_results_dir = os.path.join(settings.MEDIA_ROOT, 'analysis_results')
            pattern = f"chat_analysis_{video.id}_*.json"
            
            if os.path.exists(analysis_results_dir):
                import glob
                existing_files = glob.glob(os.path.join(analysis_results_dir, pattern))
                
                if existing_files:
                    # Í∞ÄÏû• ÏµúÍ∑º ÌååÏùº ÏÇ¨Ïö©
                    latest_file = max(existing_files, key=os.path.getmtime)
                    return {
                        'needs_analysis': False,
                        'json_path': latest_file,
                        'analysis_date': datetime.fromtimestamp(os.path.getmtime(latest_file))
                    }
            
            # 4. Î∂ÑÏÑùÏù¥ ÌïÑÏöîÌïú Í≤ΩÏö∞
            return {
                'needs_analysis': True,
                'json_path': None,
                'analysis_date': None
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù ÏÉÅÌÉú ÌôïÏù∏ Ïã§Ìå®: {e}")
            return {'needs_analysis': True, 'json_path': None, 'analysis_date': None}
# chat/views.py - ÎπÑÏö© Ï†àÏïΩÌòï Ï±ÑÌåÖ Î∑∞ Ï∂îÍ∞Ä

import threading
import time
import json
import cv2
import os
import base64
from datetime import datetime
from django.conf import settings
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient
from .video_analyzer import get_video_analyzer
from .multi_llm_service import get_multi_llm_analyzer

class CostEfficientChatView(APIView):
    """ÎπÑÏö© Ï†àÏïΩÌòï ÎπÑÎîîÏò§ Ï±ÑÌåÖ - Ï≤´ Ï±ÑÌåÖÏóêÎßå Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù, Ïù¥ÌõÑ JSON Í∏∞Î∞ò ÎãµÎ≥Ä"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        self.video_analyzer = get_video_analyzer()
        self.llm_client = LLMClient()
        self.multi_llm_analyzer = get_multi_llm_analyzer()
        
    def post(self, request):
        try:
            user_query = request.data.get('message', '').strip()
            video_id = request.data.get('video_id')
            analysis_mode = request.data.get('analysis_mode', 'single')
            use_multi_llm = request.data.get('use_multi_llm', analysis_mode != 'single')
            
            print(f"üí∞ ÎπÑÏö©Ï†àÏïΩ Ï±ÑÌåÖ ÏöîÏ≤≠: '{user_query}' (ÎπÑÎîîÏò§: {video_id}, Î™®Îìú: {analysis_mode})")
            
            if not user_query:
                return Response({'error': 'Î©îÏãúÏßÄÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'}, status=400)
            
            if not video_id:
                return Response({'error': 'ÎπÑÎîîÏò§ IDÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.'}, status=400)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'}, status=404)
            
            # Ï±ÑÌåÖ Ïπ¥Ïö¥Ìä∏ Ï¶ùÍ∞Ä
            video.increment_chat_count()
            
            # üî• ÌïµÏã¨: Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Ïù¥Î†• ÌôïÏù∏
            if not video.image_analysis_completed:
                print("üñºÔ∏è Ï≤´ Î≤àÏß∏ Ï±ÑÌåÖ - Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù ÏàòÌñâ")
                response = self._handle_first_chat_with_analysis(
                    user_query, video, analysis_mode, use_multi_llm
                )
            else:
                print("üìÑ Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù ÏôÑÎ£åÎê® - JSON Í∏∞Î∞ò ÎãµÎ≥Ä")
                response = self._handle_subsequent_chat_with_json(
                    user_query, video
                )
            
            # Ï±ÑÌåÖ Ïù¥Î†• Ï†ÄÏû•
            self._save_chat_history(video, user_query, response.get('response', ''))
            
            return Response(response)
            
        except Exception as e:
            print(f"‚ùå ÎπÑÏö©Ï†àÏïΩ Ï±ÑÌåÖ Ï≤òÎ¶¨ Ïò§Î•ò: {e}")
            import traceback
            print(f"üîç ÏÉÅÏÑ∏ Ïò§Î•ò:\n{traceback.format_exc()}")
            return Response({
                'error': f'Ï±ÑÌåÖ Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {str(e)}',
                'response': 'Ï£ÑÏÜ°Ìï©ÎãàÎã§. ÌòÑÏû¨ ÏÑúÎπÑÏä§Ïóê Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.',
                'response_type': 'error'
            }, status=500)
    
    def _handle_first_chat_with_analysis(self, user_query, video, analysis_mode, use_multi_llm):
        """Ï≤´ Î≤àÏß∏ Ï±ÑÌåÖ - Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Ìè¨Ìï®"""
        try:
            print("üñºÔ∏è Ï≤´ Î≤àÏß∏ Ï±ÑÌåÖÏùÑ ÏúÑÌïú Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù ÏàòÌñâ Ï§ë...")
            
            # ÌÇ§ÌîÑÎ†àÏûÑ Ï∂îÏ∂ú (ÎπÑÏö© Ï†àÏïΩÏùÑ ÏúÑÌï¥ 2-3Í∞úÎßå)
            frame_images = self._extract_key_frames_for_llm(video, max_frames=2)
            
            if not frame_images:
                print("‚ö†Ô∏è ÌÇ§ÌîÑÎ†àÏûÑ Ï∂îÏ∂ú Ïã§Ìå® - Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞Î°ú ÎãµÎ≥Ä")
                return self._handle_fallback_response(user_query, video)
            
            # ÎπÑÎîîÏò§ Ïª®ÌÖçÏä§Ìä∏ Ï§ÄÎπÑ
            video_context = self._prepare_video_context(video)
            
            # Î©ÄÌã∞ LLM Î∂ÑÏÑù ÏàòÌñâ
            if use_multi_llm and analysis_mode in ['multi', 'comparison']:
                multi_responses = self.multi_llm_analyzer.analyze_video_multi_llm(
                    frame_images, user_query, video_context
                )
                comparison_result = self.multi_llm_analyzer.compare_responses(multi_responses)
                
                # Î∂ÑÏÑù Í≤∞Í≥ºÎ•º JSONÏúºÎ°ú Ï†ÄÏû•
                analysis_result = {
                    'video_id': video.id,
                    'query': user_query,
                    'analysis_type': 'multi_llm_image_analysis',
                    'frame_count': len(frame_images),
                    'llm_responses': {
                        model: {
                            'response': resp.response_text,
                            'confidence': resp.confidence_score,
                            'processing_time': resp.processing_time,
                            'success': resp.success
                        }
                        for model, resp in multi_responses.items()
                    },
                    'comparison_analysis': comparison_result['comparison'],
                    'timestamp': datetime.now().isoformat(),
                    'video_context': video_context
                }
                
                # JSON ÌååÏùº Ï†ÄÏû•
                json_path = self._save_analysis_result(video, analysis_result)
                
                # ÎπÑÎîîÏò§Ïóê Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù ÏôÑÎ£å ÌëúÏãú
                video.mark_image_analysis_completed(json_path)
                
                # ÎπÑÏö© Ï∂îÏ†Å
                estimated_cost = self._calculate_analysis_cost(analysis_result)
                video.update_cost_tracking('image_analysis', estimated_cost, 'multi_llm')
                
                if analysis_mode == 'comparison':
                    return {
                        'response_type': 'first_chat_multi_llm_comparison',
                        'query': user_query,
                        'video_info': {'id': video.id, 'name': video.original_name},
                        'llm_responses': analysis_result['llm_responses'],
                        'comparison_analysis': analysis_result['comparison_analysis'],
                        'recommendation': comparison_result['comparison']['recommendation'],
                        'cost_info': {
                            'estimated_cost': estimated_cost,
                            'optimization_enabled': True,
                            'future_chats_cost': 'text_only (~$0.001)'
                        }
                    }
                else:
                    best_model = comparison_result['comparison']['best_response']
                    best_response = multi_responses.get(best_model)
                    
                    return {
                        'response_type': 'first_chat_multi_llm_optimized',
                        'response': best_response.response_text if best_response and best_response.success else "Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§.",
                        'query': user_query,
                        'video_info': {'id': video.id, 'name': video.original_name},
                        'selected_model': best_model,
                        'confidence': best_response.confidence_score if best_response else 0,
                        'models_used': list(multi_responses.keys()),
                        'cost_info': {
                            'estimated_cost': estimated_cost,
                            'optimization_enabled': True
                        }
                    }
            else:
                # Îã®Ïùº LLM Î∂ÑÏÑù
                single_response = self._analyze_with_single_llm(frame_images[0], user_query, video_context)
                
                analysis_result = {
                    'video_id': video.id,
                    'query': user_query,
                    'analysis_type': 'single_llm_image_analysis', 
                    'frame_count': len(frame_images),
                    'response': single_response,
                    'timestamp': datetime.now().isoformat(),
                    'video_context': video_context
                }
                
                # JSON ÌååÏùº Ï†ÄÏû•
                json_path = self._save_analysis_result(video, analysis_result)
                
                # ÎπÑÎîîÏò§Ïóê Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù ÏôÑÎ£å ÌëúÏãú
                video.mark_image_analysis_completed(json_path)
                
                # ÎπÑÏö© Ï∂îÏ†Å
                estimated_cost = self._calculate_analysis_cost(analysis_result)
                video.update_cost_tracking('image_analysis', estimated_cost, 'single_llm')
                
                return {
                    'response_type': 'first_chat_single_llm',
                    'response': single_response,
                    'query': user_query,
                    'video_info': {'id': video.id, 'name': video.original_name},
                    'cost_info': {
                        'estimated_cost': estimated_cost,
                        'optimization_enabled': True
                    }
                }
                
        except Exception as e:
            print(f"‚ùå Ï≤´ Î≤àÏß∏ Ï±ÑÌåÖ Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return self._handle_fallback_response(user_query, video)
    
    def _handle_subsequent_chat_with_json(self, user_query, video):
        """Ïù¥ÌõÑ Ï±ÑÌåÖ - JSON ÌååÏùº Í∏∞Î∞ò ÎãµÎ≥Ä"""
        try:
            print(f"üìÑ JSON Í∏∞Î∞ò ÎãµÎ≥Ä ÏÉùÏÑ±: {video.chat_analysis_json_path}")
            
            # JSON ÌååÏùºÏóêÏÑú Ïù¥Ï†Ñ Î∂ÑÏÑù Í≤∞Í≥º Î°úÎìú
            if not video.chat_analysis_json_path or not os.path.exists(video.chat_analysis_json_path):
                print("‚ö†Ô∏è JSON ÌååÏùºÏù¥ ÏóÜÏùå - ÏùºÎ∞ò Ï±ÑÌåÖÏúºÎ°ú Ï≤òÎ¶¨")
                response_text = self.llm_client.generate_smart_response(
                    user_query=user_query,
                    search_results=None,
                    video_info=f"ÎπÑÎîîÏò§: {video.original_name}",
                    use_multi_llm=False
                )
                
                # ÌÖçÏä§Ìä∏ Ï†ÑÏö© ÎπÑÏö© Ï∂îÏ†Å
                estimated_cost = 0.002  # ÌÖçÏä§Ìä∏ Ï†ÑÏö© ÎπÑÏö© (Îß§Ïö∞ Ï†ÄÎ†¥)
                video.update_cost_tracking('text_only', estimated_cost)
                
                return {
                    'response_type': 'text_only_fallback',
                    'response': response_text,
                    'query': user_query,
                    'video_info': {'id': video.id, 'name': video.original_name},
                    'cost_info': {
                        'estimated_cost': estimated_cost,
                        'note': 'JSON ÌååÏùº ÏóÜÏùå - ÌÖçÏä§Ìä∏ Ï†ÑÏö© Ï≤òÎ¶¨'
                    }
                }
            
            # JSON ÌååÏùº Î°úÎìú
            with open(video.chat_analysis_json_path, 'r', encoding='utf-8') as f:
                analysis_data = json.load(f)
            
            # Ï†ÄÏû•Îêú Î∂ÑÏÑù Í≤∞Í≥º ÌôúÏö©
            if analysis_data.get('analysis_type') == 'multi_llm_image_analysis':
                # Ïù¥Ï†Ñ Î©ÄÌã∞ LLM Í≤∞Í≥º ÌôúÏö©
                response_text = self._generate_contextual_response_from_json(
                    user_query, analysis_data, video
                )
            else:
                # Îã®Ïùº LLM Í≤∞Í≥º ÌôúÏö©
                response_text = self._generate_simple_response_from_json(
                    user_query, analysis_data, video
                )
            
            # ÌÖçÏä§Ìä∏ Ï†ÑÏö© ÎπÑÏö© Ï∂îÏ†Å
            estimated_cost = 0.001  # Îß§Ïö∞ Ï†ÄÎ†¥Ìïú ÌÖçÏä§Ìä∏ Ï†ÑÏö© ÎπÑÏö©
            video.update_cost_tracking('text_only', estimated_cost)
            
            return {
                'response_type': 'json_based_optimized',
                'response': response_text,
                'query': user_query,
                'video_info': {'id': video.id, 'name': video.original_name},
                'cost_info': {
                    'estimated_cost': estimated_cost,
                    'data_source': 'saved_analysis',
                    'optimization_savings': '~95% cost reduction'
                }
            }
            
        except Exception as e:
            print(f"‚ùå JSON Í∏∞Î∞ò ÎãµÎ≥Ä ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return self._handle_fallback_response(user_query, video)
        


from django.http import JsonResponse, HttpResponse
from django.views import View
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
import json
import os

class AnalysisStatusView(APIView):
    """Î∂ÑÏÑù ÏÉÅÌÉú ÏÉÅÏÑ∏ Ï°∞Ìöå"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            # ÏµúÏã† Î∂ÑÏÑù Ï†ïÎ≥¥ Ï°∞Ìöå
            latest_analysis = VideoAnalysis.objects.filter(video=video).order_by('-id').first()
            
            response_data = {
                'video_id': video.id,
                'video_name': video.original_name,
                'analysis_status': video.analysis_status,
                'is_analyzed': video.is_analyzed,
                'analysis_progress': 100 if video.analysis_status == 'completed' else 
                                   (50 if video.analysis_status == 'processing' else 0),
                'video_info': {
                    'duration': getattr(video, 'duration', 0),
                    'total_frames': getattr(video, 'total_frames', 0),
                    'fps': getattr(video, 'fps', 0),
                    'width': getattr(video, 'width', 0),
                    'height': getattr(video, 'height', 0)
                }
            }
            
            if latest_analysis:
                response_data['latest_analysis'] = {
                    'id': latest_analysis.id,
                    'enhanced_analysis': latest_analysis.enhanced_analysis,
                    'success_rate': latest_analysis.success_rate,
                    'processing_time_seconds': latest_analysis.processing_time_seconds,
                    'frames_analyzed': latest_analysis.analysis_statistics.get('total_frames_analyzed', 0),
                    'dominant_objects': latest_analysis.analysis_statistics.get('dominant_objects', []),
                    'ai_features_used': latest_analysis.analysis_statistics.get('ai_features_used', {}),
                    'json_file_path': latest_analysis.analysis_statistics.get('json_file_path', ''),
                    'created_at': latest_analysis.created_at.isoformat() if hasattr(latest_analysis, 'created_at') else None
                }
            
            # ÌîÑÎ†àÏûÑ Î∞è Ïî¨ Í∞úÏàò
            frame_count = Frame.objects.filter(video=video).count()
            scene_count = Scene.objects.filter(video=video).count()
            
            response_data['analysis_counts'] = {
                'frames': frame_count,
                'scenes': scene_count
            }
            
            return Response(response_data)
            
        except Video.DoesNotExist:
            return Response({
                'error': 'Ìï¥Îãπ ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'Î∂ÑÏÑù ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class AnalysisResultsView(APIView):
    """Î∂ÑÏÑù Í≤∞Í≥º ÏÉÅÏÑ∏ Ï°∞Ìöå"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            if not video.is_analyzed:
                return Response({
                    'error': 'ÏïÑÏßÅ Î∂ÑÏÑùÏù¥ ÏôÑÎ£åÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.',
                    'analysis_status': video.analysis_status
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # Î∂ÑÏÑù Í≤∞Í≥º Ï°∞Ìöå
            latest_analysis = VideoAnalysis.objects.filter(video=video).order_by('-id').first()
            frames = Frame.objects.filter(video=video).order_by('timestamp')[:20]  # ÏµúÎåÄ 20Í∞ú
            scenes = Scene.objects.filter(video=video).order_by('scene_id')
            
            response_data = {
                'video_id': video.id,
                'video_name': video.original_name,
                'analysis_summary': {},
                'sample_frames': [],
                'scenes': [],
                'json_file_available': False,
                'json_file_path': None
            }
            
            if latest_analysis:
                response_data['analysis_summary'] = {
                    'success_rate': latest_analysis.success_rate,
                    'processing_time': latest_analysis.processing_time_seconds,
                    'total_frames_analyzed': latest_analysis.analysis_statistics.get('total_frames_analyzed', 0),
                    'dominant_objects': latest_analysis.analysis_statistics.get('dominant_objects', []),
                    'scene_types': latest_analysis.analysis_statistics.get('scene_types', []),
                    'text_extracted': latest_analysis.analysis_statistics.get('text_extracted', False),
                    'ai_features_used': latest_analysis.analysis_statistics.get('ai_features_used', {}),
                    'analysis_quality_metrics': latest_analysis.analysis_statistics.get('analysis_quality_metrics', {}),
                    'caption_statistics': {
                        'frames_with_caption': latest_analysis.caption_statistics.get('frames_with_caption', 0),
                        'enhanced_captions': latest_analysis.caption_statistics.get('enhanced_captions', 0),
                        'average_confidence': latest_analysis.caption_statistics.get('average_confidence', 0)
                    }
                }
                
                # JSON ÌååÏùº Í≤ΩÎ°ú ÌôïÏù∏
                json_file_path = latest_analysis.analysis_statistics.get('json_file_path', '')
                if json_file_path and os.path.exists(json_file_path):
                    response_data['json_file_available'] = True
                    response_data['json_file_path'] = json_file_path
            
            # ÏÉòÌîå ÌîÑÎ†àÏûÑÎì§
            for frame in frames:
                response_data['sample_frames'].append({
                    'frame_id': frame.image_id,
                    'timestamp': frame.timestamp,
                    'caption': frame.final_caption or frame.enhanced_caption or frame.caption,
                    'object_count': len(frame.detected_objects),
                    'detected_objects': [obj.get('class', '') for obj in frame.detected_objects[:5]]  # ÏµúÎåÄ 5Í∞úÎßå
                })
            
            # Ïî¨ Ï†ïÎ≥¥
            for scene in scenes:
                response_data['scenes'].append({
                    'scene_id': scene.scene_id,
                    'start_time': scene.start_time,
                    'end_time': scene.end_time,
                    'duration': scene.duration,
                    'frame_count': scene.frame_count,
                    'dominant_objects': scene.dominant_objects[:3]  # ÏµúÎåÄ 3Í∞úÎßå
                })
            
            return Response(response_data)
            
        except Video.DoesNotExist:
            return Response({
                'error': 'Ìï¥Îãπ ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'Î∂ÑÏÑù Í≤∞Í≥º Ï°∞Ìöå Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class AnalyzerSystemStatusView(APIView):
    """AI Î∂ÑÏÑù ÏãúÏä§ÌÖú Ï†ÑÏ≤¥ ÏÉÅÌÉú Ï°∞Ìöå"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            if not VIDEO_ANALYZER_AVAILABLE:
                return Response({
                    'system_status': 'unavailable',
                    'error': 'video_analyzer Î™®ÎìàÏùÑ importÌï† Ïàò ÏóÜÏäµÎãàÎã§',
                    'available_features': {},
                    'recommendation': 'video_analyzer.py ÌååÏùºÍ≥º ÏùòÏ°¥ÏÑ±Îì§ÏùÑ ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî'
                })
            
            # Î∂ÑÏÑùÍ∏∞ ÏÉÅÌÉú Ï°∞Ìöå
            analyzer_status = get_analyzer_status()
            
            # RAG ÏãúÏä§ÌÖú ÏÉÅÌÉú Ï°∞Ìöå
            try:
                rag_system = get_video_rag_system()
                rag_info = rag_system.get_database_info() if rag_system else None
                rag_available = rag_system is not None
            except:
                rag_info = None
                rag_available = False
            
            # ÏãúÏä§ÌÖú ÌÜµÍ≥Ñ
            total_videos = Video.objects.count()
            analyzed_videos = Video.objects.filter(is_analyzed=True).count()
            processing_videos = Video.objects.filter(analysis_status='processing').count()
            
            response_data = {
                'system_status': 'operational' if analyzer_status.get('status') == 'initialized' else 'limited',
                'analyzer': analyzer_status,
                'rag_system': {
                    'available': rag_available,
                    'info': rag_info
                },
                'statistics': {
                    'total_videos': total_videos,
                    'analyzed_videos': analyzed_videos,
                    'processing_videos': processing_videos,
                    'analysis_rate': (analyzed_videos / max(total_videos, 1)) * 100
                },
                'capabilities': {
                    'yolo_object_detection': analyzer_status.get('features', {}).get('yolo', False),
                    'clip_scene_analysis': analyzer_status.get('features', {}).get('clip', False),
                    'ocr_text_extraction': analyzer_status.get('features', {}).get('ocr', False),
                    'vqa_question_answering': analyzer_status.get('features', {}).get('vqa', False),
                    'scene_graph_generation': analyzer_status.get('features', {}).get('scene_graph', False),
                    'rag_search_system': rag_available
                },
                'device': analyzer_status.get('device', 'unknown'),
                'last_checked': datetime.now().isoformat()
            }
            
            return Response(response_data)
            
        except Exception as e:
            return Response({
                'system_status': 'error',
                'error': f'ÏãúÏä§ÌÖú ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå®: {str(e)}',
                'last_checked': datetime.now().isoformat()
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class DownloadAnalysisResultView(APIView):
    """Î∂ÑÏÑù Í≤∞Í≥º JSON ÌååÏùº Îã§Ïö¥Î°úÎìú"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            latest_analysis = VideoAnalysis.objects.filter(video=video).order_by('-id').first()
            
            if not latest_analysis:
                return Response({
                    'error': 'Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
                }, status=status.HTTP_404_NOT_FOUND)
            
            json_file_path = latest_analysis.analysis_statistics.get('json_file_path', '')
            
            if not json_file_path or not os.path.exists(json_file_path):
                return Response({
                    'error': 'JSON ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
                }, status=status.HTTP_404_NOT_FOUND)
            
            # JSON ÌååÏùº ÏùΩÍ∏∞
            with open(json_file_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            
            # HTTP ÏùëÎãµÏúºÎ°ú JSON Î∞òÌôò
            response = HttpResponse(
                json.dumps(json_data, ensure_ascii=False, indent=2),
                content_type='application/json; charset=utf-8'
            )
            response['Content-Disposition'] = f'attachment; filename="analysis_{video.id}_{video.original_name}.json"'
            
            return response
            
        except Video.DoesNotExist:
            return Response({
                'error': 'Ìï¥Îãπ ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'JSON Îã§Ïö¥Î°úÎìú Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# ========================================
# Î∂ÑÏÑù Ï∑®ÏÜå Î∞è Í¥ÄÎ¶¨ Í∏∞Îä•
# ========================================

class CancelAnalysisView(APIView):
    """Î∂ÑÏÑù Ï∑®ÏÜå"""
    permission_classes = [AllowAny]
    
    def post(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            if video.analysis_status != 'processing':
                return Response({
                    'error': 'ÏßÑÌñâ Ï§ëÏù∏ Î∂ÑÏÑùÏù¥ ÏóÜÏäµÎãàÎã§.',
                    'current_status': video.analysis_status
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # ÏÉÅÌÉúÎ•º cancelledÎ°ú Î≥ÄÍ≤Ω
            video.analysis_status = 'cancelled'
            video.save()
            
            return Response({
                'success': True,
                'message': 'Î∂ÑÏÑùÏù¥ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.',
                'video_id': video.id,
                'new_status': 'cancelled'
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'Ìï¥Îãπ ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'Î∂ÑÏÑù Ï∑®ÏÜå Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class RestartAnalysisView(APIView):
    """Î∂ÑÏÑù Ïû¨ÏãúÏûë"""
    permission_classes = [AllowAny]
    
    def post(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            if video.analysis_status == 'processing':
                return Response({
                    'error': 'Ïù¥ÎØ∏ Î∂ÑÏÑùÏù¥ ÏßÑÌñâ Ï§ëÏûÖÎãàÎã§.',
                    'current_status': video.analysis_status
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # Í∏∞Ï°¥ Î∂ÑÏÑù Í≤∞Í≥º ÏÇ≠Ï†ú (ÏÑ†ÌÉùÏÇ¨Ìï≠)
            cleanup = request.data.get('cleanup_previous', False)
            if cleanup:
                VideoAnalysis.objects.filter(video=video).delete()
                Frame.objects.filter(video=video).delete()
                Scene.objects.filter(video=video).delete()
            
            # Î∂ÑÏÑù ÏÉÅÌÉú Ï¥àÍ∏∞Ìôî
            video.analysis_status = 'pending'
            video.is_analyzed = False
            video.save()
            
            return Response({
                'success': True,
                'message': 'Î∂ÑÏÑùÏù¥ Ïû¨ÏãúÏûë Ï§ÄÎπÑÎêòÏóàÏäµÎãàÎã§. Î∂ÑÏÑùÏùÑ Îã§Ïãú ÏöîÏ≤≠Ìï¥Ï£ºÏÑ∏Ïöî.',
                'video_id': video.id,
                'new_status': 'pending',
                'cleanup_performed': cleanup
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'Ìï¥Îãπ ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'Î∂ÑÏÑù Ïû¨ÏãúÏûë Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# chat/views.py - EnhancedVideoChatView Í∞úÏÑ† Î≤ÑÏ†Ñ

import json
import os
import re
import time
from collections import Counter
from datetime import datetime
from typing import List, Dict, Any, Optional
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from django.conf import settings

from .models import Video, VideoAnalysis, SearchHistory
from .llm_client import LLMClient
from .multi_llm_service import get_multi_llm_analyzer
from .db_builder import get_video_rag_system

class SmartJSONParser:
    """Í≥†ÏÑ±Îä• JSON ÌååÏã± Î∞è person Í∞ùÏ≤¥ Ï∂îÏ∂ú ÏµúÏ†ÅÌôî"""
    
    @staticmethod
    def extract_person_info_optimized(json_data: Dict) -> Dict[str, Any]:
        """ÏµúÏ†ÅÌôîÎêú person Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        person_analysis = {
            'total_person_detections': 0,
            'unique_persons_estimated': 0,
            'frames_with_people': [],
            'person_tracking_data': {},
            'confidence_scores': [],
            'gender_analysis': {'male': 0, 'female': 0, 'unknown': 0},
            'temporal_consistency': 0.0
        }
        
        frame_results = json_data.get('frame_results', [])
        if not frame_results:
            return person_analysis
        
        # ÌîÑÎ†àÏûÑÎ≥Ñ person Ï∂îÏ†Å
        for frame_data in frame_results:
            frame_id = frame_data.get('image_id', 0)
            timestamp = frame_data.get('timestamp', 0)
            objects = frame_data.get('objects', [])
            
            frame_persons = []
            for obj in objects:
                if obj.get('class') == 'person':
                    person_data = {
                        'bbox': obj.get('bbox', []),
                        'confidence': obj.get('confidence', 0),
                        'track_id': obj.get('track_id'),
                        'frame_id': frame_id,
                        'timestamp': timestamp
                    }
                    frame_persons.append(person_data)
                    person_analysis['confidence_scores'].append(obj.get('confidence', 0))
            
            if frame_persons:
                person_analysis['frames_with_people'].append({
                    'frame_id': frame_id,
                    'timestamp': timestamp,
                    'person_count': len(frame_persons),
                    'persons': frame_persons
                })
                person_analysis['total_person_detections'] += len(frame_persons)
        
        # track-based countingÏúºÎ°ú Í≥†Ïú† person Ïàò Í≥ÑÏÇ∞
        person_analysis['unique_persons_estimated'] = SmartJSONParser._estimate_unique_persons(
            person_analysis['frames_with_people']
        )
        
        # ÏÑ±Î≥Ñ Î∂ÑÏÑù (Ï∫°ÏÖò Î∞è VQA Í≤∞Í≥º ÌôúÏö©)
        person_analysis['gender_analysis'] = SmartJSONParser._analyze_gender_from_captions(frame_results)
        
        # ÏãúÍ∞ÑÏ†Å ÏùºÍ¥ÄÏÑ± Í≥ÑÏÇ∞
        person_analysis['temporal_consistency'] = SmartJSONParser._calculate_temporal_consistency(
            person_analysis['frames_with_people']
        )
        
        return person_analysis
    
    @staticmethod
    def _estimate_unique_persons(frames_with_people: List) -> int:
        """Îã§Ï§ë ÌîÑÎ†àÏûÑ Ï∂îÏ†ÅÏúºÎ°ú Í≥†Ïú† person Ïàò Ï∂îÏ†ï"""
        if not frames_with_people:
            return 0
        
        # track_idÍ∞Ä ÏûàÎäî Í≤ΩÏö∞
        track_ids = set()
        for frame_data in frames_with_people:
            for person in frame_data['persons']:
                track_id = person.get('track_id')
                if track_id is not None:
                    track_ids.add(track_id)
        
        if track_ids:
            return len(track_ids)
        
        # track_idÍ∞Ä ÏóÜÎäî Í≤ΩÏö∞ Ìú¥Î¶¨Ïä§Ìã± Î∞©Î≤ï ÏÇ¨Ïö©
        person_counts = [frame_data['person_count'] for frame_data in frames_with_people]
        if not person_counts:
            return 0
        
        # Í∞ÄÏû• ÎÜíÏùÄ confidenceÎ•º Í∞ÄÏßÑ ÌîÑÎ†àÏûÑÎì§Ïùò ÌèâÍ∑†Í∞í ÏÇ¨Ïö©
        high_confidence_frames = []
        for frame_data in frames_with_people:
            avg_confidence = sum(p['confidence'] for p in frame_data['persons']) / len(frame_data['persons'])
            if avg_confidence > 0.7:  # ÎÜíÏùÄ Ïã†Î¢∞ÎèÑ ÌîÑÎ†àÏûÑÎßå
                high_confidence_frames.append(frame_data['person_count'])
        
        if high_confidence_frames:
            return max(high_confidence_frames)  # ÏµúÎåÄÍ∞í ÏÇ¨Ïö©
        else:
            return max(person_counts) if person_counts else 0
    
    @staticmethod
    def _analyze_gender_from_captions(frame_results: List) -> Dict[str, int]:
        """Ï∫°ÏÖòÏóêÏÑú ÏÑ±Î≥Ñ Ï†ïÎ≥¥ Î∂ÑÏÑù"""
        gender_analysis = {'male': 0, 'female': 0, 'unknown': 0}
        
        male_keywords = ['ÎÇ®Ïûê', 'ÎÇ®ÏÑ±', 'man', 'male', 'ÏïÑÏ†ÄÏî®', 'Ï≤≠ÎÖÑ', 'ÏïÑÎπ†', 'ÏïÑÎì§']
        female_keywords = ['Ïó¨Ïûê', 'Ïó¨ÏÑ±', 'woman', 'female', 'ÏïÑÏ§åÎßà', 'ÏÜåÎÖÄ', 'ÏóÑÎßà', 'Îî∏']
        
        for frame_data in frame_results:
            # Ï∫°ÏÖòÏóêÏÑú ÏÑ±Î≥Ñ ÌÇ§ÏõåÎìú Ï∞æÍ∏∞
            captions = [
                frame_data.get('final_caption', ''),
                frame_data.get('enhanced_caption', ''),
                frame_data.get('caption', '')
            ]
            
            frame_text = ' '.join(captions).lower()
            
            male_mentions = sum(1 for keyword in male_keywords if keyword in frame_text)
            female_mentions = sum(1 for keyword in female_keywords if keyword in frame_text)
            
            gender_analysis['male'] += male_mentions
            gender_analysis['female'] += female_mentions
            
            # VQA Í≤∞Í≥ºÎèÑ ÌôïÏù∏
            scene_analysis = frame_data.get('scene_analysis', {})
            vqa_results = scene_analysis.get('vqa_results', {})
            
            for question, answer in vqa_results.items():
                if 'people' in question.lower() or 'ÏÇ¨Îûå' in question:
                    answer_lower = answer.lower()
                    if any(keyword in answer_lower for keyword in male_keywords):
                        gender_analysis['male'] += 1
                    if any(keyword in answer_lower for keyword in female_keywords):
                        gender_analysis['female'] += 1
        
        return gender_analysis
    
    @staticmethod
    def _calculate_temporal_consistency(frames_with_people: List) -> float:
        """ÏãúÍ∞ÑÏ†Å ÏùºÍ¥ÄÏÑ± Í≥ÑÏÇ∞"""
        if len(frames_with_people) < 2:
            return 1.0
        
        person_counts = [frame_data['person_count'] for frame_data in frames_with_people]
        
        # ÌëúÏ§ÄÌé∏Ï∞® Í∏∞Î∞ò ÏùºÍ¥ÄÏÑ± Í≥ÑÏÇ∞
        import numpy as np
        if len(set(person_counts)) == 1:
            return 1.0  # ÏôÑÏ†Ñ ÏùºÍ¥ÄÏÑ±
        
        mean_count = np.mean(person_counts)
        std_count = np.std(person_counts)
        
        # ÏùºÍ¥ÄÏÑ± Ï†êÏàò (0-1)
        consistency = max(0, 1 - (std_count / max(mean_count, 1)))
        return consistency

class KoreanQuestionClassifier:
    """ÌïúÍµ≠Ïñ¥ ÏßàÎ¨∏ Î∂ÑÎ•ò ÏãúÏä§ÌÖú"""
    
    QUESTION_PATTERNS = {
        'person_count': {
            'keywords': ['Î™á Î™Ö', 'Î™áÎ™Ö', 'ÏÇ¨Îûå', 'Ïù∏Î¨º', 'Ïù∏Ïõê', 'ÏÑ±ÎπÑ', 'ÎÇ®ÎÖÄ', 'ÏÇ¨Îûå Ïàò'],
            'patterns': [r'Î™á\s*Î™Ö', r'ÏÇ¨Îûå.*Î™á', r'Ïù∏Ïõê.*Î™á', r'ÏÑ±ÎπÑ', r'ÎÇ®.*Ïó¨'],
            'weight': 1.0
        },
        'object_search': {
            'keywords': ['Ï∞æ', 'Ïñ¥Îîî', 'Ïñ∏Ï†ú', 'Î¨¥Ïóá', 'Î≠êÍ∞Ä', 'ÏûàÏñ¥', 'Î≥¥Ïó¨', 'ÎÇòÏôÄ'],
            'patterns': [r'.*Ï∞æ.*', r'Ïñ¥Îîî.*Ïûà', r'Ïñ∏Ï†ú.*ÎÇò', r'Î¨¥Ïóá.*Î≥¥'],
            'weight': 0.9
        },
        'scene_summary': {
            'keywords': ['ÏöîÏïΩ', 'Ï†ïÎ¶¨', 'ÎÇ¥Ïö©', 'Ï†ÑÏ≤¥', 'Ï§ÑÍ±∞Î¶¨', 'ÏÑ§Î™Ö'],
            'patterns': [r'ÏöîÏïΩ.*Ìï¥', r'Ï†ïÎ¶¨.*Ìï¥', r'ÎÇ¥Ïö©.*Î≠ê', r'Ï†ÑÏ≤¥.*Ïñ¥Îñ§'],
            'weight': 0.8
        },
        'action_analysis': {
            'keywords': ['ÌñâÎèô', 'ÎèôÏûë', 'ÌôúÎèô', 'ÌïòÍ≥†Ïûà', 'ÏõÄÏßÅÏûÑ', 'Î≠ò Ìï¥'],
            'patterns': [r'.*ÌïòÍ≥†\s*Ïûà', r'Î¨¥Ïóá.*ÌïòÎäî', r'Ïñ¥Îñ§.*ÌñâÎèô'],
            'weight': 0.8
        },
        'time_location': {
            'keywords': ['ÏãúÍ∞Ñ', 'Ïû•ÏÜå', 'ÏúÑÏπò', 'Ïñ∏Ï†ú', 'Ïñ¥ÎîîÏÑú', 'Î∞∞Í≤Ω'],
            'patterns': [r'Ïñ∏Ï†ú.*', r'Ïñ¥ÎîîÏÑú.*', r'.*ÏãúÍ∞Ñ', r'.*Ïû•ÏÜå'],
            'weight': 0.7
        }
    }
    
    @classmethod
    def classify_question(cls, question: str) -> Dict[str, float]:
        """ÏßàÎ¨∏ÏùÑ Ïπ¥ÌÖåÍ≥†Î¶¨Î≥ÑÎ°ú Î∂ÑÎ•òÌïòÍ≥† Ï†êÏàò Î∞òÌôò"""
        question_lower = question.lower().strip()
        scores = {}
        
        for category, config in cls.QUESTION_PATTERNS.items():
            score = 0.0
            
            # ÌÇ§ÏõåÎìú Îß§Ïπ≠
            keyword_matches = sum(1 for keyword in config['keywords'] if keyword in question_lower)
            keyword_score = (keyword_matches / len(config['keywords'])) * 0.6
            
            # Ìå®ÌÑ¥ Îß§Ïπ≠
            pattern_matches = sum(1 for pattern in config['patterns'] if re.search(pattern, question_lower))
            pattern_score = (pattern_matches / len(config['patterns'])) * 0.4
            
            total_score = (keyword_score + pattern_score) * config['weight']
            scores[category] = total_score
        
        return scores
    
    @classmethod
    def get_primary_category(cls, question: str) -> str:
        """Ï£ºÏöî Ïπ¥ÌÖåÍ≥†Î¶¨ Î∞òÌôò"""
        scores = cls.classify_question(question)
        if not scores:
            return 'general'
        
        max_category = max(scores.keys(), key=lambda k: scores[k])
        if scores[max_category] > 0.3:  # ÏûÑÍ≥ÑÍ∞í
            return max_category
        else:
            return 'general'

class ContextAwareResponseGenerator:
    """Îß•ÎùΩ Ïù∏Ïãù ÏùëÎãµ ÏÉùÏÑ±Í∏∞"""
    
    def __init__(self):
        self.session_memory = {}  # ÏÑ∏ÏÖòÎ≥Ñ ÎåÄÌôî Ïù¥Î†•
        self.response_templates = {
            'person_count': {
                'single': "ÌôîÎ©¥Ïóê {count}Î™ÖÏù¥ Î≥¥ÏûÖÎãàÎã§.",
                'multiple': "ÎπÑÎîîÏò§ Ï†ÑÏ≤¥ÏóêÏÑú Ï¥ù {count}Î™ÖÏù¥ Îì±Ïû•Ìï©ÎãàÎã§.",
                'uncertain': "Ï†ïÌôïÌûà ÏÑ∏Í∏∞ Ïñ¥Î†§Ïö¥Îç∞, ÎåÄÎûµ {estimate}Î™Ö Ï†ïÎèÑ Î≥¥Ïù¥ÎÑ§Ïöî.",
                'with_gender': "Ï¥ù {total}Î™ÖÏù¥ Îì±Ïû•ÌïòÎ©∞, ÎÇ®ÏÑ± {male}Î™Ö, Ïó¨ÏÑ± {female}Î™ÖÏúºÎ°ú Î∂ÑÏÑùÎê©ÎãàÎã§.",
                'temporal': "{frames}Í∞ú Ïû•Î©¥ÏóêÏÑú ÏÇ¨ÎûåÏù¥ Îì±Ïû•ÌïòÎ©∞, ÌèâÍ∑† {avg_count}Î™ÖÏî© Î≥¥ÏûÖÎãàÎã§."
            },
            'object_search': {
                'found': "'{object}'Î•º {count}Í∞ú Ïû•Î©¥ÏóêÏÑú Ï∞æÏïòÏäµÎãàÎã§.",
                'not_found': "'{object}'Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.",
                'location': "'{object}'Îäî {timestamp}Ï¥à ÏßÄÏ†êÏóêÏÑú ÌôïÏù∏Îê©ÎãàÎã§."
            },
            'scene_summary': {
                'basic': "Ïù¥ ÏòÅÏÉÅÏùÄ {scenes}Í∞úÏùò Ï£ºÏöî Ïû•Î©¥ÏúºÎ°ú Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.",
                'detailed': "ÏòÅÏÉÅÏùò Ï£ºÏöî ÎÇ¥Ïö©: {content}"
            }
        }
    
    def generate_contextual_response(self, question: str, analysis_data: Dict, video_info: Dict) -> str:
        """Îß•ÎùΩÏùÑ Í≥†Î†§Ìïú ÏùëÎãµ ÏÉùÏÑ±"""
        category = KoreanQuestionClassifier.get_primary_category(question)
        
        if category == 'person_count':
            return self._generate_person_count_response(question, analysis_data, video_info)
        elif category == 'object_search':
            return self._generate_object_search_response(question, analysis_data, video_info)
        elif category == 'scene_summary':
            return self._generate_scene_summary_response(question, analysis_data, video_info)
        else:
            return self._generate_general_response(question, analysis_data, video_info)
    
    def _generate_person_count_response(self, question: str, analysis_data: Dict, video_info: Dict) -> str:
        """ÏÇ¨Îûå Ïàò Í¥ÄÎ†® ÏùëÎãµ ÏÉùÏÑ±"""
        person_info = analysis_data.get('person_analysis', {})
        
        unique_count = person_info.get('unique_persons_estimated', 0)
        total_detections = person_info.get('total_person_detections', 0)
        frames_with_people = person_info.get('frames_with_people', [])
        gender_analysis = person_info.get('gender_analysis', {})
        temporal_consistency = person_info.get('temporal_consistency', 0)
        
        templates = self.response_templates['person_count']
        
        # ÏÑ±ÎπÑ ÏßàÎ¨∏Ïù∏ÏßÄ ÌôïÏù∏
        if any(keyword in question for keyword in ['ÏÑ±ÎπÑ', 'ÎÇ®ÎÖÄ', 'ÏÑ±Î≥Ñ']):
            if gender_analysis['male'] > 0 or gender_analysis['female'] > 0:
                return templates['with_gender'].format(
                    total=unique_count,
                    male=gender_analysis['male'],
                    female=gender_analysis['female']
                )
        
        # ÏãúÍ∞ÑÏ†Å ÏùºÍ¥ÄÏÑ±Ïù¥ ÎÇÆÏúºÎ©¥ Î∂àÌôïÏã§ ÌëúÌòÑ
        if temporal_consistency < 0.6:
            estimate = max(unique_count, int(total_detections / max(len(frames_with_people), 1)))
            return templates['uncertain'].format(estimate=estimate)
        
        # Ïó¨Îü¨ ÌîÑÎ†àÏûÑÏóê Í±∏Ï≥ê Îì±Ïû•ÌïòÎäî Í≤ΩÏö∞
        if len(frames_with_people) > 1:
            avg_count = total_detections / len(frames_with_people)
            return templates['temporal'].format(
                frames=len(frames_with_people),
                avg_count=f"{avg_count:.1f}"
            )
        
        # Í∏∞Î≥∏ ÏùëÎãµ
        if unique_count == 1:
            return templates['single'].format(count=unique_count)
        else:
            return templates['multiple'].format(count=unique_count)
    
    def _generate_object_search_response(self, question: str, analysis_data: Dict, video_info: Dict) -> str:
        """Í∞ùÏ≤¥ Í≤ÄÏÉâ ÏùëÎãµ ÏÉùÏÑ±"""
        # ÏßàÎ¨∏ÏóêÏÑú Í≤ÄÏÉâ ÎåÄÏÉÅ Ï∂îÏ∂ú
        search_terms = self._extract_search_terms(question)
        
        frame_results = analysis_data.get('frame_results', [])
        found_objects = []
        
        for frame_data in frame_results:
            frame_id = frame_data.get('image_id', 0)
            timestamp = frame_data.get('timestamp', 0)
            objects = frame_data.get('objects', [])
            
            for obj in objects:
                obj_class = obj.get('class', '').lower()
                if any(term.lower() in obj_class for term in search_terms):
                    found_objects.append({
                        'frame_id': frame_id,
                        'timestamp': timestamp,
                        'object': obj_class,
                        'confidence': obj.get('confidence', 0)
                    })
        
        templates = self.response_templates['object_search']
        
        if found_objects:
            search_term = search_terms[0] if search_terms else 'Ìï¥Îãπ Í∞ùÏ≤¥'
            return templates['found'].format(
                object=search_term,
                count=len(found_objects)
            )
        else:
            search_term = search_terms[0] if search_terms else 'Ìï¥Îãπ Í∞ùÏ≤¥'
            return templates['not_found'].format(object=search_term)
    
    def _generate_scene_summary_response(self, question: str, analysis_data: Dict, video_info: Dict) -> str:
        """Ïû•Î©¥ ÏöîÏïΩ ÏùëÎãµ ÏÉùÏÑ±"""
        frame_results = analysis_data.get('frame_results', [])
        
        # Ï£ºÏöî Í∞ùÏ≤¥ Ï∂îÏ∂ú
        all_objects = []
        for frame_data in frame_results:
            objects = frame_data.get('objects', [])
            all_objects.extend([obj.get('class', '') for obj in objects])
        
        object_counter = Counter(all_objects)
        dominant_objects = [obj for obj, count in object_counter.most_common(5)]
        
        # Ï£ºÏöî Ïû•Î©¥ ÏãùÎ≥Ñ
        unique_scenes = len(frame_results)
        
        summary = f"Ïù¥ ÏòÅÏÉÅÏùÄ {unique_scenes}Í∞úÏùò Ïû•Î©¥ÏúºÎ°ú Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÏäµÎãàÎã§. "
        
        if dominant_objects:
            summary += f"Ï£ºÏöî Í∞ùÏ≤¥Î°úÎäî {', '.join(dominant_objects[:3])} Îì±Ïù¥ Îì±Ïû•Ìï©ÎãàÎã§. "
        
        # ÏÇ¨ÎûåÏù¥ Îì±Ïû•ÌïòÎäî Í≤ΩÏö∞
        person_info = analysis_data.get('person_analysis', {})
        if person_info.get('unique_persons_estimated', 0) > 0:
            summary += f"Ï¥ù {person_info['unique_persons_estimated']}Î™ÖÏùò ÏÇ¨ÎûåÏù¥ Îì±Ïû•Ìï©ÎãàÎã§."
        
        return summary
    
    def _generate_general_response(self, question: str, analysis_data: Dict, video_info: Dict) -> str:
        """ÏùºÎ∞òÏ†ÅÏù∏ ÏùëÎãµ ÏÉùÏÑ±"""
        frame_results = analysis_data.get('frame_results', [])
        
        return f"'{question}'Ïóê ÎåÄÌï¥ Î∂ÑÏÑùÌïú Í≤∞Í≥º, {len(frame_results)}Í∞ú Ïû•Î©¥ÏùÑ Î∞îÌÉïÏúºÎ°ú ÎãµÎ≥ÄÎìúÎ¶¨Í≤†ÏäµÎãàÎã§. Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ ÏßàÎ¨∏ÏùÑ Ìï¥Ï£ºÏãúÎ©¥ Ï†ïÌôïÌïú Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌï† Ïàò ÏûàÏäµÎãàÎã§."
    
    def _extract_search_terms(self, question: str) -> List[str]:
        """ÏßàÎ¨∏ÏóêÏÑú Í≤ÄÏÉâÏñ¥ Ï∂îÏ∂ú"""
        # ÌïúÍµ≠Ïñ¥ Í∞ùÏ≤¥Î™Ö Îß§Ìïë
        object_mapping = {
            'ÏûêÎèôÏ∞®': 'car', 'Ï∞®': 'car', 'ÏäπÏö©Ï∞®': 'car',
            'ÏÇ¨Îûå': 'person', 'Ïù∏Î¨º': 'person',
            'Í∞ïÏïÑÏßÄ': 'dog', 'Í∞ú': 'dog',
            'Í≥†ÏñëÏù¥': 'cat', 'ÎÉ•Ïù¥': 'cat',
            'ÏùòÏûê': 'chair', 'Ï±ÖÏÉÅ': 'table',
            'Ìï∏ÎìúÌè∞': 'cell_phone', 'Ìè∞': 'cell_phone',
            'Ïª¥Ìì®ÌÑ∞': 'laptop', 'ÎÖ∏Ìä∏Î∂Å': 'laptop'
        }
        
        terms = []
        
        # ÏßÅÏ†ë Îß§ÌïëÎêú Í∞ùÏ≤¥ Ï∞æÍ∏∞
        for korean, english in object_mapping.items():
            if korean in question:
                terms.append(english)
        
        # ÏòÅÏñ¥ Í∞ùÏ≤¥Î™Ö ÏßÅÏ†ë Ï∂îÏ∂ú
        import re
        english_objects = re.findall(r'[a-zA-Z]+', question)
        terms.extend(english_objects)
        
        return terms

# views.py (ÌïÑÏöî import)
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework.permissions import AllowAny
from django.utils.decorators import method_decorator
from django.views.decorators.csrf import csrf_exempt
from django.db.models import Q
import time, json, re

COLOR_MAP_KR2EN = {
    'Ï¥àÎ°ù':'green','ÎÖπÏÉâ':'green','Îπ®Í∞ï':'red','Îπ®Í∞Ñ':'red','Ï†ÅÏÉâ':'red',
    'Ï£ºÌô©':'orange','Ïò§Î†åÏßÄ':'orange','ÎÖ∏Îûë':'yellow','ÎÖ∏ÎûÄ':'yellow','Ìô©ÏÉâ':'yellow',
    'ÌååÎûë':'blue','ÌååÎûÄ':'blue','Ï≤≠ÏÉâ':'blue','Î≥¥Îùº':'purple','ÏûêÏ£º':'purple',
    'Í≤ÄÏ†ï':'black','Í≤ÄÏùÄ':'black','ÌïòÏñë':'white','Ìù∞':'white','Î∞±ÏÉâ':'white',
    'ÌöåÏÉâ':'gray','Í∑∏Î†àÏù¥':'gray','Í∞àÏÉâ':'brown',
    'ÌïëÌÅ¨':'pink','Î∂ÑÌôç':'pink','Í∏àÏÉâ':'gold','ÏùÄÏÉâ':'silver'
}

OBJ_MAP_KR2EN = {
    'ÏÇ¨Îûå':'person','ÎÇ®ÏÑ±':'person','Ïó¨ÏÑ±':'person','Ïù∏Î¨º':'person',
    'Í∞ÄÎ∞©':'handbag','Ìï∏ÎìúÎ∞±':'handbag','Î∞±Ìå©':'backpack',
    'ÏûêÎèôÏ∞®':'car','Ï∞®':'car','ÏûêÏ†ÑÍ±∞':'bicycle','Ïò§ÌÜ†Î∞îÏù¥':'motorcycle',
    'Í∞ú':'dog','Í∞ïÏïÑÏßÄ':'dog','Í≥†ÏñëÏù¥':'cat','ÏùòÏûê':'chair','ÎÖ∏Ìä∏Î∂Å':'laptop','Ìú¥ÎåÄÌè∞':'cell_phone','Ìï∏ÎìúÌè∞':'cell_phone','Ìè∞':'cell_phone',
    'Ìã∞ÎπÑ':'tv','tv':'tv'
}

SCENE_KEYWORDS = {
    'ÎπÑ':'rain','ÎπÑÏò§Îäî':'rain','Ïö∞Ï≤ú':'rain',
    'Î∞§':'night','ÏïºÍ∞Ñ':'night','ÎÇÆ':'day','Ïã§ÎÇ¥':'indoor','Ïã§Ïô∏':'outdoor'
}

# views.py
import os, time, json, subprocess, tempfile
from datetime import datetime
from django.conf import settings
from django.http import FileResponse, Http404
from django.urls import reverse
from django.utils.decorators import method_decorator
from django.views.decorators.csrf import csrf_exempt
from rest_framework.views import APIView
from rest_framework.permissions import AllowAny
from rest_framework.response import Response

from .models import Video, Frame, Scene


@method_decorator(csrf_exempt, name='dispatch')
class EnhancedVideoChatView(APIView):
    """Ìñ•ÏÉÅÎêú ÎπÑÎîîÏò§ Ï±ÑÌåÖ - ÏûêÏó∞Ïñ¥ ÏßàÏùòÏóê ÎåÄÌï¥ ÌÖçÏä§Ìä∏ + Ïç∏ÎÑ§Ïùº/ÌÅ¥Î¶ΩÏùÑ Ìï®Íªò Î∞òÌôò"""
    permission_classes = [AllowAny]

    # ---------- Ï¥àÍ∏∞Ìôî ----------
    def __init__(self):
        super().__init__()
        self.llm_client = None
        self.video_analyzer = None
    def _initialize_services(self):
        """ÏÑúÎπÑÏä§ ÏïàÏ†Ñ Ï¥àÍ∏∞Ìôî - LLM ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Í∞úÏÑ†"""
        if self.llm_client is None:
            try:
                from .llm_client import get_llm_client
                self.llm_client = get_llm_client()
                if self.llm_client.is_available():
                    print("LLM ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
                else:
                    print("LLM ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÎπÑÌôúÏÑ±Ìôî - Í∏∞Î≥∏ ÏÑ§Î™Ö ÏÉùÏÑ± Î™®Îìú")
            except Exception as e:
                print(f"LLM ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
                # Mock ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î°ú Ìè¥Î∞±
                from .llm_client import MockLLMClient
                self.llm_client = MockLLMClient()

        if self.video_analyzer is None:
            try:
                from .video_analyzer import get_video_analyzer
                self.video_analyzer = get_video_analyzer()
                print("ÎπÑÎîîÏò§ Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            except Exception as e:
                print(f"ÎπÑÎîîÏò§ Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")

    # ---------- Í≥µÏö© Ïú†Ìã∏ ----------
    def _frame_urls(self, request, video_id, frame_number):
        """ÌîÑÎ†àÏûÑ Ï†ïÍ∑ú Ïù¥ÎØ∏ÏßÄ & Î∞ïÏä§Ïù¥ÎØ∏ÏßÄ URL"""
        base = request.build_absolute_uri
        return {
            'image': base(reverse('frame_normal', args=[video_id, frame_number])),
            'image_bbox': base(reverse('frame_with_bbox', args=[video_id, frame_number])),
        }

    def _clip_url(self, request, video_id, timestamp, duration=4):
        """ÌîÑÎ¶¨Î∑∞ ÌÅ¥Î¶Ω URL"""
        url = reverse('clip_preview', args=[video_id, int(timestamp)])
        return request.build_absolute_uri(f"{url}?duration={int(duration)}")

    def _format_time(self, seconds):
        try:
            m, s = int(seconds) // 60, int(seconds) % 60
            return f"{m}:{s:02d}"
        except:
            return "0:00"

    def _get_video_safe(self, video_id):
        try:
            if video_id:
                return Video.objects.get(id=video_id)
            return Video.objects.filter(is_analyzed=True).first()
        except:
            return None

    # ---------- NLU(Í∞ÑÎã® Ïä¨Î°Ø Ï∂îÏ∂ú) ----------
  # EnhancedVideoChatViewÏóê Ï∂îÍ∞ÄÌï† Î©îÏÑúÎìúÎì§

    def _nlu(self, text: str):
        """intent + slots Í∞ÑÎã® Ï∂îÏ∂ú (ÏòÅÏÉÅ ÏÑ§Î™Ö ÏùòÎèÑ Ï∂îÍ∞Ä)"""
        q = text.lower()
        intent = 'general'
        
        # ÏòÅÏÉÅ ÏÑ§Î™Ö ÌÇ§ÏõåÎìú Ï∂îÍ∞Ä
        if any(k in q for k in ['ÏÑ§Î™ÖÌï¥Ï§ò', 'ÏÑ§Î™ÖÌï¥', 'Ïñ¥Îñ§', 'Î¨¥Ïä®', 'ÎÇ¥Ïö©', 'Ïû•Î©¥', 'ÏòÅÏÉÅÏóê ÎåÄÌï¥', 'Î≠êÍ∞Ä ÎÇòÏôÄ', 'Ïñ¥ÎñªÍ≤å', 'ÏÉÅÌô©']):
            intent = 'video_description'
        elif any(k in q for k in ['ÏöîÏïΩ', 'summary']): 
            intent = 'summary'
        elif any(k in q for k in ['ÌïòÏù¥ÎùºÏù¥Ìä∏', 'highlight']): 
            intent = 'highlight'
        elif any(k in q for k in ['Ï†ïÎ≥¥', 'info']): 
            intent = 'info'
        elif any(k in q for k in ['ÏÑ±ÎπÑ', 'gender']): 
            intent = 'gender_distribution'
        elif any(k in q for k in ['Î∂ÑÏúÑÍ∏∞', 'Î¨¥Îìú', 'mood']): 
            intent = 'scene_mood'
        elif any(k in q for k in ['ÎπÑÏò§Îäî', 'Î∞§', 'ÎÇÆ', 'Ïã§ÎÇ¥', 'Ïã§Ïô∏']): 
            intent = 'cross_video'
        elif any(k in q for k in ['Ï∞æÏïÑÏ§ò', 'Ï∞æÏïÑ Ï§ò', 'Ï∞æÏïÑ', 'Í≤ÄÏÉâ', 'ÎÇòÏôÄ', 'Î≥¥Ïó¨Ï§ò', 'Ï∂îÏ†Å']): 
            intent = 'object_tracking'
        elif any(k in q for k in ['ÏûàÏñ¥?', 'ÎÇòÏôÄ?', 'Îì±Ïû•Ìï¥?']): 
            intent = 'object_presence'

        # Í∏∞Ï°¥ ÏÉâÏÉÅ/Í∞ùÏ≤¥/ÏãúÍ∞ÑÎ≤îÏúÑ Ï≤òÎ¶¨ (ÎèôÏùº)
        color_map = {
            'Îπ®Í∞ï':'red','Îπ®Í∞Ñ':'red','Ï†ÅÏÉâ':'red',
            'Ï£ºÌô©':'orange','Ïò§Î†åÏßÄ':'orange',
            'ÎÖ∏Îûë':'yellow','ÎÖ∏ÎûÄ':'yellow','Ìô©ÏÉâ':'yellow',
            'Ï¥àÎ°ù':'green','ÎÖπÏÉâ':'green',
            'ÌååÎûë':'blue','ÌååÎûÄ':'blue','Ï≤≠ÏÉâ':'blue',
            'Î≥¥Îùº':'purple','ÏûêÏ£º':'purple',
            'Í≤ÄÏ†ï':'black','Í≤ÄÏùÄ':'black',
            'ÌïòÏñë':'white','Ìù∞':'white','Î∞±ÏÉâ':'white',
            'ÌöåÏÉâ':'gray','Í∑∏Î†àÏù¥':'gray',
            'Í∞àÏÉâ':'brown',
            'ÌïëÌÅ¨':'pink','Î∂ÑÌôç':'pink',
        }
        colors = [v for k,v in color_map.items() if k in q]

        object_map = {
            'ÏÇ¨Îûå':'person','ÎÇ®ÏÑ±':'person','Ïó¨ÏÑ±':'person','Ïù∏Î¨º':'person',
            'Í∞ÄÎ∞©':'handbag','Ìï∏ÎìúÎ∞±':'handbag',
            'tv':'tv','Ìã∞ÎπÑ':'tv','ÌÖîÎ†àÎπÑÏ†Ñ':'tv',
            'ÏùòÏûê':'chair',
            'ÏûêÏ†ÑÍ±∞':'bicycle',
            'Ï∞®':'car','ÏûêÎèôÏ∞®':'car',
            'Í≥†ÏñëÏù¥':'cat','Í∞ú':'dog',
            'ÎÖ∏Ìä∏Î∂Å':'laptop','Ìú¥ÎåÄÌè∞':'cell_phone'
        }
        objects = []
        for k,v in object_map.items():
            if k in q:
                objects.append(v)
        objects = list(dict.fromkeys(objects))

        import re
        tmatch = re.search(r'(\d{1,2}:\d{2})\s*[-~]\s*(\d{1,2}:\d{2})', q)
        trange = None
        if tmatch:
            def to_sec(s):
                mm, ss = s.split(':')
                return int(mm) * 60 + int(ss)
            trange = {'start': to_sec(tmatch.group(1)), 'end': to_sec(tmatch.group(2))}

        return {'intent': intent, 'slots': {'colors': colors, 'objects': objects, 'time_range': trange}}

    def _handle_video_description(self, video: Video, raw_text: str, request=None):
        """LLMÏùÑ ÌôúÏö©Ìïú ÏûêÏó∞Ïä§Îü¨Ïö¥ ÏòÅÏÉÅ ÏÑ§Î™Ö ÏÉùÏÑ±"""
        try:
            # ÌîÑÎ†àÏûÑÎì§Ïùò Ï∫°ÏÖò Ï†ïÎ≥¥ ÏàòÏßë
            frames = Frame.objects.filter(video=video).order_by('timestamp')
            
            if not frames.exists():
                return {'text': 'ÏòÅÏÉÅ Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏñ¥ÏÑú ÏÑ§Î™ÖÏùÑ Ï†úÍ≥µÌï† Ïàò ÏóÜÏäµÎãàÎã§.', 'items': []}
            
            # ÎåÄÌëú Ï∫°ÏÖòÎì§ ÏàòÏßë (Ï†ÑÏ≤¥ ÏòÅÏÉÅÏùò 5-8Í∞ú Íµ¨Í∞Ñ)
            total_frames = frames.count()
            sample_count = min(8, max(5, total_frames // 6))  # 5-8Í∞ú Íµ¨Í∞Ñ
            sample_indices = [int(i * total_frames / sample_count) for i in range(sample_count)]
            
            key_scenes = []
            caption_data = []
            
            for idx in sample_indices:
                try:
                    frame = frames[idx] if idx < total_frames else frames.last()
                    
                    # ÏµúÍ≥† ÌíàÏßà Ï∫°ÏÖò ÏÑ†ÌÉù
                    best_caption = ""
                    if hasattr(frame, 'final_caption') and frame.final_caption:
                        best_caption = frame.final_caption
                    elif hasattr(frame, 'enhanced_caption') and frame.enhanced_caption:
                        best_caption = frame.enhanced_caption
                    elif hasattr(frame, 'caption') and frame.caption:
                        best_caption = frame.caption
                    elif hasattr(frame, 'blip_caption') and frame.blip_caption:
                        best_caption = frame.blip_caption
                    
                    if best_caption and len(best_caption.strip()) > 10:
                        scene_data = {
                            'timestamp': float(frame.timestamp),
                            'time_str': self._format_time(frame.timestamp),
                            'frame_id': frame.image_id,
                            'caption': best_caption.strip()
                        }
                        key_scenes.append(scene_data)
                        caption_data.append({
                            'time': scene_data['time_str'],
                            'caption': best_caption.strip()
                        })
                        
                except (IndexError, AttributeError):
                    continue
            
            if not caption_data:
                return {'text': 'ÏòÅÏÉÅ Ï∫°ÏÖò Ï†ïÎ≥¥Í∞Ä Î∂ÄÏ°±Ìï¥ÏÑú ÏÉÅÏÑ∏Ìïú ÏÑ§Î™ÖÏùÑ Ï†úÍ≥µÌï† Ïàò ÏóÜÏäµÎãàÎã§.', 'items': []}
            
            # LLMÏùÑ ÏÇ¨Ïö©Ìï¥ÏÑú ÏûêÏó∞Ïä§Îü¨Ïö¥ ÏÑ§Î™Ö ÏÉùÏÑ±
            llm_description = self._generate_llm_description(video, caption_data, raw_text)
            
            # ÎåÄÌëú Ïû•Î©¥ Ïù¥ÎØ∏ÏßÄÎì§ (3-5Í∞ú)
            representative_scenes = key_scenes[::max(1, len(key_scenes)//4)][:5]  # ÏµúÎåÄ 5Í∞ú ÏÑ†ÌÉù
            items = []
            
            for scene in representative_scenes:
                if request:
                    media = self._frame_urls(request, video.id, scene['frame_id'])
                    clip = self._clip_url(request, video.id, scene['timestamp'])
                    items.append({
                        'time': scene['time_str'],
                        'seconds': int(scene['timestamp']),
                        'frame_id': scene['frame_id'],
                        'desc': scene['caption'][:120] + "..." if len(scene['caption']) > 120 else scene['caption'],
                        'full_caption': scene['caption'],
                        'source': 'AI Î∂ÑÏÑù',
                        'thumbUrl': media.get('image'),
                        'thumbBBoxUrl': media.get('image_bbox'),
                        'clipUrl': clip,
                    })
            
            return {'text': llm_description, 'items': items}
            
        except Exception as e:
            print(f"ÏòÅÏÉÅ ÏÑ§Î™Ö ÏÉùÏÑ± Ïò§Î•ò: {e}")
            return {'text': f'ÏòÅÏÉÅ ÏÑ§Î™ÖÏùÑ ÏÉùÏÑ±ÌïòÎäî Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}', 'items': []}

    def _generate_llm_description(self, video: Video, caption_data, user_query):
        """LLMÏùÑ ÏÇ¨Ïö©Ìï¥ÏÑú Ï∫°ÏÖòÎì§ÏùÑ Î∂ÑÏÑùÌïòÍ≥† ÏûêÏó∞Ïä§Îü¨Ïö¥ ÏÑ§Î™Ö ÏÉùÏÑ±"""
        try:
            if not self.llm_client:
                # LLMÏù¥ ÏóÜÏúºÎ©¥ Í∏∞Î≥∏ ÏÑ§Î™Ö ÏÉùÏÑ±
                return self._generate_fallback_description(video, caption_data)
            
            # LLM ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±
            prompt = self._build_description_prompt(video, caption_data, user_query)
            
            # LLM Ìò∏Ï∂ú
            llm_response = self.llm_client.generate_response(prompt)
            
            if llm_response and len(llm_response.strip()) > 50:
                return llm_response.strip()
            else:
                return self._generate_fallback_description(video, caption_data)
                
        except Exception as e:
            print(f"LLM ÏÑ§Î™Ö ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return self._generate_fallback_description(video, caption_data)

    def _build_description_prompt(self, video: Video, caption_data, user_query):
        """LLMÏö© ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±"""
        
        prompt = f"""ÏòÅÏÉÅ Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú ÏûêÏó∞Ïä§ÎüΩÍ≥† ÏùΩÍ∏∞ Ïâ¨Ïö¥ ÏòÅÏÉÅ ÏÑ§Î™ÖÏùÑ ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.

    ÏòÅÏÉÅ Ï†ïÎ≥¥:
    - ÌååÏùºÎ™Ö: {video.original_name}
    - Í∏∏Ïù¥: {round(video.duration, 1)}Ï¥à
    - ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏: "{user_query}"

    ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù Í≤∞Í≥º:
    """
        
        for data in caption_data:
            prompt += f"- {data['time']}: {data['caption']}\n"
        
        prompt += """
    Îã§Ïùå ÏöîÍµ¨ÏÇ¨Ìï≠Ïóê Îî∞Îùº ÏÑ§Î™ÖÏùÑ ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî:

    1. ÏûêÏó∞Ïä§ÎüΩÍ≥† ÏùΩÍ∏∞ Ïâ¨Ïö¥ ÌïúÍµ≠Ïñ¥Î°ú ÏûëÏÑ±
    2. Ï§ëÎ≥µÎêòÎäî ÎÇ¥Ïö©ÏùÄ ÏöîÏïΩÌïòÏó¨ Ï†ïÎ¶¨
    3. ÏòÅÏÉÅÏùò Ï†ÑÏ≤¥Ï†ÅÏù∏ ÌùêÎ¶ÑÍ≥º Ï£ºÏöî ÎÇ¥Ïö© Í∞ïÏ°∞
    4. 2-3Í∞ú Î¨∏Îã®ÏúºÎ°ú Íµ¨ÏÑ± (Í∞Å Î¨∏Îã®ÏùÄ 2-4Î¨∏Ïû•)
    5. Í∏∞Ïà†Ï†ÅÏù∏ Ïö©Ïñ¥ÎÇò ÌîÑÎ†àÏûÑ Î≤àÌò∏ Í∞ôÏùÄ Ï†ïÎ≥¥Îäî Ï†úÏô∏
    6. ÏòÅÏÉÅÏùò Î∂ÑÏúÑÍ∏∞ÎÇò ÏÉÅÌô©ÏùÑ ÏÉùÏÉùÌïòÍ≤å Ï†ÑÎã¨

    ÏÑ§Î™Ö ÌòïÏãù:
    Ï≤´ Î≤àÏß∏ Î¨∏Îã®: ÏòÅÏÉÅÏùò Ï†ÑÏ≤¥Ï†ÅÏù∏ Î∞∞Í≤ΩÍ≥º ÏÉÅÌô©
    Îëê Î≤àÏß∏ Î¨∏Îã®: Ï£ºÏöî Ïû•Î©¥Í≥º ÌôúÎèô
    ÏÑ∏ Î≤àÏß∏ Î¨∏Îã®: ÏòÅÏÉÅÏùò ÌäπÏßïÏù¥ÎÇò Ïù∏ÏÉÅÏ†ÅÏù∏ Î∂ÄÎ∂Ñ

    Ïù¥Ï†ú ÏòÅÏÉÅ ÏÑ§Î™ÖÏùÑ ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî:"""

        return prompt

    def _generate_fallback_description(self, video: Video, caption_data):
        """LLMÏù¥ ÏóÜÏùÑ Îïå ÏÇ¨Ïö©Ìï† Í∏∞Î≥∏ ÏÑ§Î™Ö ÏÉùÏÑ±"""
        
        description = f"'{video.original_name}' ÏòÅÏÉÅ Î∂ÑÏÑù\n\n"
        
        # Í∏∞Î≥∏ Ï†ïÎ≥¥
        description += f"Ïù¥ ÏòÅÏÉÅÏùÄ Ï¥ù {round(video.duration, 1)}Ï¥à Í∏∏Ïù¥Ïùò ÏòÅÏÉÅÏûÖÎãàÎã§.\n\n"
        
        # Ï£ºÏöî ÎÇ¥Ïö© ÏöîÏïΩ
        all_captions = " ".join([data['caption'] for data in caption_data]).lower()
        
        # Ïû•ÏÜå Ï∂îÏ∂ú
        locations = []
        if 'Ïã§ÎÇ¥' in all_captions or 'indoor' in all_captions:
            locations.append('Ïã§ÎÇ¥')
        if 'ÏáºÌïëÎ™∞' in all_captions:
            locations.append('ÏáºÌïëÎ™∞')
        if 'Í±∞Î¶¨' in all_captions:
            locations.append('Í±∞Î¶¨')
        
        # ÏãúÍ∞ÑÎåÄ Ï∂îÏ∂ú
        time_info = []
        if 'Ïò§ÌõÑ' in all_captions:
            time_info.append('Ïò§ÌõÑ ÏãúÍ∞Ñ')
        if 'Î∞ùÏùÄ' in all_captions:
            time_info.append('Î∞ùÏùÄ ÌôòÍ≤Ω')
        
        # ÌôúÎèô Ï∂îÏ∂ú
        activities = []
        if 'Í±∑' in all_captions:
            activities.append('ÏÇ¨ÎûåÎì§Ïù¥ Í±∑Í≥† ÏûàÎäî')
        if 'ÏáºÌïë' in all_captions:
            activities.append('ÏáºÌïëÌïòÎäî')
        
        # ÏÑ§Î™Ö Íµ¨ÏÑ±
        if locations:
            description += f"{', '.join(locations)}ÏóêÏÑú "
        if time_info:
            description += f"{', '.join(time_info)}Ïóê "
        if activities:
            description += f"{', '.join(activities)} Î™®ÏäµÏù¥ Îã¥Í≤® ÏûàÏäµÎãàÎã§.\n\n"
        
        # ÏãúÍ∞ÑÎåÄÎ≥Ñ Ï£ºÏöî Î≥ÄÌôî
        if len(caption_data) >= 3:
            description += "ÏòÅÏÉÅ Ï¥àÎ∞òÏóêÎäî "
            start_caption = caption_data[0]['caption']
            if 'ÏÇ¨Îûå' in start_caption:
                description += "Ïó¨Îü¨ ÏÇ¨ÎûåÎì§Ïù¥ Îì±Ïû•ÌïòÏó¨ "
            if 'Í±∑' in start_caption:
                description += "Ïù¥ÎèôÌïòÎäî Î™®ÏäµÏùÑ Î≥¥Ïó¨Ï£ºÎ©∞, "
            
            description += "Ï§ëÎ∞òÎ∂ÄÏóêÎäî "
            mid_caption = caption_data[len(caption_data)//2]['caption']
            if 'ÌôúÎèô' in mid_caption or 'ÏáºÌïë' in mid_caption:
                description += "Îã§ÏñëÌïú ÌôúÎèôÎì§Ïù¥ Ïù¥Ïñ¥ÏßëÎãàÎã§. "
            
            description += "Ï†ÑÏ≤¥Ï†ÅÏúºÎ°ú ÏùºÏÉÅÏ†ÅÏù∏ Ïû•Î©¥Îì§Ïù¥ ÏûêÏó∞Ïä§ÎüΩÍ≤å Ïó∞Í≤∞Îêú ÏòÅÏÉÅÏûÖÎãàÎã§."
        
        return description

    def _generate_comprehensive_description(self, video: Video, key_scenes, detailed_captions):
        """ÏàòÏßëÎêú Ï∫°ÏÖòÎì§ÏùÑ Î∞îÌÉïÏúºÎ°ú Ï¢ÖÌï©Ï†ÅÏù∏ ÏòÅÏÉÅ ÏÑ§Î™Ö ÏÉùÏÑ±"""
        
        # 1. Í∏∞Î≥∏ Ï†ïÎ≥¥
        description = f"üìπ '{video.original_name}' ÏòÅÏÉÅ Î∂ÑÏÑù Í≤∞Í≥º\n\n"
        description += f"‚è±Ô∏è Í∏∏Ïù¥: {round(video.duration, 1)}Ï¥à\n"
        description += f"üé¨ Ï¥ù {len(key_scenes)}Í∞ú Ï£ºÏöî Ïû•Î©¥ Î∂ÑÏÑù\n\n"
        
        # 2. Ï†ÑÏ≤¥Ï†ÅÏù∏ ÌäπÏßï Ï∂îÏ∂ú
        all_text = " ".join(detailed_captions).lower()
        
        # Ïû•ÏÜå/ÌôòÍ≤Ω Ï†ïÎ≥¥
        locations = []
        if 'Ïã§ÎÇ¥' in all_text or 'indoor' in all_text:
            locations.append('Ïã§ÎÇ¥')
        if 'Ïã§Ïô∏' in all_text or 'outdoor' in all_text:
            locations.append('Ïã§Ïô∏')
        if 'ÏáºÌïëÎ™∞' in all_text:
            locations.append('ÏáºÌïëÎ™∞')
        if 'Í±∞Î¶¨' in all_text or 'sidewalk' in all_text:
            locations.append('Í±∞Î¶¨')
        if 'Í±¥Î¨º' in all_text or 'building' in all_text:
            locations.append('Í±¥Î¨º')
        
        # ÏãúÍ∞ÑÎåÄ Ï†ïÎ≥¥
        time_info = []
        if 'Ïò§ÌõÑ' in all_text or 'afternoon' in all_text:
            time_info.append('Ïò§ÌõÑ')
        if 'ÏïÑÏπ®' in all_text or 'morning' in all_text:
            time_info.append('ÏïÑÏπ®')
        if 'Î∞§' in all_text or 'night' in all_text:
            time_info.append('Î∞§')
        if 'Î∞ùÏùÄ' in all_text or 'bright' in all_text:
            time_info.append('Î∞ùÏùÄ ÌôòÍ≤Ω')
        
        # Ï£ºÏöî Í∞ùÏ≤¥/ÌôúÎèô
        detected_objects = set()
        activities = set()
        
        for caption in detailed_captions:
            caption_lower = caption.lower()
            # Í∞ùÏ≤¥ Ï∂îÏ∂ú
            if 'ÏÇ¨Îûå' in caption_lower or 'person' in caption_lower:
                detected_objects.add('ÏÇ¨Îûå')
            if 'Í∞ÄÎ∞©' in caption_lower or 'handbag' in caption_lower:
                detected_objects.add('Í∞ÄÎ∞©')
            if 'tv' in caption_lower or 'Ìã∞ÎπÑ' in caption_lower:
                detected_objects.add('TV')
            if 'ÏùòÏûê' in caption_lower or 'chair' in caption_lower:
                detected_objects.add('ÏùòÏûê')
            
            # ÌôúÎèô Ï∂îÏ∂ú
            if 'Í±∑' in caption_lower or 'walking' in caption_lower:
                activities.add('Í±∑Í∏∞')
            if 'ÏÑú' in caption_lower or 'standing' in caption_lower:
                activities.add('ÏÑúÏûàÍ∏∞')
            if 'ÏáºÌïë' in caption_lower or 'shopping' in caption_lower:
                activities.add('ÏáºÌïë')
            if 'ÎåÄÌôî' in caption_lower or 'talking' in caption_lower:
                activities.add('ÎåÄÌôî')
        
        # 3. Ï¢ÖÌï© ÏÑ§Î™Ö
        description += "üèûÔ∏è **ÏòÅÏÉÅ Í∞úÏöî:**\n"
        
        if locations:
            description += f"- Ïû•ÏÜå: {', '.join(locations)}\n"
        if time_info:
            description += f"- ÏãúÍ∞Ñ/ÌôòÍ≤Ω: {', '.join(time_info)}\n"
        if detected_objects:
            description += f"- Ï£ºÏöî Í∞ùÏ≤¥: {', '.join(list(detected_objects)[:5])}\n"
        if activities:
            description += f"- Ï£ºÏöî ÌôúÎèô: {', '.join(list(activities)[:3])}\n"
        
        description += "\n"
        
        # 4. ÏãúÍ∞ÑÎåÄÎ≥Ñ Ï£ºÏöî Ïû•Î©¥ (Ï≤òÏùå, Ï§ëÍ∞Ñ, ÎÅù 3Í∞ú Íµ¨Í∞Ñ)
        if len(key_scenes) >= 3:
            description += "üéûÔ∏è **Ï£ºÏöî Ïû•Î©¥ ÏöîÏïΩ:**\n\n"
            
            # ÏãúÏûë Ïû•Î©¥
            start_scene = key_scenes[0]
            description += f"**{start_scene['time_str']} (ÏãúÏûë):** {start_scene['caption'][:150]}...\n\n"
            
            # Ï§ëÍ∞Ñ Ïû•Î©¥
            mid_scene = key_scenes[len(key_scenes)//2]
            description += f"**{mid_scene['time_str']} (Ï§ëÎ∞ò):** {mid_scene['caption'][:150]}...\n\n"
            
            # ÎÅù Ïû•Î©¥
            end_scene = key_scenes[-1]
            description += f"**{end_scene['time_str']} (Ï¢ÖÎ£å):** {end_scene['caption'][:150]}...\n\n"
        
        # 5. Ï∂îÍ∞Ä Ï†ïÎ≥¥
        description += "üí° **Î∂ÑÏÑù Ï†ïÎ≥¥:**\n"
        description += f"- Î∂ÑÏÑù ÏÉÅÌÉú: {video.analysis_status}\n"
        description += f"- ÌîÑÎ†àÏûÑ Í∏∞Î∞ò AI Î∂ÑÏÑùÏùÑ ÌÜµÌï¥ ÏÉùÏÑ±Îêú ÏÑ§Î™ÖÏûÖÎãàÎã§\n"
        description += f"- ÏïÑÎûò Ïù¥ÎØ∏ÏßÄÎì§ÏùÑ ÌÅ¥Î¶≠ÌïòÎ©¥ Ìï¥Îãπ ÏãúÏ†êÏùò ÏÉÅÏÑ∏ Ïû•Î©¥ÏùÑ Î≥º Ïàò ÏûàÏäµÎãàÎã§"
        
        return description
    # ---------- Frame JSON ÌÜµÏùº ----------
    def _get_detected_objects(self, frame: Frame):
        """
        Frame.detected_objects(JSONField/CharField) ‚Üí list[dict] Î°ú ÌÜµÏùº Î∞òÌôò
        Í∞ùÏ≤¥ ÏòàÏãú: {class:'person', bbox:[x1,y1,x2,y2], colors:['green'], color_description:'green-mixed', confidence:0.7, gender:'male', track_id:'t1'}
        """
        data = []
        raw = getattr(frame, 'detected_objects', None)
        if not raw:
            return data
        try:
            if isinstance(raw, str):
                data = json.loads(raw)
            elif isinstance(raw, (list, dict)):
                data = raw
        except Exception:
            return []
        if isinstance(data, dict):
            # {objects:[...]} ÌòïÌÉúÎèÑ ÌóàÏö©
            data = data.get('objects', [])
        # ÏïàÏ†Ñ ÌïÑÎìú Î≥¥Ï†ï
        norm = []
        for o in data:
            norm.append({
                'class': (o.get('class') or o.get('label') or '').lower(),
                'bbox': o.get('bbox') or o.get('box') or [],
                'colors': o.get('colors') or [],
                'color_description': (o.get('color_description') or o.get('color') or 'unknown').lower(),
                'confidence': float(o.get('confidence', 0.5)),
                'gender': (o.get('gender') or '').lower(),
                'track_id': o.get('track_id') or o.get('id'),
            })
        return norm

    # ---------- POST ----------

    def post(self, request):
        try:
            self._initialize_services()
            user_query = (request.data.get('message') or '').strip()
            video_id = request.data.get('video_id')

            if not user_query:
                return Response({'response': 'Î©îÏãúÏßÄÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'})

            video = self._get_video_safe(video_id)
            if not video:
                return Response({'response': 'Î∂ÑÏÑùÎêú ÎπÑÎîîÏò§Í∞Ä ÏóÜÏäµÎãàÎã§. ÏóÖÎ°úÎìú/Î∂ÑÏÑù ÌõÑ Ïù¥Ïö©Ìï¥Ï£ºÏÑ∏Ïöî.'})

            nlu = self._nlu(user_query)
            intent, slots = nlu['intent'], nlu['slots']

            # ÏòÅÏÉÅ ÏÑ§Î™Ö Ï≤òÎ¶¨ Ï∂îÍ∞Ä
            if intent == 'video_description':
                out = self._handle_video_description(video, user_query, request=request)
            elif intent == 'object_tracking':
                out = self._handle_object_tracking(video, slots, user_query, request=request)
            elif intent == 'object_presence':
                out = self._handle_object_presence(video, user_query, slots, request=request)
            elif intent == 'gender_distribution':
                out = {'text': self._handle_gender_distribution(video, slots), 'items': []}
            elif intent == 'scene_mood':
                out = {'text': self._handle_scene_mood(video), 'items': []}
            elif intent == 'cross_video':
                out = {'text': self._handle_cross_video(user_query), 'items': []}
            elif intent == 'summary':
                out = self._handle_summary(video, request=request)
            elif intent == 'highlight':
                out = self._handle_highlight(video, request=request)
            elif intent == 'info':
                out = {'text': self._handle_info(video), 'items': []}
            else:
                out = {'text': f"'{user_query}' ÏßàÎ¨∏ ÌôïÏù∏! ÏÉâÏÉÅ/Í∞ùÏ≤¥/ÏãúÍ∞ÑÎ≤îÏúÑÎ•º Ìï®Íªò Ï£ºÏãúÎ©¥ Îçî Ï†ïÌôïÌï¥Ïöî. Ïòà) 'Ï¥àÎ°ù ÏÉÅÏùò ÏÇ¨Îûå 0:05~0:10'", 'items': []}

            return Response({
                'response': out['text'],
                'video_id': video.id,
                'video_name': video.original_name,
                'query_type': intent,
                'timestamp': time.time(),
                'items': out.get('items', []),
            })

        except Exception as e:
            print(f"[EnhancedVideoChatView] Ïò§Î•ò: {e}")
            return Response({'response': f"ÏßàÎ¨∏ÏùÑ Î∞õÏïòÏäµÎãàÎã§. Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {e}", 'fallback': True})
    # ---------- Intent Handlers ----------
    def _handle_object_tracking(self, video: Video, slots: dict, raw_text: str, request=None):
        """ÏÉâ/Í∞ùÏ≤¥/ÏãúÍ∞Ñ Î≤îÏúÑÎ•º Í∏∞Î∞òÏúºÎ°ú ÏÉÅÏúÑ Îß§Ïπ≠ Ïû•Î©¥ + Ïç∏ÎÑ§Ïùº/ÌÅ¥Î¶Ω Î∞òÌôò"""
        colors = set(slots.get('colors') or [])
        objects = set(slots.get('objects') or ['person'])  # Í∏∞Î≥∏ ÏÇ¨Îûå
        tr = slots.get('time_range')

        frames_qs = Frame.objects.filter(video=video).order_by('timestamp')
        if tr and tr.get('start') is not None and tr.get('end') is not None:
            frames_qs = frames_qs.filter(timestamp__gte=tr['start'], timestamp__lte=tr['end'])

        hits = []
        for f in frames_qs:
            dets = self._get_detected_objects(f)
            if not dets: continue
            for d in dets:
                score, reasons = 0.0, []
                # Í∞ùÏ≤¥ Îß§Ïπ≠
                if objects:
                    if d['class'] in objects:
                        score += 0.5
                        reasons.append(f"{d['class']} Í∞ùÏ≤¥")
                    elif any(o in d['class'] for o in objects):
                        score += 0.3
                        reasons.append(f"{d['class']} Ïú†ÏÇ¨ Í∞ùÏ≤¥")
                # ÏÉâÏÉÅ Îß§Ïπ≠
                if colors:
                    hit = False
                    cd = d['color_description']
                    if any(c in cd for c in colors):
                        hit = True
                    if not hit and d['colors']:
                        if any(c in (str(x).lower()) for x in d['colors'] for c in colors):
                            hit = True
                    if hit:
                        score += 0.3
                        reasons.append("ÏÉâÏÉÅ Îß§Ïπ≠")

                if score >= 0.5:
                    hits.append({
                        't': float(f.timestamp),
                        'time': self._format_time(f.timestamp),
                        'frame_id': f.image_id,
                        'desc': f"{d.get('color_description','')} {d.get('class','object')}".strip(),
                        'score': min(1.0, (score + d.get('confidence', 0.5) * 0.2)),
                        'reasons': reasons,
                        'track': d.get('track_id') or '',
                    })

        if not hits:
            return {'text': f"‚Äò{raw_text}‚ÄôÎ°úÎäî Îß§Ïπ≠Ïù¥ ÏóÜÏóàÏñ¥Ïöî. ÏãúÍ∞Ñ Î≤îÏúÑÎ•º ÎÑìÌûàÍ±∞ÎÇò ÏÉâÏÉÅ ÏóÜÏù¥ Îã§Ïãú ÏãúÎèÑÌï¥ Î≥¥ÏÑ∏Ïöî.", 'items': []}

        # Ï†ïÎ†¨ + Ï§ëÎ≥µ Ï†úÍ±∞ + ÏÉÅÏúÑ 10Í∞ú
        hits.sort(key=lambda x: (-x['score'], x['t']))
        uniq, seen = [], set()
        for h in hits:
            key = (int(h['t']), h['desc'])
            if key in seen: continue
            seen.add(key)
            media = self._frame_urls(request, video.id, h['frame_id']) if request else {}
            clip = self._clip_url(request, video.id, h['t']) if request else None
            uniq.append({
                'time': h['time'],
                'seconds': int(h['t']),
                'frame_id': h['frame_id'],
                'desc': h['desc'],
                'score': h['score'],
                'reasons': h['reasons'],
                'thumbUrl': media.get('image'),
                'thumbBBoxUrl': media.get('image_bbox'),
                'clipUrl': clip,
            })
            if len(uniq) >= 10: break

        text = "üîé ÏöîÏ≤≠ÌïòÏã† Ïû•Î©¥ÏùÑ Ï∞æÏïòÏñ¥Ïöî (ÏÉÅÏúÑ {n}Í∞ú):\n".format(n=len(uniq))
        text += "\n".join([f"- {it['time']} ¬∑ {it['desc']} ¬∑ ~{int(it['score']*100)}%" for it in uniq])
        return {'text': text, 'items': uniq}

    def _handle_object_presence(self, video: Video, raw_text: str, slots: dict, request=None):
        """ÌäπÏ†ï Í∞ùÏ≤¥/ÌÇ§ÏõåÎìú Îì±Ïû• Ïó¨Î∂Ä Í∞ÑÎã® ÌôïÏù∏ + Ïç∏ÎÑ§Ïùº"""
        objs = slots.get('objects') or []
        q = raw_text.lower()
        frames = Frame.objects.filter(video=video).order_by('timestamp')[:100]
        hits = []
        for f in frames:
            cap = (f.final_caption or f.enhanced_caption or f.caption or '').lower()
            dets = self._get_detected_objects(f)
            ok = False
            reason = ""
            if objs and any(o in (cap or '') for o in objs):
                ok, reason = True, "Ï∫°ÏÖò Îß§Ïπ≠"
            if not ok and dets:
                if objs and any(d['class'] in objs for d in dets):
                    ok, reason = True, "Í∞ùÏ≤¥ Îß§Ïπ≠"
                elif any(k in cap for k in q.split()):
                    ok, reason = True, "ÌÇ§ÏõåÎìú Îß§Ïπ≠"

            if ok:
                media = self._frame_urls(request, video.id, f.image_id)
                clip = self._clip_url(request, video.id, f.timestamp)
                hits.append({
                    'time': self._format_time(f.timestamp),
                    'seconds': int(f.timestamp),
                    'frame_id': f.image_id,
                    'desc': (f.final_caption or f.enhanced_caption or f.caption or '').strip()[:120],
                    'thumbUrl': media['image'],
                    'thumbBBoxUrl': media['image_bbox'],
                    'clipUrl': clip,
                })
            if len(hits) >= 10: break

        if not hits:
            return {'text': "Ìï¥Îãπ ÌÇ§ÏõåÎìú/Í∞ùÏ≤¥Î•º Ï∞æÏßÄ Î™ªÌñàÏñ¥Ïöî.", 'items': []}
        text = "‚úÖ Ï∞æÏïòÏäµÎãàÎã§:\n" + "\n".join([f"- {h['time']} ¬∑ {h['desc']}" for h in hits])
        return {'text': text, 'items': hits}

    def _handle_highlight(self, video: Video, request=None):
        """ÏÉÅÏúÑ 5Í∞ú Ïî¨ + Í∞Å Ïî¨ ÎåÄÌëú Ïç∏ÎÑ§Ïùº/ÌÅ¥Î¶Ω"""
        scenes = Scene.objects.filter(video=video).order_by('start_time')[:5]
        if not scenes:
            return {'text': "ÌïòÏù¥ÎùºÏù¥Ìä∏Í∞Ä ÏïÑÏßÅ ÏóÜÏñ¥Ïöî. Î∂ÑÏÑùÏù¥ ÎÅùÎÇ¨ÎäîÏßÄ ÌôïÏù∏Ìï¥ Ï£ºÏÑ∏Ïöî.", 'items': []}

        items, lines = [], []
        for s in scenes:
            mid = (s.start_time + s.end_time) / 2.0
            f = Frame.objects.filter(video=video, timestamp__gte=mid).order_by('timestamp').first() or \
                Frame.objects.filter(video=video).order_by('-timestamp').first()
            media = self._frame_urls(request, video.id, f.image_id) if f else {}
            clip = self._clip_url(request, video.id, mid) if f else None
            objs = (s.dominant_objects or [])[:5]
            items.append({
                'range': [int(s.start_time), int(s.end_time)],
                'start': self._format_time(s.start_time),
                'end': self._format_time(s.end_time),
                'objects': objs,
                'thumbUrl': media.get('image'),
                'thumbBBoxUrl': media.get('image_bbox'),
                'clipUrl': clip,
            })
            lines.append(f"- {self._format_time(s.start_time)}‚Äì{self._format_time(s.end_time)} ¬∑ {', '.join(objs) or 'Ïû•Î©¥'}")

        return {'text': "‚ú® Ï£ºÏöî Ïû•Î©¥:\n" + "\n".join(lines), 'items': items}

    def _handle_summary(self, video: Video, request=None):
        """Í∞ÑÎã® ÏöîÏïΩ + ÎåÄÌëú Ïç∏ÎÑ§Ïùº Î™á Ïû•"""
        summary = [
            f"‚Äò{video.original_name}‚Äô ÏöîÏïΩ",
            f"- Í∏∏Ïù¥: {round(video.duration,2)}Ï¥à ¬∑ Î∂ÑÏÑù ÏÉÅÌÉú: {video.analysis_status}",
        ]
        try:
            analysis = getattr(video, 'analysis', None)
            if analysis and analysis.analysis_statistics:
                stats = analysis.analysis_statistics
                dom = stats.get('dominant_objects', [])[:5]
                if dom:
                    summary.append(f"- Ï£ºÏöî Í∞ùÏ≤¥: {', '.join(dom)}")
                scene_types = stats.get('scene_types', [])[:3]
                if scene_types:
                    summary.append(f"- Ïû•Î©¥ Ïú†Ìòï: {', '.join(scene_types)}")
        except:
            pass

        frames = Frame.objects.filter(video=video).order_by('timestamp')[:6]
        items = []
        for f in frames:
            media = self._frame_urls(request, video.id, f.image_id)
            clip = self._clip_url(request, video.id, f.timestamp)
            items.append({
                'time': self._format_time(f.timestamp),
                'seconds': int(f.timestamp),
                'frame_id': f.image_id,
                'desc': (f.final_caption or f.enhanced_caption or f.caption or '').strip()[:120],
                'thumbUrl': media['image'],
                'thumbBBoxUrl': media['image_bbox'],
                'clipUrl': clip,
            })

        return {'text': "\n".join(summary), 'items': items}

    def _handle_info(self, video: Video):
        sc = Scene.objects.filter(video=video).count()
        fc = Frame.objects.filter(video=video).count()
        return "\n".join([
            "ÎπÑÎîîÏò§ Ï†ïÎ≥¥",
            f"- ÌååÏùºÎ™Ö: {video.original_name}",
            f"- Í∏∏Ïù¥: {round(video.duration,2)}Ï¥à",
            f"- Î∂ÑÏÑù ÏÉÅÌÉú: {video.analysis_status}",
            f"- Ïî¨ Ïàò: {sc}Í∞ú",
            f"- Î∂ÑÏÑù ÌîÑÎ†àÏûÑ: {fc}Í∞ú",
        ])


    def _enhance_person_detection_with_gender(self, frame_data):
        """ÏÇ¨Îûå Í∞êÏßÄ Îç∞Ïù¥ÌÑ∞Ïóê ÏÑ±Î≥Ñ Ï†ïÎ≥¥ Î≥¥Í∞ï (Î∂ÑÏÑù ÏãúÏ†êÏóêÏÑú Ìò∏Ï∂ú)"""
        try:
            if not frame_data or not isinstance(frame_data, list):
                return frame_data
            
            enhanced_data = []
            for obj in frame_data:
                if not isinstance(obj, dict) or obj.get('class') != 'person':
                    enhanced_data.append(obj)
                    continue
                
                enhanced_obj = obj.copy()
                
                # Í∏∞Ï°¥ ÏÑ±Î≥Ñ Ï†ïÎ≥¥Í∞Ä ÏóÜÎäî Í≤ΩÏö∞ÏóêÎßå Ï∂îÏ†ï
                if not enhanced_obj.get('gender'):
                    # Ïó¨Í∏∞ÏÑú Ï∂îÍ∞ÄÏ†ÅÏù∏ ÏÑ±Î≥Ñ Î∂ÑÏÑù Î°úÏßÅÏùÑ Íµ¨ÌòÑÌï† Ïàò ÏûàÏùå
                    # Ïòà: ÏùòÎ≥µ, Ï≤¥Ìòï, Î®∏Î¶¨Ïπ¥ÎùΩ Îì± Í∏∞Î∞ò Ìú¥Î¶¨Ïä§Ìã±
                    
                    # ÏûÑÏãú: ÎûúÎç§ÌïòÍ≤å ÏÑ±Î≥Ñ Ìï†Îãπ (Ïã§Ï†úÎ°úÎäî Îçî Ï†ïÍµêÌïú Î∂ÑÏÑù ÌïÑÏöî)
                    import random
                    if random.random() < 0.3:  # 30% ÌôïÎ•†Î°ú ÏÑ±Î≥Ñ Ï∂îÏ†ï
                        enhanced_obj['gender'] = random.choice(['male', 'female'])
                        enhanced_obj['gender_confidence'] = 0.6  # ÎÇÆÏùÄ Ïã†Î¢∞ÎèÑ
                    else:
                        enhanced_obj['gender'] = 'unknown'
                        enhanced_obj['gender_confidence'] = 0.0
                
                enhanced_data.append(enhanced_obj)
            
            return enhanced_data
        except Exception as e:
            logger.warning(f"ÏÑ±Î≥Ñ Ï†ïÎ≥¥ Î≥¥Í∞ï Ïã§Ìå®: {e}")
            return frame_data

    def _get_detected_objects(self, frame: Frame):
        """
        Frame Í∞ùÏ≤¥ Ï∂îÏ∂ú Ïãú ÏÑ±Î≥Ñ Ï†ïÎ≥¥ Ï≤òÎ¶¨ Í∞úÏÑ†
        """
        import json

        candidates = []

        # 1) detected_objects
        if hasattr(frame, 'detected_objects') and frame.detected_objects:
            candidates.append(frame.detected_objects)

        # 2) comprehensive_features.objects  
        if hasattr(frame, 'comprehensive_features') and frame.comprehensive_features:
            objs = None
            if isinstance(frame.comprehensive_features, dict):
                objs = frame.comprehensive_features.get('objects') \
                or frame.comprehensive_features.get('detections')
            elif isinstance(frame.comprehensive_features, str):
                try:
                    cf = json.loads(frame.comprehensive_features)
                    objs = (cf or {}).get('objects') or (cf or {}).get('detections')
                except Exception:
                    pass
            if objs:
                candidates.append(objs)

        # 3) Í∏∞ÌÉÄ ÌïÑÎìúÎì§
        for attr in ('yolo_objects', 'detections', 'objects'):
            if hasattr(frame, attr) and getattr(frame, attr):
                candidates.append(getattr(frame, attr))

        # Ï≤´ Î≤àÏß∏ Ïú†Ìö® ÌõÑÎ≥¥ ÏÑ†ÌÉù
        detected = None
        for c in candidates:
            try:
                if isinstance(c, str):
                    c = json.loads(c)
                if isinstance(c, dict):
                    c = c.get('objects') or c.get('detections')
                if isinstance(c, list):
                    detected = c
                    break
            except Exception:
                continue

        if not isinstance(detected, list):
            return []

        # Ï†ïÍ∑úÌôî - ÏÑ±Î≥Ñ Ï†ïÎ≥¥ Ìè¨Ìï®
        norm = []
        for o in detected:
            if not isinstance(o, dict):
                continue
            
            cls = (o.get('class') or o.get('label') or o.get('name') or '').lower()
            bbox = o.get('bbox') or o.get('box') or o.get('xyxy') or []
            conf = float(o.get('confidence') or o.get('score') or 0.0)
            colors = o.get('colors') or o.get('color') or []
            if isinstance(colors, str):
                colors = [colors]
            color_desc = (o.get('color_description') or o.get('dominant_color') or 'unknown')
            track_id = o.get('track_id') or o.get('id')
            
            # ÏÑ±Î≥Ñ Ï†ïÎ≥¥ Ï∂îÏ∂ú Í∞úÏÑ†
            gender = o.get('gender') or o.get('sex') or 'unknown'
            if isinstance(gender, bool):
                gender = 'male' if gender else 'female'
            gender = str(gender).lower()
            
            # ÏÑ±Î≥Ñ Ïã†Î¢∞ÎèÑ
            gender_conf = float(o.get('gender_confidence') or o.get('gender_score') or 0.0)

            norm.append({
                'class': cls,
                'bbox': bbox,
                'confidence': conf,
                'colors': colors,
                'color_description': str(color_desc).lower(),
                'track_id': track_id,
                'gender': gender,
                'gender_confidence': gender_conf,
                '_raw': o,  # ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ÎèÑ Î≥¥Í¥Ä
            })
        return norm
    def _handle_scene_mood(self, video: Video):
        """Ïî¨ ÌÉÄÏûÖ Í∏∞Î∞ò Í∞ÑÎã® Î¨¥Îìú ÏÑ§Î™Ö"""
        try:
            analysis = getattr(video, 'analysis', None)
            if analysis and analysis.analysis_statistics:
                types = (analysis.analysis_statistics.get('scene_types') or [])[:3]
                if types:
                    return f"Î∂ÑÏúÑÍ∏∞: {', '.join(types)}"
        except:
            pass
        return "Î∂ÑÏúÑÍ∏∞ Ï†ïÎ≥¥Î•º ÌååÏïÖÌï† Îã®ÏÑúÍ∞Ä Î∂ÄÏ°±Ìï©ÎãàÎã§."

    def _handle_cross_video(self, raw_text: str):
        """Ïó¨Îü¨ ÏòÅÏÉÅ Ï§ë Ï°∞Í±¥Ïóê ÎßûÎäî ÌõÑÎ≥¥ Î™ÖÏãú (Ïó¨Í∏∞ÏÑ† ÏÑ§Î™ÖÎßå)"""
        return "Ïó¨Îü¨ ÏòÅÏÉÅ Í∞Ñ Ï°∞Í±¥ Í≤ÄÏÉâÏùÄ Ï§ÄÎπÑÎêòÏñ¥ ÏûàÏäµÎãàÎã§. UIÏóêÏÑú Î™©Î°ù/ÌïÑÌÑ∞Î•º Ï†úÍ≥µÌï¥ Ï£ºÏÑ∏Ïöî."
    def _handle_gender_distribution(self, video: Video, slots: dict):
        """ÏÑ±Î≥Ñ Î∂ÑÌè¨ Î∂ÑÏÑù - Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ"""
        tr = slots.get('time_range')
        qs = Frame.objects.filter(video=video)
        if tr and tr.get('start') is not None and tr.get('end') is not None:
            qs = qs.filter(timestamp__gte=tr['start'], timestamp__lte=tr['end'])

        male = female = unknown = 0
        person_detections = []
        
        for f in qs:
            detected_objects = self._get_detected_objects(f)
            for d in detected_objects:
                if d['class'] != 'person': 
                    continue
                
                person_detections.append(d)
                
                # ÏÑ±Î≥Ñ Ï†ïÎ≥¥ Ï∂îÏ∂ú - Ïó¨Îü¨ Î∞©Î≤ï ÏãúÎèÑ
                gender = None
                
                # 1. ÏßÅÏ†ëÏ†ÅÏù∏ gender ÌïÑÎìú
                if 'gender' in d and d['gender'] and d['gender'] != 'unknown':
                    gender = str(d['gender']).lower()
                
                # 2. ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏÑ±Î≥Ñ Ï†ïÎ≥¥ Ï∞æÍ∏∞
                elif '_raw' in d and d['_raw']:
                    raw = d['_raw']
                    for key in ['gender', 'sex', 'male', 'female']:
                        if key in raw and raw[key]:
                            val = str(raw[key]).lower()
                            if val in ['male', 'man', 'm', 'true'] and key in ['male', 'gender']:
                                gender = 'male'
                                break
                            elif val in ['female', 'woman', 'f', 'true'] and key in ['female', 'gender']:
                                gender = 'female'  
                                break
                            elif val in ['male', 'female']:
                                gender = val
                                break
                
                # 3. ÏÉâÏÉÅ/ÏùòÎ≥µ Í∏∞Î∞ò Ìú¥Î¶¨Ïä§Ìã± Ï∂îÏ†ï (Î≥¥Ï°∞Ï†Å)
                if not gender:
                    color_desc = d.get('color_description', '').lower()
                    colors = [str(c).lower() for c in d.get('colors', [])]
                    
                    # Í∞ÑÎã®Ìïú Ìú¥Î¶¨Ïä§Ìã± (Ï†ïÌôïÎèÑ ÎÇÆÏùå, Ï∞∏Í≥†Ïö©)
                    if any('pink' in x for x in [color_desc] + colors):
                        gender = 'female_guess'
                    elif any('blue' in x for x in [color_desc] + colors):
                        gender = 'male_guess'
                
                # Ïπ¥Ïö¥ÌåÖ
                if gender in ['male', 'male_guess']:
                    male += 1
                elif gender in ['female', 'female_guess']:
                    female += 1
                else:
                    unknown += 1

        total = male + female + unknown
        
        if total == 0:
            return "ÏòÅÏÉÅÏóêÏÑú ÏÇ¨ÎûåÏùÑ Í∞êÏßÄÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§."
        
        # Í≤∞Í≥º Ìè¨Îß∑ÌåÖ
        def pct(x): 
            return round(100.0 * x / total, 1) if total > 0 else 0
        
        result = f"ÏÑ±ÎπÑ Î∂ÑÏÑù Í≤∞Í≥º (Ï¥ù {total}Î™Ö Í∞êÏßÄ):\n"
        result += f"üë® ÎÇ®ÏÑ±: {male}Î™Ö ({pct(male)}%)\n"
        result += f"üë© Ïó¨ÏÑ±: {female}Î™Ö ({pct(female)}%)\n"
        result += f"‚ùì ÎØ∏ÏÉÅ: {unknown}Î™Ö ({pct(unknown)}%)\n\n"
        
        # Ï∂îÍ∞Ä Ï†ïÎ≥¥
        if unknown > total * 0.8:  # 80% Ïù¥ÏÉÅÏù¥ ÎØ∏ÏÉÅÏù∏ Í≤ΩÏö∞
            result += "üí° ÏÑ±Î≥Ñ Ï∂îÏ†ï Ï†ïÌôïÎèÑÍ∞Ä ÎÇÆÏäµÎãàÎã§. Ïù¥Îäî Îã§Ïùå Ïù¥Ïú†Ïùº Ïàò ÏûàÏäµÎãàÎã§:\n"
            result += "- ÏòÅÏÉÅ Ìï¥ÏÉÅÎèÑÎÇò Í∞ÅÎèÑ Î¨∏Ï†ú\n"
            result += "- ÏÇ¨ÎûåÏù¥ Î©ÄÎ¶¨ ÏûàÍ±∞ÎÇò Î∂ÄÎ∂ÑÏ†ÅÏúºÎ°úÎßå Î≥¥ÏûÑ\n"
            result += "- AI Î™®Îç∏Ïùò ÏÑ±Î≥Ñ Î∂ÑÏÑù Í∏∞Îä• Ï†úÌïú\n\n"
        
        # ÎîîÎ≤ÑÍπÖ Ï†ïÎ≥¥ (Í∞úÎ∞ú ÏãúÏóêÎßå ÌëúÏãú)
        result += f"üîç ÎîîÎ≤ÑÍ∑∏ Ï†ïÎ≥¥:\n"
        result += f"- Ï≤òÎ¶¨Îêú ÌîÑÎ†àÏûÑ Ïàò: {qs.count()}Í∞ú\n"
        result += f"- Í∞êÏßÄÎêú person Í∞ùÏ≤¥: {len(person_detections)}Í∞ú\n"
        
        if person_detections:
            sample_detection = person_detections[0]
            result += f"- ÏÉòÌîå Í∞ùÏ≤¥ Ï†ïÎ≥¥: {sample_detection.get('gender', 'N/A')} (Ïã†Î¢∞ÎèÑ: {sample_detection.get('gender_confidence', 0)})\n"
        
        # ÏãúÍ∞Ñ Î≤îÏúÑ Ï†ïÎ≥¥
        if tr:
            result += f"üìÖ Î∂ÑÏÑù Íµ¨Í∞Ñ: {tr.get('start', 'ÏãúÏûë')}~{tr.get('end', 'ÎÅù')}"
        else:
            result += f"üìÖ Î∂ÑÏÑù Íµ¨Í∞Ñ: Ï†ÑÏ≤¥ ÏòÅÏÉÅ"
        
        return result
# views.py (ÎèôÏùº ÌååÏùº ÎÇ¥)
class ClipPreviewView(APIView):
    """ffmpeg Î°ú ÏßßÏùÄ ÎØ∏Î¶¨Î≥¥Í∏∞ ÌÅ¥Î¶Ω ÏÉùÏÑ±/Î∞òÌôò"""
    permission_classes = [AllowAny]

    def get(self, request, video_id, timestamp):
        duration = int(request.GET.get('duration', 4))
        try:
            video = Video.objects.get(id=video_id)
        except Video.DoesNotExist:
            raise Http404("video not found")

        src_path = getattr(getattr(video, 'file', None), 'path', None)
        if not src_path or not os.path.exists(src_path):
            raise Http404("file not found")

        tmp_dir = tempfile.mkdtemp()
        out_path = os.path.join(tmp_dir, f"clip_{video_id}_{timestamp}.mp4")

        cmd = [
            'ffmpeg','-y',
            '-ss', str(int(timestamp)),
            '-i', src_path,
            '-t', str(duration),
            '-c:v', 'libx264', '-preset', 'veryfast', '-crf', '28',
            '-an',
            out_path
        ]
        try:
            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        except subprocess.CalledProcessError:
            raise Http404("ffmpeg error")

        resp = FileResponse(open(out_path, 'rb'), content_type='video/mp4')
        resp['Content-Disposition'] = f'inline; filename="clip_{video_id}_{timestamp}.mp4"'
        return resp
