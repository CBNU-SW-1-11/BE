from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
import logging
import json
import openai
import anthropic
from groq import Groq
from django.conf import settings
from bs4 import BeautifulSoup
logger = logging.getLogger(__name__)
import re
import json
import logging
import os
from dotenv import load_dotenv

def fetch_and_clean_url(url, timeout=10):
    """
    Ï£ºÏñ¥ÏßÑ URLÏùò HTMLÏùÑ ÏöîÏ≤≠Ìï¥, Ïä§ÌÅ¨Î¶ΩÌä∏¬∑Ïä§ÌÉÄÏùº Ï†úÍ±∞ ÌõÑ ÌÖçÏä§Ìä∏Îßå Î∞òÌôòÌï©ÎãàÎã§.
    """
    resp = requests.get(url, timeout=timeout)
    resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "html.parser")
    # Ïä§ÌÅ¨Î¶ΩÌä∏¬∑Ïä§ÌÉÄÏùº¬∑ÎÑ§ÎπÑÍ≤åÏù¥ÏÖò ÌÉúÍ∑∏ Ï†úÍ±∞
    for tag in soup(["script", "style", "header", "footer", "nav"]):
        tag.decompose()

    text = soup.get_text(separator="\n")
    # Îπà Ï§Ñ Ï†úÍ±∞
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    return "\n".join(lines)

# Add this function to your ChatBot class or as a standalone function
def sanitize_and_parse_json(text, selected_models, responses):
    """
    Sanitize and parse the JSON response from AI models.
    Handles various edge cases and formatting issues.
    """
    import re
    import json
    import logging
    
    logger = logging.getLogger(__name__)
    
    try:
        # Step 1: Basic cleanup
        text = text.strip()
        
        # Step 2: Handle code blocks
        if text.startswith('```json') and '```' in text:
            text = re.sub(r'```json(.*?)```', r'\1', text, flags=re.DOTALL).strip()
        elif text.startswith('```') and text.endswith('```'):
            text = text[3:-3].strip()
            
        # Step 3: Extract JSON object if embedded in other text
        json_pattern = r'({[\s\S]*})'
        json_matches = re.findall(json_pattern, text)
        if json_matches:
            text = json_matches[0]
            
        # Step 4: Handle escaped backslashes in the text
        # First identify all occurrences of escaped backslashes followed by characters like "_"
        text = re.sub(r'\\([_"])', r'\1', text)
        
        # Step 5: Attempt to parse the JSON
        result = json.loads(text)
        
        # Ensure the required fields exist
        required_fields = ["preferredModel", "best_response", "analysis", "reasoning"]
        for field in required_fields:
            if field not in result:
                if field == "best_response" and "bestResponse" in result:
                    result["best_response"] = result["bestResponse"]
                else:
                    result[field] = "" if field != "analysis" else {}
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå JSON ÌååÏã± Ïã§Ìå®: {str(e)}")
        logger.error(f"ÏõêÎ≥∏ ÌÖçÏä§Ìä∏: {text[:200]}..." if len(text) > 200 else text)
        
        # Advanced recovery attempt for malformed JSON
        try:
            # Remove problematic escaped characters
            fixed_text = text.replace("\\_", "_").replace('\\"', '"')
            
            # Try to fix common issues with JSON (missing quotes, commas, etc.)
            fixed_text = re.sub(r'([{,])\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', fixed_text)
            fixed_text = re.sub(r':\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*([,}])', r':"\1"\2', fixed_text)
            
            # Handle unclosed strings
            for match in re.finditer(r':\s*"([^"\\]*(\\.[^"\\]*)*)', fixed_text):
                if not re.search(r':\s*"([^"\\]*(\\.[^"\\]*)*)"', match.group(0)):
                    pos = match.end()
                    fixed_text = fixed_text[:pos] + '"' + fixed_text[pos:]
            
            result = json.loads(fixed_text)
            logger.info("‚úÖ Recovered JSON after fixing format issues")
            return result
        except:
            # Last resort: construct a sensible fallback response
            error_analysis = {}
            for model in selected_models:
                model_lower = model.lower()
                error_analysis[model_lower] = {"Ïû•Ï†ê": "Î∂ÑÏÑù Ïã§Ìå®", "Îã®Ï†ê": "Î∂ÑÏÑù Ïã§Ìå®"}
            
            # Find the largest response to use as best_response
            best_response = ""
            if responses:
                best_response = max(responses.values(), key=len) 
            
            return {
                "preferredModel": "FALLBACK",
                "best_response": best_response,
                "analysis": error_analysis,
                "reasoning": "ÏùëÎãµ Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌïòÏó¨ ÏµúÏ†ÅÏùò ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±ÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§."
            }

# src/chatbot_backend/chat/bot.py

# import logging
# import openai
# import anthropic
# import base64
# import imghdr
# from groq import Groq
# from io import BytesIO

# logger = logging.getLogger(__name__)

# class ChatBot:
#     def __init__(self, api_key, model, api_type):
#         self.conversation_history = []
#         self.api_type = api_type
#         self.api_key = api_key

#         # Anthropic Î©ÄÌã∞Î™®Îã¨ÏùÄ Opus Î™®Îç∏ Í∂åÏû•
#         if api_type == 'anthropic' and not model.startswith('claude-3-opus-20240229'):
#             logger.info(f"Overriding Anthropic model '{model}' to 'claude-3-opus-20240229' for image support")
#             self.model = 'claude-3-opus-20240229'
#         else:
#             self.model = model

#         if api_type == 'openai':
#             openai.api_key = api_key
#         elif api_type == 'anthropic':
#             # Anthropic Python SDK Ï¥àÍ∏∞Ìôî
#             self.client = anthropic.Client(api_key=api_key)
#         elif api_type == 'groq':
#             self.client = Groq(api_key=api_key)
#         else:
#             raise ValueError(f"Unsupported api_type: {api_type}")

#     def chat(self, prompt=None, user_input=None, image_file=None, analysis_mode=None, user_language=None):
#         """
#         prompt       : ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ (ÌÇ§ÏõåÎìú)
#         user_input   : ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ (ÏúÑÏπò Ïù∏Ïûê)
#         image_file   : ÌååÏùº Í∞ùÏ≤¥ (BytesIO, InMemoryUploadedFile Îì±)
#         analysis_mode: 'describe'|'ocr'|'objects'
#         user_language: 'ko','en'
#         """
#         text = prompt if prompt is not None else user_input
#         try:
#             logger.info(f"[{self.api_type}] Received input: {text}")

#             # Î™®Îç∏Î≥Ñ Ìò∏Ï∂ú
#             if self.api_type == 'openai':
#                 # GPT-4 Vision ÏßÄÏõê
#                 params = {
#                     'model': self.model,
#                     'messages': self.conversation_history + [{"role": "user", "content": text}],
#                     'temperature': 0.7,
#                     'max_tokens': 1024
#                 }
#                 if image_file:
#                     params['files'] = [("image", image_file)]
#                 resp = openai.ChatCompletion.create(**params)
#                 assistant_response = resp.choices[0].message.content

#             elif self.api_type == 'anthropic':
#                 # Claude 3 Opus: Ïù¥ÎØ∏ÏßÄ+ÌÖçÏä§Ìä∏ ÏßÄÏõê via Messages API
#                 messages = []
#                 # ÌÜ†ÌÅ∞ Ïàò ÏÑ§Ï†ï
#                 max_tokens = 1024 if image_file else 4096
#                 if image_file:
#                     # Ïù¥ÎØ∏ÏßÄ Î∞îÏù¥ÎÑàÎ¶¨ ÏùΩÍ∏∞ Î∞è ÎØ∏ÎîîÏñ¥ ÌÉÄÏûÖ ÏûêÎèô Í∞êÏßÄ
#                     image_file.seek(0)
#                     data_bytes = image_file.read()
#                     ext = imghdr.what(None, h=data_bytes) or 'jpeg'
#                     mime_map = {
#                         'jpeg': 'image/jpeg', 'jpg': 'image/jpeg',
#                         'png': 'image/png', 'gif': 'image/gif',
#                         'bmp': 'image/bmp', 'webp': 'image/webp'
#                     }
#                     media_type = mime_map.get(ext, 'image/jpeg')
#                     b64 = base64.b64encode(data_bytes).decode('utf-8')

#                     # Ïù¥ÎØ∏ÏßÄ Î∏îÎ°ùÍ≥º ÌÖçÏä§Ìä∏ Î∏îÎ°ùÏùÑ Î¶¨Ïä§Ìä∏Î°ú Íµ¨ÏÑ±
#                     image_block = {
#                         'type': 'image',
#                         'source': {'type': 'base64', 'media_type': media_type, 'data': b64}
#                     }
#                     text_block = {'type': 'text', 'text': text}
#                     content_blocks = [image_block, text_block]

#                     # Îã®Ïùº Î©îÏãúÏßÄÏóê Î∏îÎ°ù Î¶¨Ïä§Ìä∏ Ï†ÑÎã¨
#                     messages.append({'role': 'user', 'content': content_blocks})
#                 else:
#                     # ÌÖçÏä§Ìä∏ Ï†ÑÏö© Î©îÏãúÏßÄ
#                     messages.append({'role': 'user', 'content': [{'type': 'text', 'text': text}]})

#                 # Messages API Ìò∏Ï∂ú
#                 resp = self.client.messages.create(
#                     model=self.model,
#                     messages=messages,
#                     max_tokens=max_tokens
#                 )
#                 # ÏùëÎãµ Î∏îÎ°ùÏóêÏÑú ÌÖçÏä§Ìä∏Îßå Ìï©ÏπòÍ∏∞
#                 assistant_response = ' '.join(getattr(block, 'text', '') for block in resp.content)

#             elif self.api_type == 'groq':
#                 # Groq Chat API
#                 resp = self.client.chat.completions.create(
#                     model=self.model,
#                     messages=self.conversation_history + [{"role": "user", "content": text}],
#                     temperature=0.7,
#                     max_tokens=1024
#                 )
#                 assistant_response = resp.choices[0].message.content

#             else:
#                 raise ValueError(f"Unsupported api_type: {self.api_type}")

#             # ÏùëÎãµ Í∏∞Î°ù Î∞è Î∞òÌôò
#             self.conversation_history.append({"role": "assistant", "content": assistant_response})
#             logger.info(f"[{self.api_type}] Response: {assistant_response[:100]}...")
#             return assistant_response

#         except Exception as e:
#             logger.error(f"Error in chat method ({self.api_type}): {e}", exc_info=True)
#             raise


# paste-2.txt ÏàòÏ†ïÎêú ÎÇ¥Ïö©

# chatbot.py - OpenAI v1.0+ Ìò∏Ìôò Î≤ÑÏ†Ñ
import openai
import anthropic
from groq import Groq
import logging
import json
import asyncio
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)

# sanitize_and_parse_json Ìï®Ïàò (Í∏∞Ï°¥ Ìï®Ïàò Ìè¨Ìï®)
def sanitize_and_parse_json(text, selected_models=None, responses=None):
    """JSON ÏùëÎãµÏùÑ Ï†ïÎ¶¨ÌïòÍ≥† ÌååÏã±ÌïòÎäî Ìï®Ïàò"""
    import re
    try:
        text = text.strip()
        
        # ÏΩîÎìú Î∏îÎ°ù Ï†úÍ±∞
        if text.startswith('```json') and '```' in text:
            text = re.sub(r'```json(.*?)```', r'\1', text, flags=re.DOTALL).strip()
        elif text.startswith('```') and text.endswith('```'):
            text = text[3:-3].strip()
        
        # JSON Ìå®ÌÑ¥ Ï∂îÏ∂ú
        json_pattern = r'({[\s\S]*})'
        json_matches = re.findall(json_pattern, text)
        if json_matches:
            text = json_matches[0]
        
        # Ïù¥Ïä§ÏºÄÏù¥ÌîÑ Î¨∏Ïûê Ï≤òÎ¶¨
        text = re.sub(r'\\([_"])', r'\1', text)
        
        # JSON ÌååÏã±
        result = json.loads(text)
        
        # ÌïÑÏàò ÌïÑÎìú ÌôïÏù∏ Î∞è Î≥¥Ï†ï
        required_fields = ["preferredModel", "best_response", "analysis", "reasoning"]
        for field in required_fields:
            if field not in result:
                if field == "best_response" and "bestResponse" in result:
                    result["best_response"] = result["bestResponse"]
                else:
                    result[field] = "" if field != "analysis" else {}
        
        return result
        
    except Exception as e:
        logger.error(f"JSON ÌååÏã± Ïã§Ìå®: {e}")
        # Ìè¥Î∞± ÏùëÎãµ ÏÉùÏÑ±
        error_analysis = {}
        if selected_models:
            for model in selected_models:
                model_lower = model.lower()
                error_analysis[model_lower] = {"Ïû•Ï†ê": "Î∂ÑÏÑù Ïã§Ìå®", "Îã®Ï†ê": "Î∂ÑÏÑù Ïã§Ìå®"}
        
        return {
            "preferredModel": "ERROR",
            "best_response": max(responses.values(), key=len) if responses else "Î∂ÑÏÑù Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.",
            "analysis": error_analysis,
            "reasoning": "ÏùëÎãµ Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."
        }
import openai
import os 
import anthropic
from groq import Groq
import logging
from .langchain_config import LangChainManager

logger = logging.getLogger(__name__)

class ChatBot:
    def __init__(self, api_key, model, api_type, langchain_manager=None):
        self.conversation_history = []
        self.model = model
        self.api_type = api_type
        self.api_key = api_key
        self.langchain_manager = langchain_manager
        
        # LangChain ÏÇ¨Ïö© Ïó¨Î∂Ä Í≤∞Ï†ï
        self.use_langchain = langchain_manager is not None
        
        if not self.use_langchain:
            # Í∏∞Ï°¥ Î∞©Ïãù Ï¥àÍ∏∞Ìôî
            if api_type == 'openai':
                openai.api_key = api_key
            elif api_type == 'anthropic':
                self.client = anthropic.Anthropic(api_key=api_key)
            elif api_type == 'groq':
                self.client = Groq(api_key=api_key)
        else:
            # LangChain Ï≤¥Ïù∏ ÏÉùÏÑ±
            try:
                if api_type in ['gpt', 'claude']:
                    self.chat_chain = langchain_manager.create_chat_chain(api_type)
                elif api_type == 'groq' or api_type == 'mixtral':
                    # GroqÎäî Î≥ÑÎèÑ Ï≤òÎ¶¨
                    self.groq_llm = langchain_manager.groq_llm if hasattr(langchain_manager, 'groq_llm') else None
                logger.info(f"LangChain Ï≤¥Ïù∏ ÏÉùÏÑ± ÏôÑÎ£å: {api_type}")
            except Exception as e:
                logger.warning(f"LangChain Ï≤¥Ïù∏ ÏÉùÏÑ± Ïã§Ìå®, Í∏∞Ï°¥ Î∞©Ïãù ÏÇ¨Ïö©: {e}")
                self.use_langchain = False
   
    async def chat_async(self, user_input, image_file=None, analysis_mode=None, user_language=None):
        """ÎπÑÎèôÍ∏∞ Ï±ÑÌåÖ Î©îÏÑúÎìú (LangChain Ïö©)"""
        if self.use_langchain:
            return await self._chat_with_langchain(user_input, user_language)
        else:
            return self.chat(user_input, image_file, analysis_mode, user_language)
    
    async def _chat_with_langchain(self, user_input, user_language='ko'):
        """LangChainÏùÑ ÏÇ¨Ïö©Ìïú Ï±ÑÌåÖ"""
        try:
            if self.api_type in ['gpt', 'claude']:
                result = await self.chat_chain.arun(
                    user_input=user_input,
                    user_language=user_language
                )
                return result
            elif self.api_type == 'groq' or self.api_type == 'mixtral':
                if self.groq_llm:
                    prompt = f"ÏÇ¨Ïö©ÏûêÍ∞Ä ÏÑ†ÌÉùÌïú Ïñ∏Ïñ¥Îäî '{user_language}'ÏûÖÎãàÎã§. Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú ÏùëÎãµÌïòÏÑ∏Ïöî.\n\n{user_input}"
                    result = self.groq_llm(prompt)
                    return result
                else:
                    # Ìè¥Î∞±: Í∏∞Ï°¥ Î∞©Ïãù
                    return self.chat(user_input, user_language=user_language)
            else:
                raise ValueError(f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî API ÌÉÄÏûÖ: {self.api_type}")
                
        except Exception as e:
            logger.error(f"LangChain Ï±ÑÌåÖ ÏóêÎü¨: {e}")
            # Ìè¥Î∞±: Í∏∞Ï°¥ Î∞©Ïãù
            return self.chat(user_input, user_language=user_language)

    def chat(self, user_input, image_file=None, analysis_mode=None, user_language=None):
        """Í∏∞Ï°¥ ÎèôÍ∏∞ Ï±ÑÌåÖ Î©îÏÑúÎìú (Ìò∏ÌôòÏÑ± Ïú†ÏßÄ)"""
        try:
            logger.info(f"Processing chat request for {self.api_type}")
            logger.info(f"User input: {user_input}")
            
            # ÎåÄÌôî Í∏∞Î°ùÏóê ÏÇ¨Ïö©Ïûê ÏûÖÎ†• Ï∂îÍ∞Ä
            if image_file:
                self.conversation_history = [{
                    "role": "system",
                    "content": f"Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Î™®Îìú: {analysis_mode}, ÏùëÎãµ Ïñ∏Ïñ¥: {user_language}"
                }]
                messages = [
                    {"role": "user", "content": user_input}
                ]
            else:
                self.conversation_history.append({"role": "user", "content": user_input})
                messages = self.conversation_history

            try:
                if self.api_type == 'openai':
                    response = openai.ChatCompletion.create(
                        model=self.model,
                        messages=self.conversation_history,
                        temperature=0.7,
                        max_tokens=1024
                    )
                    assistant_response = response['choices'][0]['message']['content']
                    
                elif self.api_type == 'anthropic':
                    try:
                        # ÏãúÏä§ÌÖú Î©îÏãúÏßÄ Ï∞æÍ∏∞
                        system_message = next((msg['content'] for msg in self.conversation_history 
                                            if msg['role'] == 'system'), '')
                        
                        # ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄ Ï∞æÍ∏∞
                        user_content = next((msg['content'] for msg in self.conversation_history 
                                        if msg['role'] == 'user'), '')

                        message = self.client.messages.create(
                            model=self.model,
                            max_tokens=4096,
                            temperature=0,
                            system=system_message,
                            messages=[{
                                "role": "user",
                                "content": user_content
                            }]
                        )
                        assistant_response = message.content[0].text
                        logger.info(f"Anthropic response with system message: {system_message[:100]}")
                        
                    except Exception as e:
                        logger.error(f"Anthropic API error: {str(e)}")
                        raise
               
                elif self.api_type == 'groq':
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=self.conversation_history,
                        temperature=0.7,
                        max_tokens=1024
                    )
                    assistant_response = response.choices[0].message.content
               
                # ÏùëÎãµ Í∏∞Î°ù
                self.conversation_history.append({
                    "role": "assistant",
                    "content": assistant_response
                })
               
                logger.info(f"Generated response: {assistant_response[:100]}...")
                return assistant_response
               
            except Exception as e:
                logger.error(f"API error in {self.api_type}: {str(e)}", exc_info=True)
                raise
               
        except Exception as e:
            logger.error(f"Error in chat method: {str(e)}", exc_info=True)
            raise

    async def analyze_responses_async(self, responses, query, user_language, selected_models):
        """ÎπÑÎèôÍ∏∞ ÏùëÎãµ Î∂ÑÏÑù (LangChain Ïö©)"""
        if self.use_langchain and self.langchain_manager:
            return await self._analyze_with_langchain(responses, query, user_language, selected_models)
        else:
            return self.analyze_responses(responses, query, user_language, selected_models)
    
    async def _analyze_with_langchain(self, responses, query, user_language, selected_models):
        """LangChainÏùÑ ÏÇ¨Ïö©Ìïú ÏùëÎãµ Î∂ÑÏÑù"""
        try:
            logger.info("\n" + "="*100)
            logger.info("üìä LangChain Î∂ÑÏÑù ÏãúÏûë")
            logger.info(f"ü§ñ Î∂ÑÏÑù ÏàòÌñâ AI: {self.api_type.upper()}")
            logger.info(f"üîç ÏÑ†ÌÉùÎêú Î™®Îç∏Îì§: {', '.join(selected_models)}")
            logger.info("="*100)
            
            # Î∂ÑÏÑù Ï≤¥Ïù∏ ÏÉùÏÑ±
            analysis_chain = self.langchain_manager.create_analysis_chain(self.api_type)
            
            # ÏùëÎãµ Ìè¨Îß∑ÌåÖ
            formatted = self.langchain_manager.format_responses_for_analysis(
                responses, selected_models
            )
            
            # Î∂ÑÏÑù Ïã§Ìñâ
            analysis_result = await analysis_chain.arun(
                query=query,
                user_language=user_language,
                selected_models=selected_models,
                **formatted
            )
            
            # preferredModel ÏÑ§Ï†ï
            analysis_result['preferredModel'] = self.api_type.upper()
            
            logger.info("‚úÖ LangChain Î∂ÑÏÑù ÏôÑÎ£å\n")
            return analysis_result
            
        except Exception as e:
            logger.error(f"‚ùå LangChain Î∂ÑÏÑù ÏóêÎü¨: {str(e)}")
            # Ìè¥Î∞±: Í∏∞Ï°¥ Î∞©Ïãù
            return self.analyze_responses(responses, query, user_language, selected_models)

    def analyze_responses(self, responses, query, user_language, selected_models):
        """Í∏∞Ï°¥ ÎèôÍ∏∞ ÏùëÎãµ Î∂ÑÏÑù Î©îÏÑúÎìú (Ìò∏ÌôòÏÑ± Ïú†ÏßÄ)"""
        try:
            logger.info("\n" + "="*100)
            logger.info("üìä Î∂ÑÏÑù ÏãúÏûë")
            logger.info(f"ü§ñ Î∂ÑÏÑù ÏàòÌñâ AI: {self.api_type.upper()}")
            logger.info(f"üîç ÏÑ†ÌÉùÎêú Î™®Îç∏Îì§: {', '.join(selected_models)}")
            logger.info("="*100)

            # ÏÑ†ÌÉùÎêú Î™®Îç∏Îì§Îßå Î∂ÑÏÑùÏóê Ìè¨Ìï®
            responses_section = ""
            analysis_section = ""
            
            for model in selected_models:
                model_lower = model.lower()
                responses_section += f"\n{model.upper()} ÏùëÎãµ: Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú ÏûëÏÑ± {responses.get(model_lower, 'ÏùëÎãµ ÏóÜÏùå')}"
                
                analysis_section += f"""
                        "{model_lower}": {{
                            "Ïû•Ï†ê": "Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú ÏûëÏÑ± {model.upper()} ÎãµÎ≥ÄÏùò Ïû•Ï†ê",
                            "Îã®Ï†ê": "Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú ÏûëÏÑ± {model.upper()} ÎãµÎ≥ÄÏùò Îã®Ï†ê"
                        }}{"," if model_lower != selected_models[-1].lower() else ""}"""

            # Í∏∞Ï°¥ Î∂ÑÏÑù ÌîÑÎ°¨ÌîÑÌä∏ (Î≥ÄÍ≤Ω ÏóÜÏùå)
            analysis_prompt = f"""Îã§ÏùåÏùÄ ÎèôÏùºÌïú ÏßàÎ¨∏Ïóê ÎåÄÌïú {len(selected_models)}Í∞ÄÏßÄ AIÏùò ÏùëÎãµÏùÑ Î∂ÑÏÑùÌïòÎäî Í≤ÉÏûÖÎãàÎã§.
                    ÏÇ¨Ïö©ÏûêÍ∞Ä ÏÑ†ÌÉùÌïú Ïñ∏Ïñ¥Îäî '{user_language}'ÏûÖÎãàÎã§.
                    Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú ÏµúÏ†ÅÏùò ÎãµÏùÑ ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
                    Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú Ïû•Ï†êÏùÑ ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
                    Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú Îã®Ï†êÏùÑ ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
                    Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú Î∂ÑÏÑù Í∑ºÍ±∞Î•º ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.

                    ÏßàÎ¨∏: {query}
                    {responses_section}

                     [ÏµúÏ†ÅÏùò ÏùëÎãµÏùÑ ÎßåÎì§ Îïå Í≥†Î†§Ìï† ÏÇ¨Ìï≠]
                    - Î™®Îì† AIÏùò ÎãµÎ≥ÄÎì§ÏùÑ Ï¢ÖÌï©ÌïòÏó¨ ÏµúÏ†ÅÏùò ÎãµÎ≥ÄÏúºÎ°ú Î∞òÎìúÏãú Ïû¨Íµ¨ÏÑ±Ìï©ÎãàÎã§
                    - Í∏∞Ï°¥ AIÏùò ÎãµÎ≥ÄÏùÑ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©ÌïòÎ©¥ ÏïàÎê©ÎãàÎã§
                    - Ï¶â, Í∏∞Ï°¥ AIÏùò ÎãµÎ≥ÄÍ≥º ÏµúÏ†ÅÏùò ÎãµÎ≥ÄÏù¥ ÎèôÏùºÌïòÎ©¥ ÏïàÎê©ÎãàÎã§.
                    - Îã§ÏàòÏùò AIÍ∞Ä Í≥µÌÜµÏúºÎ°ú Ï†úÍ≥µÌïú Ï†ïÎ≥¥Îäî Í∞ÄÏû• Ïã†Î¢∞Ìï† Ïàò ÏûàÎäî Ïò¨Î∞îÎ•∏ Ï†ïÎ≥¥Î°ú Í∞ÑÏ£ºÌï©ÎãàÎã§
                    - ÏΩîÎìúÎ•º Î¨ªÎäî ÏßàÎ¨∏ÏùºÎïåÎäî, AIÏùò ÎãµÎ≥Ä Ï§ë Ï†úÏùº Ï¢ãÏùÄ ÎãµÎ≥ÄÏùÑ ÏÑ†ÌÉùÌï¥ÏÑú Ïû¨Íµ¨ÏÑ±Ìï¥Ï§ò
                    - Î∞òÎìúÏãú JSON ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî
                    [Ï∂úÎ†• ÌòïÏãù]
                    {{
                        "preferredModel": "{self.api_type.upper()}",
                        "best_response": "ÏµúÏ†ÅÏùò ÎãµÎ≥Ä ({user_language}Î°ú ÏûëÏÑ±)",
                        "analysis": {{
                            {analysis_section}
                        }},
                        "reasoning": "Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú ÏûëÏÑ± ÏµúÏ†ÅÏùò ÏùëÎãµÏùÑ ÏÑ†ÌÉùÌïú Ïù¥Ïú†"
                    }}"""

            # Í∏∞Ï°¥ API Ìò∏Ï∂ú Î°úÏßÅ (Î≥ÄÍ≤Ω ÏóÜÏùå)
            if self.api_type == 'openai':
                response = openai.ChatCompletion.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant. Always respond with valid JSON ONLY, no additional text or explanations."},
                        {"role": "user", "content": analysis_prompt}
                    ],
                    temperature=0,
                    max_tokens=4096
                )
                analysis_text = response['choices'][0]['message']['content']
                
            elif self.api_type == 'anthropic':
                system_message = next((msg['content'] for msg in self.conversation_history 
                                    if msg['role'] == 'system'), '')
                
                message = self.client.messages.create(
                    model=self.model,
                    max_tokens=4096,
                    temperature=0,
                    system=f"{system_message}\nYou must respond with valid JSON only in the specified language. No other text or formatting.",
                    messages=[{
                        "role": "user", 
                        "content": analysis_prompt
                    }]
                )
                analysis_text = message.content[0].text.strip()
            
            elif self.api_type == 'groq':
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant. Always respond with valid JSON ONLY, no additional text or explanations."},
                        {"role": "user", "content": analysis_prompt}
                    ],
                    temperature=0,
                    max_tokens=4096
                )
                analysis_text = response.choices[0].message.content

            logger.info("‚úÖ Î∂ÑÏÑù ÏôÑÎ£å\n")
            
            # JSON ÌååÏã± (Í∏∞Ï°¥ Ìï®Ïàò ÏÇ¨Ïö©)
            from paste_3 import sanitize_and_parse_json  # Í∏∞Ï°¥ Ìï®Ïàò import
            analysis_result = sanitize_and_parse_json(analysis_text, selected_models, responses)
            analysis_result['preferredModel'] = self.api_type.upper()
            
            return analysis_result
        
        except Exception as e:
            logger.error(f"‚ùå Analysis error: {str(e)}")
            # Í∏∞Ï°¥ Ìè¥Î∞± Î°úÏßÅ
            error_analysis = {}
            for model in selected_models:
                model_lower = model.lower()
                error_analysis[model_lower] = {"Ïû•Ï†ê": "Î∂ÑÏÑù Ïã§Ìå®", "Îã®Ï†ê": "Î∂ÑÏÑù Ïã§Ìå®"}
            
            return {
                "preferredModel": self.api_type.upper(),
                "best_response": max(responses.values(), key=len) if responses else "",
                "analysis": error_analysis,
                "reasoning": "ÏùëÎãµ Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌïòÏó¨ ÏµúÏ†ÅÏùò ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±ÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§."
            }
# class ChatView(APIView):
#     permission_classes = [AllowAny]

#     def post(self, request, preferredModel):
#         try:
#             logger.info(f"Received chat request for {preferredModel}")
            
#             data = request.data
#             user_message = data.get('message')
#             compare_responses = data.get('compare', True)
            
#             # ÏÉàÎ°úÏö¥ ÌååÎùºÎØ∏ÌÑ∞: ÏÑ†ÌÉùÎêú Î™®Îç∏Îì§
#             selected_models = data.get('selectedModels', ['gpt', 'claude', 'mixtral'])
            
#             # ÏÑ†ÌÉùÎêú Î™®Îç∏ Î°úÍ∑∏
#             logger.info(f"Selected models: {selected_models}")
            
#             # ÌÜ†ÌÅ∞ Ïú†Î¨¥Ïóê Îî∞Î•∏ Ïñ∏Ïñ¥ Î∞è ÏÑ†Ìò∏ Î™®Îç∏ Ï≤òÎ¶¨
#             token = request.headers.get('Authorization')
#             if not token:
#                 # ÎπÑÎ°úÍ∑∏Ïù∏: Í∏∞Î≥∏ Ïñ∏Ïñ¥Îäî ko, ÏÑ†Ìò∏ Î™®Îç∏ÏùÄ GPTÎ°ú Í≥†Ï†ï
#                 user_language = 'ko'
#                 preferredModel = 'gpt'
#             else:
#                 # Î°úÍ∑∏Ïù∏: ÏöîÏ≤≠ Îç∞Ïù¥ÌÑ∞Ïùò Ïñ∏Ïñ¥ ÏÇ¨Ïö© (ÌòπÏùÄ ÏÇ¨Ïö©ÏûêÏùò ÏÑ§Ï†ïÏùÑ Îî∞Î¶Ñ)
#                 user_language = data.get('language', 'ko')
#                 # URLÏóê Ï†ÑÎã¨Îêú preferredModelÏùÑ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö© (ÌîÑÎ°†Ìä∏ÏóîÎìúÏóêÏÑú ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ï Î∞òÏòÅ)

#             logger.info(f"Received language setting: {user_language}")

#             if not user_message:
#                 return Response({'error': 'No message provided'}, 
#                                 status=status.HTTP_400_BAD_REQUEST)

#             # ÎπÑÎèôÍ∏∞ ÏùëÎãµÏùÑ ÏúÑÌïú StreamingHttpResponse ÏÇ¨Ïö©
#             from django.http import StreamingHttpResponse
#             import json
#             import time

#             def stream_responses():
#                 try:
#                     system_message = {
#                         "role": "system",
#                         "content": f"ÏÇ¨Ïö©ÏûêÍ∞Ä ÏÑ†ÌÉùÌïú Ïñ∏Ïñ¥Îäî '{user_language}'ÏûÖÎãàÎã§. Î∞òÎìúÏãú Î™®Îì† ÏùëÎãµÏùÑ Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú Ï†úÍ≥µÌï¥Ï£ºÏÑ∏Ïöî."
#                     }
                    
#                     responses = {}
                    
#                     # ÌòÑÏû¨ ÏöîÏ≤≠Ïóê ÎåÄÌïú Í≥†Ïú† ÏãùÎ≥ÑÏûê ÏÉùÏÑ± (ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ ÌôúÏö©)
#                     request_id = str(time.time())
                    
#                     # ÏÑ†ÌÉùÎêú Î™®Îç∏Îì§Îßå ÎåÄÌôîÏóê Ï∞∏Ïó¨ÏãúÌÇ¥
#                     selected_chatbots = {model: chatbots.get(model) for model in selected_models if model in chatbots}
                    
#                     # Í∞Å Î¥áÏùò ÏùëÎãµÏùÑ Í∞úÎ≥ÑÏ†ÅÏúºÎ°ú Ï≤òÎ¶¨ÌïòÍ≥† Ï¶âÏãú ÏùëÎãµ
#                     for bot_id, bot in selected_chatbots.items():
#                         if bot is None:
#                             logger.warning(f"Selected model {bot_id} not available in chatbots")
#                             yield json.dumps({
#                                 'type': 'bot_error',
#                                 'botId': bot_id,
#                                 'error': f"Model {bot_id} is not available"
#                             }) + '\n'
#                             continue
                            
#                         try:
#                             # Îß§Î≤à ÏÉàÎ°úÏö¥ ÎåÄÌôî Ïª®ÌÖçÏä§Ìä∏ ÏÉùÏÑ± (Ïù¥Ï†Ñ ÎÇ¥Ïö© Ï¥àÍ∏∞Ìôî)
#                             bot.conversation_history = [system_message]
#                             response = bot.chat(user_message)
#                             responses[bot_id] = response
                            
#                             # Í∞Å Î¥á ÏùëÎãµÏùÑ Ï¶âÏãú Ï†ÑÏÜ°
#                             yield json.dumps({
#                                 'type': 'bot_response',
#                                 'botId': bot_id,
#                                 'response': response,
#                                 'requestId': request_id  # ÏöîÏ≤≠ ID Ï∂îÍ∞Ä
#                             }) + '\n'
                            
#                         except Exception as e:
#                             logger.error(f"Error from {bot_id}: {str(e)}")
#                             responses[bot_id] = f"Error: {str(e)}"
                            
#                             # ÏóêÎü¨ÎèÑ Ï¶âÏãú Ï†ÑÏÜ°
#                             yield json.dumps({
#                                 'type': 'bot_error',
#                                 'botId': bot_id,
#                                 'error': str(e),
#                                 'requestId': request_id  # ÏöîÏ≤≠ ID Ï∂îÍ∞Ä
#                             }) + '\n'
                    
#                     # ÏÑ†ÌÉùÎêú Î™®Îç∏Ïù¥ ÏûàÍ≥† ÏùëÎãµÏù¥ ÏûàÏùÑ ÎïåÎßå Î∂ÑÏÑù ÏàòÌñâ
#                     if selected_models and responses:
#                         # Î∂ÑÏÑù(ÎπÑÍµê)ÏùÄ Î°úÍ∑∏Ïù∏ Ïãú ÏÇ¨Ïö©ÏûêÏùò ÏÑ†Ìò∏ Î™®Îç∏ÏùÑ, ÎπÑÎ°úÍ∑∏Ïù∏ Ïãú GPTÎ•º ÏÇ¨Ïö©
#                         if token:
#                             analyzer_bot = chatbots.get(preferredModel) or chatbots.get('gpt')
#                         else:
#                             analyzer_bot = chatbots.get('gpt')
                        
#                         # Î∂ÑÏÑùÏö© Î¥áÎèÑ ÏÉàÎ°úÏö¥ ÎåÄÌôî Ïª®ÌÖçÏä§Ìä∏Î°ú Ï¥àÍ∏∞Ìôî
#                         analyzer_bot.conversation_history = [system_message]
                        
#                         # Î∂ÑÏÑù Ïã§Ìñâ (Ìï≠ÏÉÅ ÏÉàÎ°≠Í≤å Ïã§Ìñâ)
#                         analysis = analyzer_bot.analyze_responses(responses, user_message, user_language, selected_models)
                        
#                         # Î∂ÑÏÑù Í≤∞Í≥º Ï†ÑÏÜ°
#                         yield json.dumps({
#                             'type': 'analysis',
#                             'preferredModel': analyzer_bot and analyzer_bot.api_type.upper(),
#                             'best_response': analysis.get('best_response', ''),
#                             'analysis': analysis.get('analysis', {}),
#                             'reasoning': analysis.get('reasoning', ''),
#                             'language': user_language,
#                             'requestId': request_id,  # ÏöîÏ≤≠ ID Ï∂îÍ∞Ä
#                             'timestamp': time.time()  # ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ Ï∂îÍ∞Ä
#                         }) + '\n'
#                     else:
#                         logger.warning("No selected models or responses to analyze")
                    
#                 except Exception as e:
#                     logger.error(f"Stream processing error: {str(e)}", exc_info=True)
#                     yield json.dumps({
#                         'type': 'error',
#                         'error': f"Stream processing error: {str(e)}"
#                     }) + '\n'

#             # StreamingHttpResponse Î∞òÌôò
#             response = StreamingHttpResponse(
#                 streaming_content=stream_responses(),
#                 content_type='text/event-stream'
#             )
#             response['Cache-Control'] = 'no-cache'
#             response['X-Accel-Buffering'] = 'no'
#             return response
                
#         except Exception as e:
#             logger.error(f"Unexpected error: {str(e)}", exc_info=True)
#             return Response({
#                 'error': str(e)
#             }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
from django.http import StreamingHttpResponse
import logging
import json
import openai
import anthropic
from groq import Groq
from django.conf import settings
from bs4 import BeautifulSoup
import re
import time
import asyncio
from asgiref.sync import sync_to_async

# ÏÉàÎ°ú Ï∂îÍ∞ÄÎêú import
from .langchain_config import LangChainManager
from .langgraph_workflow import AIComparisonWorkflow

logger = logging.getLogger(__name__)

def convert_to_serializable(obj):
    """Í∞ùÏ≤¥Î•º ÏßÅÎ†¨Ìôî Í∞ÄÎä•Ìïú ÌòïÌÉúÎ°ú Î≥ÄÌôò"""
    if hasattr(obj, '__dict__'):
        return {k: convert_to_serializable(v) for k, v in obj.__dict__.items()}
    elif isinstance(obj, dict):
        return {k: convert_to_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_serializable(item) for item in obj]
    else:
        return obj

class ChatView(APIView):
    permission_classes = [AllowAny]

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # Í∏∞Ï°¥ Ïú†ÏÇ¨ÎèÑ Î∂ÑÏÑùÍ∏∞
        from .similarity_analyzer import SimilarityAnalyzer  # Ïã§Ï†ú import Í≤ΩÎ°úÎ°ú Î≥ÄÍ≤Ω ÌïÑÏöî
        self.similarity_analyzer = SimilarityAnalyzer(threshold=0.85)
        
        # LangChain Í¥ÄÎ¶¨Ïûê Ï¥àÍ∏∞Ìôî
        self.langchain_manager = LangChainManager(
            openai_key=OPENAI_API_KEY,
            anthropic_key=ANTHROPIC_API_KEY,
            groq_key=GROQ_API_KEY
        )
        
        # LangGraph ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ï¥àÍ∏∞Ìôî
        self.workflow = AIComparisonWorkflow(
            langchain_manager=self.langchain_manager,
            similarity_analyzer=self.similarity_analyzer
        )
        
        # Í∏∞Ï°¥ ChatBot Ïù∏Ïä§ÌÑ¥Ïä§Îì§ÎèÑ LangChain ÏÇ¨Ïö©ÌïòÎèÑÎ°ù ÏóÖÎç∞Ïù¥Ìä∏
        self.update_chatbots_with_langchain()

    def update_chatbots_with_langchain(self):
        """Í∏∞Ï°¥ ChatBotÎì§ÏùÑ LangChainÏùÑ ÏÇ¨Ïö©ÌïòÎèÑÎ°ù ÏóÖÎç∞Ïù¥Ìä∏"""
        global chatbots
        
        # Í∏∞Ï°¥ ChatBotÎì§Ïóê LangChain Îß§ÎãàÏ†Ä Ï∂îÍ∞Ä
        for bot_id, bot in chatbots.items():
            bot.langchain_manager = self.langchain_manager
            bot.use_langchain = True
            
            # LangChain Ï≤¥Ïù∏ ÏÉùÏÑ± ÏãúÎèÑ
            try:
                if bot_id == 'gpt':
                    bot.chat_chain = self.langchain_manager.create_chat_chain('gpt')
                elif bot_id == 'claude':
                    bot.chat_chain = self.langchain_manager.create_chat_chain('claude')
                elif bot_id == 'mixtral':
                    bot.groq_llm = self.langchain_manager.groq_llm if hasattr(self.langchain_manager, 'groq_llm') else None
                logger.info(f"LangChain Ï≤¥Ïù∏ ÏÉùÏÑ± ÏôÑÎ£å: {bot_id}")
            except Exception as e:
                logger.warning(f"LangChain Ï≤¥Ïù∏ ÏÉùÏÑ± Ïã§Ìå® ({bot_id}), Í∏∞Ï°¥ Î∞©Ïãù ÏÇ¨Ïö©: {e}")
                bot.use_langchain = False

    def post(self, request, preferredModel):
        try:
            logger.info(f"Received chat request for {preferredModel}")
            data = request.data
            user_message = data.get('message')
            selected_models = data.get('selectedModels', ['gpt', 'claude', 'mixtral'])
            token = request.headers.get('Authorization')
            user_language = 'ko' if not token else data.get('language', 'ko')
            use_workflow = data.get('useWorkflow', True)  # ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏÇ¨Ïö© Ïó¨Î∂Ä
            
            if not user_message:
                return Response({'error': 'No message provided'}, status=status.HTTP_400_BAD_REQUEST)

            # URL Ï≤òÎ¶¨ Î°úÏßÅ (Í∏∞Ï°¥Í≥º ÎèôÏùº)
            url_pattern = r'^(https?://\S+)$'
            match = re.match(url_pattern, user_message.strip())
            if match:
                url = match.group(1)
                try:
                    page_text = fetch_and_clean_url(url)
                    if len(page_text) > 10000:
                        page_text = page_text[:5000] + "\n\n‚Ä¶(Ï§ëÎûµ)‚Ä¶\n\n" + page_text[-5000:]
                    user_message = (
                        f"Îã§Ïùå ÏõπÌéòÏù¥ÏßÄÏùò ÎÇ¥Ïö©ÏùÑ Î∂ÑÏÑùÌï¥ Ï£ºÏÑ∏Ïöî:\n"
                        f"URL: {url}\n\n"
                        f"{page_text}"
                    )
                except Exception as e:
                    logger.error(f"URL fetch error: {e}")
                    return Response({'error': f"URLÏùÑ Í∞ÄÏ†∏Ïò§ÏßÄ Î™ªÌñàÏäµÎãàÎã§: {e}"}, status=status.HTTP_400_BAD_REQUEST)

            # ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏÇ¨Ïö© Ïó¨Î∂ÄÏóê Îî∞Î•∏ Î∂ÑÍ∏∞
            if use_workflow:
                return self.handle_with_workflow(user_message, selected_models, user_language, preferredModel)
            else:
                return self.handle_with_legacy(user_message, selected_models, user_language, preferredModel)

        except Exception as e:
            logger.error(f"Unexpected error: {e}", exc_info=True)
            return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def handle_with_workflow(self, user_message, selected_models, user_language, preferred_model):
        """LangGraph ÏõåÌÅ¨ÌîåÎ°úÏö∞Î•º ÏÇ¨Ïö©Ìïú Ï≤òÎ¶¨"""
        def stream_workflow_responses():
            try:
                request_id = str(time.time())
                
                # ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ïã§ÌñâÏùÑ ÏúÑÌïú async ÎûòÌçº
                async def run_workflow_async():
                    return await self.workflow.run_workflow(
                        user_message=user_message,
                        selected_models=selected_models,
                        user_language=user_language,
                        request_id=request_id
                    )
                
                # asyncio Ïù¥Î≤§Ìä∏ Î£®ÌîÑÏóêÏÑú Ïã§Ìñâ
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                try:
                    workflow_result = loop.run_until_complete(run_workflow_async())
                finally:
                    loop.close()
                
                # Í∞úÎ≥Ñ ÏùëÎãµ Ïä§Ìä∏Î¶¨Î∞ç
                for bot_id, response in workflow_result["individual_responses"].items():
                    yield json.dumps({
                        'type': 'bot_response',
                        'botId': bot_id,
                        'response': response,
                        'requestId': request_id
                    }) + '\n'
                
                # Ïú†ÏÇ¨ÎèÑ Î∂ÑÏÑù Í≤∞Í≥º
                if workflow_result["similarity_analysis"]:
                    yield json.dumps({
                        'type': 'similarity_analysis',
                        'result': workflow_result["similarity_analysis"],
                        'requestId': request_id,
                        'timestamp': time.time(),
                        'userMessage': user_message
                    }) + '\n'
                
                # ÏµúÏ¢Ö Î∂ÑÏÑù Í≤∞Í≥º
                final_analysis = workflow_result["final_analysis"]
                yield json.dumps({
                    'type': 'analysis',
                    'preferredModel': final_analysis.get('preferredModel', preferred_model.upper()),
                    'best_response': final_analysis.get('best_response', ''),
                    'analysis': final_analysis.get('analysis', {}),
                    'reasoning': final_analysis.get('reasoning', ''),
                    'language': user_language,
                    'requestId': request_id,
                    'timestamp': time.time(),
                    'userMessage': user_message,
                    'workflowUsed': True,
                    'errors': workflow_result.get("errors", [])
                }) + '\n'
                
            except Exception as e:
                logger.error(f"ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ïä§Ìä∏Î¶¨Î∞ç ÏóêÎü¨: {e}")
                yield json.dumps({
                    'type': 'error',
                    'error': f"Workflow error: {e}",
                    'fallbackToLegacy': True
                }) + '\n'

        return StreamingHttpResponse(stream_workflow_responses(), content_type='text/event-stream')

    def handle_with_legacy(self, user_message, selected_models, user_language, preferred_model):
        """Í∏∞Ï°¥ Î∞©ÏãùÏúºÎ°ú Ï≤òÎ¶¨ (Ìò∏ÌôòÏÑ± Ïú†ÏßÄ)"""
        def stream_responses():
            try:
                system_message = {
                    'role': 'system',
                    'content': f"ÏÇ¨Ïö©ÏûêÍ∞Ä ÏÑ†ÌÉùÌïú Ïñ∏Ïñ¥Îäî '{user_language}'ÏûÖÎãàÎã§. Î∞òÎìúÏãú Ïù¥ Ïñ∏Ïñ¥({user_language})Î°ú ÏùëÎãµÌïòÏÑ∏Ïöî."
                }
                responses = {}
                request_id = str(time.time())
                
                # Í∞Å Î™®Îç∏Î≥Ñ Ï±óÎ¥á Ïù∏Ïä§ÌÑ¥Ïä§ Í∞ÄÏ†∏Ïò§Í∏∞
                selected_chatbots = {m: chatbots.get(m) for m in selected_models if chatbots.get(m)}

                # Î™®Îç∏ ÏùëÎãµ ÏàòÏßë (ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨ ÏãúÎèÑ)
                async def collect_responses_async():
                    responses = {}
                    tasks = []
                    
                    for bot_id, bot in selected_chatbots.items():
                        if hasattr(bot, 'chat_async') and bot.use_langchain:
                            # LangChain ÎπÑÎèôÍ∏∞ ÏÇ¨Ïö©
                            task = bot.chat_async(user_message, user_language=user_language)
                        else:
                            # Í∏∞Ï°¥ ÎèôÍ∏∞ Î∞©ÏãùÏùÑ ÎπÑÎèôÍ∏∞Î°ú ÎûòÌïë
                            task = sync_to_async(self.sync_chat)(bot, user_message, system_message)
                        tasks.append((bot_id, task))
                    
                    for bot_id, task in tasks:
                        try:
                            response = await task
                            responses[bot_id] = response
                            logger.info(f"‚úÖ {bot_id} ÏùëÎãµ ÏôÑÎ£å")
                        except Exception as e:
                            logger.error(f"‚ùå {bot_id} ÏùëÎãµ Ïã§Ìå®: {e}")
                    
                    return responses

                # ÎπÑÎèôÍ∏∞ ÏùëÎãµ ÏàòÏßë
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                try:
                    responses = loop.run_until_complete(collect_responses_async())
                finally:
                    loop.close()

                # Í∞úÎ≥Ñ ÏùëÎãµ Ïä§Ìä∏Î¶¨Î∞ç
                for bot_id, resp_text in responses.items():
                    yield json.dumps({
                        'type': 'bot_response',
                        'botId': bot_id,
                        'response': resp_text,
                        'requestId': request_id
                    }) + '\n'

                # Ïú†ÏÇ¨ÎèÑ Î∂ÑÏÑù
                if len(responses) >= 2:
                    sim_res = self.similarity_analyzer.cluster_responses(responses)
                    serial = convert_to_serializable(sim_res)
                    yield json.dumps({
                        'type': 'similarity_analysis',
                        'result': serial,
                        'requestId': request_id,
                        'timestamp': time.time(),
                        'userMessage': user_message
                    }) + '\n'

                # ÏµúÏ¢Ö ÎπÑÍµê Î∞è Î∂ÑÏÑù
                analyzer_bot = chatbots.get(preferred_model) or chatbots.get('gpt')
                analyzer_bot.conversation_history = [system_message]
                
                # LangChain ÎπÑÎèôÍ∏∞ Î∂ÑÏÑù ÏãúÎèÑ
                if hasattr(analyzer_bot, 'analyze_responses_async') and analyzer_bot.use_langchain:
                    async def analyze_async():
                        return await analyzer_bot.analyze_responses_async(
                            responses, user_message, user_language, list(responses.keys())
                        )
                    
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    
                    try:
                        analysis = loop.run_until_complete(analyze_async())
                    finally:
                        loop.close()
                else:
                    # Í∏∞Ï°¥ ÎèôÍ∏∞ Î∞©Ïãù
                    analysis = analyzer_bot.analyze_responses(
                        responses, user_message, user_language, list(responses.keys())
                    )
                
                yield json.dumps({
                    'type': 'analysis',
                    'preferredModel': analyzer_bot.api_type.upper(),
                    'best_response': analysis.get('best_response', ''),
                    'analysis': analysis.get('analysis', {}),
                    'reasoning': analysis.get('reasoning', ''),
                    'language': user_language,
                    'requestId': request_id,
                    'timestamp': time.time(),
                    'userMessage': user_message,
                    'workflowUsed': False
                }) + '\n'
                
            except Exception as e:
                yield json.dumps({
                    'type': 'error',
                    'error': f"Stream error: {e}"
                }) + '\n'

        return StreamingHttpResponse(stream_responses(), content_type='text/event-stream')

    def sync_chat(self, bot, user_message, system_message):
        """ÎèôÍ∏∞ Ï±ÑÌåÖÏùÑ ÏúÑÌïú Ìó¨Ìçº Î©îÏÑúÎìú"""
        bot.conversation_history = [system_message]
        return bot.chat(user_message)
from dotenv import load_dotenv
load_dotenv()
# API ÌÇ§ ÏÑ§Ï†ï (Í∏∞Ï°¥Í≥º ÎèôÏùº)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
# ChatBot import (ÏàòÏ†ïÎêú Î≤ÑÏ†Ñ)

chatbots = {
    'gpt': ChatBot(OPENAI_API_KEY, 'gpt-3.5-turbo', 'openai'),
    'claude': ChatBot(ANTHROPIC_API_KEY, 'claude-3-5-haiku-20241022', 'anthropic'), 
    'mixtral': ChatBot(GROQ_API_KEY, 'llama3-8b-8192', 'groq'),
}
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import IsAuthenticated
from .models import UserProfile, UserSettings
from .serializers import UserSerializer

class UserSettingsView(APIView):
    permission_classes = [IsAuthenticated]

    def get(self, request):
        try:
            user_settings, created = UserSettings.objects.get_or_create(
                user=request.user,
                defaults={
                    'language': 'ko',
                    'analyzer_bot': 'claude'
                }
            )
            serializer = UserSerializer(user_settings)
            return Response(serializer.data)
        except Exception as e:
            return Response({
                'language': 'ko', 
                'analyzer_bot': 'claude'
            }, status=status.HTTP_200_OK)

    def post(self, request):
        try:
            user_settings, created = UserSettings.objects.get_or_create(
                user=request.user,
                defaults={
                    'language': request.data.get('language', 'ko'),
                    'analyzer_bot': request.data.get('analyzer_bot', 'claude')
                }
            )

            # Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÎäî Í≤ΩÏö∞ ÏóÖÎç∞Ïù¥Ìä∏
            if not created:
                user_settings.language = request.data.get('language', user_settings.language)
                user_settings.analyzer_bot = request.data.get('analyzer_bot', user_settings.analyzer_bot)
                user_settings.save()

            serializer = UserSettingsSerializer(user_settings)
            return Response(serializer.data, status=status.HTTP_200_OK)
        except Exception as e:
            return Response({
                'error': str(e)
            }, status=status.HTTP_400_BAD_REQUEST)
from rest_framework import status
from rest_framework.decorators import api_view, permission_classes
from rest_framework.permissions import AllowAny
from rest_framework.response import Response
import requests
import logging
from django.contrib.auth import get_user_model
from .models import SocialAccount

import uuid

logger = logging.getLogger(__name__)
User = get_user_model()

# def generate_unique_username(email, name=None):
#     """Í≥†Ïú†Ìïú username ÏÉùÏÑ±"""
#     base = name or email.split('@')[0]
#     username = base
#     suffix = 1
    
#     # usernameÏù¥ Í≥†Ïú†Ìï† ÎïåÍπåÏßÄ Ïà´Ïûê Ï∂îÍ∞Ä
#     while User.objects.filter(username=username).exists():
#         username = f"{base}_{suffix}"
#         suffix += 1
    
#     return username

def generate_unique_username(email, name=None):
    """username ÏÉùÏÑ± - Ïù¥Î©îÏùº ÏïûÎ∂ÄÎ∂Ñ ÎòêÎäî Ïù¥Î¶Ñ ÏÇ¨Ïö©"""
    if name:
        return name  # Ïù¥Î¶ÑÏù¥ ÏûàÏúºÎ©¥ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©
    return email.split('@')[0]  # Ïù¥Î¶ÑÏù¥ ÏóÜÏúºÎ©¥ Ïù¥Î©îÏùº ÏïûÎ∂ÄÎ∂Ñ ÏÇ¨Ïö©
from rest_framework.authentication import TokenAuthentication
from rest_framework.permissions import AllowAny
from rest_framework.authtoken.models import Token
from rest_framework import status
from rest_framework.decorators import api_view, authentication_classes, permission_classes
# views.py
from django.db import transaction, IntegrityError

# @api_view(['GET'])
# @authentication_classes([TokenAuthentication])
# @permission_classes([AllowAny])
# @api_view(['GET'])
# @permission_classes([AllowAny])
# def google_callback(request):
#     logger.info("Starting Google callback process")  # Î°úÍπÖ Ï∂îÍ∞Ä
#     try:
#         with transaction.atomic():
#             # 1. ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
#             auth_header = request.headers.get('Authorization', '')
#             access_token = auth_header.split(' ')[1]
            
#             user_info_response = requests.get(
#                 'https://www.googleapis.com/oauth2/v3/userinfo',
#                 headers={'Authorization': f'Bearer {access_token}'}
#             )
            
#             user_info = user_info_response.json()
#             email = user_info.get('email')
#             name = user_info.get('name')

#             logger.info(f"Processing user: {email}")  # Î°úÍπÖ Ï∂îÍ∞Ä

#             # 2. User Í∞ùÏ≤¥ Í∞ÄÏ†∏Ïò§Í∏∞ ÎòêÎäî ÏÉùÏÑ±
#             user = User.objects.filter(email=email).first()
#             if not user:
#                 user = User.objects.create(
#                     username=email,
#                     email=email,
#                     first_name=name or '',
#                     is_active=True
#                 )
#                 logger.info(f"Created new user: {user.id}")
#             else:
#                 logger.info(f"Found existing user: {user.id}")

#             # 3. Í∏∞Ï°¥ UserSettings ÏÇ≠Ï†ú (ÏûàÎã§Î©¥)
#             UserSettings.objects.filter(user=user).delete()
#             logger.info("Deleted any existing settings")

#             # 4. ÏÉàÎ°úÏö¥ UserSettings ÏÉùÏÑ±
#             settings = UserSettings.objects.create(
#                 user=user,
#                 language='ko',
#                 preferred_model='default'
#             )
#             logger.info(f"Created new settings for user: {user.id}")

#             # 5. ÌÜ†ÌÅ∞ ÏÉùÏÑ±
#             token, _ = Token.objects.get_or_create(user=user)
            
#             return Response({
#                 'user': {
#                     'id': user.id,
#                     'email': user.email,
#                     'username': user.username,
#                     'first_name': user.first_name,
#                     'settings': {
#                         'language': settings.language,
#                         'preferred_model': settings.preferred_model
#                     }
#                 },
#                 'access_token': token.key
#             })
#     except Exception as e:
#         logger.error(f"Error in google_callback: {str(e)}")
#         return Response(
#             {'error': str(e)},
#             status=status.HTTP_400_BAD_REQUEST
        # )
@api_view(['GET'])
@authentication_classes([TokenAuthentication])
@permission_classes([AllowAny])
def google_callback(request):
    try:
        # Ïï°ÏÑ∏Ïä§ ÌÜ†ÌÅ∞ Ï∂îÏ∂ú
        auth_header = request.headers.get('Authorization', '')
        if not auth_header.startswith('Bearer '):
            return Response(
                {'error': 'ÏûòÎ™ªÎêú Ïù∏Ï¶ù Ìó§Îçî'}, 
                status=status.HTTP_401_UNAUTHORIZED
            )
        
        access_token = auth_header.split(' ')[1]

        # Google APIÎ°ú ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥ ÏöîÏ≤≠
        user_info_response = requests.get(
            'https://www.googleapis.com/oauth2/v3/userinfo',
            headers={'Authorization': f'Bearer {access_token}'}
        )

        if user_info_response.status_code != 200:
            return Response(
                {'error': 'GoogleÏóêÏÑú ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Ïò§ÎäîÎç∞ Ïã§Ìå®ÌñàÏäµÎãàÎã§'}, 
                status=status.HTTP_400_BAD_REQUEST
            )

        user_info = user_info_response.json()
        email = user_info.get('email')
        name = user_info.get('name')
        
        if not email:
            return Response(
                {'error': 'Ïù¥Î©îÏùºÏù¥ Ï†úÍ≥µÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§'}, 
                status=status.HTTP_400_BAD_REQUEST
            )

        try:
            # Í∏∞Ï°¥ ÏÇ¨Ïö©Ïûê Í≤ÄÏÉâ
            user = User.objects.get(email=email)
        except User.DoesNotExist:
            # ÏÉàÎ°úÏö¥ ÏÇ¨Ïö©Ïûê ÏÉùÏÑ±
            username = generate_unique_username(email, name)
            user = User.objects.create(
                username=username,
                email=email,
                is_active=True
            )
            
            # Í∏∞Î≥∏ ÎπÑÎ∞ÄÎ≤àÌò∏ ÏÑ§Ï†ï (ÏÑ†ÌÉùÏ†Å)
            random_password = uuid.uuid4().hex
            user.set_password(random_password)
            user.save()

        # ÏÜåÏÖú Í≥ÑÏ†ï Ï†ïÎ≥¥ ÏÉùÏÑ± ÎòêÎäî ÏóÖÎç∞Ïù¥Ìä∏
        social_account, created = SocialAccount.objects.get_or_create(
            email=email,
            provider='google',
            defaults={'user': user}
        )

        if not created and social_account.user != user:
            social_account.user = user
            social_account.save()

        # ÌÜ†ÌÅ∞ ÏÉùÏÑ± ÎòêÎäî Í∞ÄÏ†∏Ïò§Í∏∞
        token, created = Token.objects.get_or_create(user=user)
        logger.info(f"GOOGLE Token created: {created}")
        logger.info(f"Token key: {token.key}")
        logger.info(f"Token user: {token.user.username}")


        # ÏÇ¨Ïö©Ïûê Îç∞Ïù¥ÌÑ∞ Î∞òÌôò
        serializer = UserSerializer(user)
        return Response({
            'user': serializer.data,
            'access_token': token.key,  # Django REST Framework Token Î∞òÌôò
            'token_created': created,
            'google_access_token': access_token,  # Google OAuth Ïï°ÏÑ∏Ïä§ ÌÜ†ÌÅ∞

        })

    except Exception as e:
        logger.error(f"Error in google_callback: {str(e)}")
        return Response(
            {'error': str(e)}, 
            status=status.HTTP_500_INTERNAL_SERVER_ERROR
        )
import requests
import json
import requests
from django.conf import settings
from django.http import JsonResponse
from django.shortcuts import redirect
from .models import User  # User Î™®Îç∏ÏùÑ ÏûÑÌè¨Ìä∏


@api_view(['GET'])
@permission_classes([AllowAny])
def kakao_callback(request):
    try:
        auth_code = request.GET.get('code')
        logger.info(f"Received Kakao auth code: {auth_code}")
        
        # Ïπ¥Ïπ¥Ïò§ ÌÜ†ÌÅ∞ Î∞õÍ∏∞
        token_url = "https://kauth.kakao.com/oauth/token"
        data = {
            "grant_type": "authorization_code",
            "client_id": settings.KAKAO_CLIENT_ID,
            "redirect_uri": settings.KAKAO_REDIRECT_URI,
            "code": auth_code,
        }
        
        token_response = requests.post(token_url, data=data)
        
        if not token_response.ok:
            return Response({
                'error': 'Ïπ¥Ïπ¥Ïò§ ÌÜ†ÌÅ∞ Î∞õÍ∏∞ Ïã§Ìå®',
                'details': token_response.text
            }, status=status.HTTP_400_BAD_REQUEST)
        
        token_data = token_response.json()
        access_token = token_data.get('access_token')
        
        if not access_token:
            return Response({
                'error': 'Ïï°ÏÑ∏Ïä§ ÌÜ†ÌÅ∞ ÏóÜÏùå',
                'details': token_data
            }, status=status.HTTP_400_BAD_REQUEST)

        # Ïπ¥Ïπ¥Ïò§ ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥ Î∞õÍ∏∞
        user_info_url = "https://kapi.kakao.com/v2/user/me"
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-type": "application/x-www-form-urlencoded;charset=utf-8"
        }
        
        user_info_response = requests.get(
            user_info_url,
            headers=headers,
            params={
                'property_keys': json.dumps([
                    "kakao_account.email",
                    "kakao_account.profile",
                    "kakao_account.name"
                ])
            }
        )
        
        if not user_info_response.ok:
            return Response({
                'error': 'ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥ Î∞õÍ∏∞ Ïã§Ìå®',
                'details': user_info_response.text
            }, status=status.HTTP_400_BAD_REQUEST)
        
        user_info = user_info_response.json()
        kakao_account = user_info.get('kakao_account', {})
        email = kakao_account.get('email')
        profile = kakao_account.get('profile', {})
        nickname = profile.get('nickname')
        
        logger.info(f"Kakao user info - email: {email}, nickname: {nickname}")
        
        if not email:
            return Response({
                'error': 'Ïù¥Î©îÏùº Ï†ïÎ≥¥ ÏóÜÏùå',
                'details': 'Ïπ¥Ïπ¥Ïò§ Í≥ÑÏ†ïÏùò Ïù¥Î©îÏùº Ï†ïÎ≥¥Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.'
            }, status=status.HTTP_400_BAD_REQUEST)

        # ÏÇ¨Ïö©Ïûê ÏÉùÏÑ± ÎòêÎäî ÏóÖÎç∞Ïù¥Ìä∏
        try:
            user = User.objects.get(email=email)
            logger.info(f"Updated existing user with nickname: {nickname}")
        except User.DoesNotExist:
            unique_username = generate_unique_username(email, nickname)
            user = User.objects.create(
                email=email,
                username=unique_username,
                is_active=True
            )            
            logger.info(f"Created new user with nickname: {nickname}")

        # ÏÜåÏÖú Í≥ÑÏ†ï ÏÉùÏÑ± ÎòêÎäî ÏóÖÎç∞Ïù¥Ìä∏
        social_account, created = SocialAccount.objects.update_or_create(
            email=email,
            provider='kakao',
            defaults={
                'user': user,
                'nickname': nickname
            }
        )
        logger.info(f"Social account updated - email: {email}, nickname: {nickname}")

        if not created and social_account.user != user:
            social_account.user = user
            social_account.save()

        # ÌÜ†ÌÅ∞ ÏÉùÏÑ± ÎòêÎäî Í∞ÄÏ†∏Ïò§Í∏∞
        token, token_created = Token.objects.get_or_create(user=user)
        logger.info(f"KAKAO Token created: {token_created}")
        logger.info(f"Token key: {token.key}")
        logger.info(f"Token user: {token.user.username}")

        serializer = UserSerializer(user)
        return Response({
            'user': serializer.data,
            'access_token': token.key,  # Django REST Framework Token Î∞òÌôò
            'token_created': created,
            'kakao_access_token': access_token,  # Google OAuth Ïï°ÏÑ∏Ïä§ ÌÜ†ÌÅ∞

        })


        
    except Exception as e:
        logger.exception("Unexpected error in kakao_callback")
        return Response({
            'error': str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


from rest_framework.authtoken.models import Token  # Token Î™®Îç∏ Ï∂îÍ∞Ä

@api_view(['GET'])
@permission_classes([AllowAny])
def naver_callback(request):
    try:
        code = request.GET.get('code')
        state = request.GET.get('state')
        logger.info(f"Received Naver auth code: {code}")

        # ÎÑ§Ïù¥Î≤Ñ ÌÜ†ÌÅ∞ Î∞õÍ∏∞
        token_url = "https://nid.naver.com/oauth2.0/token"
        token_params = {
            "grant_type": "authorization_code",
            "client_id": settings.NAVER_CLIENT_ID,
            "client_secret": settings.NAVER_CLIENT_SECRET,
            "code": code,
            "state": state
        }

        token_response = requests.get(token_url, params=token_params)

        if not token_response.ok:
            return Response({
                'error': 'ÎÑ§Ïù¥Î≤Ñ ÌÜ†ÌÅ∞ Î∞õÍ∏∞ Ïã§Ìå®',
                'details': token_response.text
            }, status=status.HTTP_400_BAD_REQUEST)

        token_data = token_response.json()
        access_token = token_data.get('access_token')

        if not access_token:
            return Response({
                'error': 'Ïï°ÏÑ∏Ïä§ ÌÜ†ÌÅ∞ ÏóÜÏùå',
                'details': token_data
            }, status=status.HTTP_400_BAD_REQUEST)

        # ÎÑ§Ïù¥Î≤Ñ ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥ Î∞õÍ∏∞
        user_info_url = "https://openapi.naver.com/v1/nid/me"
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-type": "application/x-www-form-urlencoded;charset=utf-8"
        }

        user_info_response = requests.get(user_info_url, headers=headers)

        if not user_info_response.ok:
            return Response({
                'error': 'ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥ Î∞õÍ∏∞ Ïã§Ìå®',
                'details': user_info_response.text
            }, status=status.HTTP_400_BAD_REQUEST)

        user_info = user_info_response.json()
        response = user_info.get('response', {})
        email = response.get('email')
        nickname = response.get('nickname')
        username = email.split('@')[0]

        if not email:
            return Response({
                'error': 'Ïù¥Î©îÏùº Ï†ïÎ≥¥ ÏóÜÏùå',
                'details': 'ÎÑ§Ïù¥Î≤Ñ Í≥ÑÏ†ïÏùò Ïù¥Î©îÏùº Ï†ïÎ≥¥Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.'
            }, status=status.HTTP_400_BAD_REQUEST)

        # ÏÇ¨Ïö©Ïûê ÏÉùÏÑ± ÎòêÎäî Ï°∞Ìöå
        user, created = User.objects.get_or_create(
            email=email,
            defaults={'username': generate_unique_username(email, username), 'is_active': True}
        )

        # ÏÜåÏÖú Í≥ÑÏ†ï Ï°∞Ìöå Î∞è ÏóÖÎç∞Ïù¥Ìä∏
        social_account, social_created = SocialAccount.objects.update_or_create(
            provider='naver',
            email=email,
            defaults={'user': user, 'nickname': nickname}
        )

        logger.info(f"Social account updated - email: {email}, nickname: {nickname}")

        # ‚úÖ Django REST Framework Token ÏÉùÏÑ±
        token, token_created = Token.objects.get_or_create(user=user)
        logger.info(f"Naver Token created: {token_created}")
        logger.info(f"Token key: {token.key}")
        logger.info(f"Token user: {token.user.username}")

        serializer = UserSerializer(user)
        return Response({
            'user': serializer.data,
            'access_token': token.key,  # Django REST Framework Token Î∞òÌôò
            'token_created': created,
            'naver_access_token': access_token,  # ÎÑ§Ïù¥Î≤Ñ Ïï°ÏÑ∏Ïä§ ÌÜ†ÌÅ∞
        })

    except Exception as e:
        logger.exception("Unexpected error in naver_callback")
        return Response({
            'error': str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# views.py
import logging
from rest_framework.decorators import api_view, permission_classes
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from django.contrib.auth import get_user_model

# logger = logging.getLogger(__name__)

# @api_view(['PUT'])
# @permission_classes([IsAuthenticated])
# def update_user_settings(request):
#     # Ï∂îÍ∞Ä Î°úÍπÖ Î∞è ÎîîÎ≤ÑÍπÖ
#     logger.info(f"User authentication status: {request.user.is_authenticated}")
#     logger.info(f"User: {request.user}")
#     logger.info(f"Request headers: {request.headers}")
    
#     try:
#         # Ïù∏Ï¶ù ÏÉÅÌÉú Î™ÖÏãúÏ†Å ÌôïÏù∏
#         if not request.user.is_authenticated:
#             logger.error("Unauthenticated user attempt")
#             return Response({
#                 'status': 'error',
#                 'message': 'Ïù∏Ï¶ùÎêòÏßÄ ÏïäÏùÄ ÏÇ¨Ïö©ÏûêÏûÖÎãàÎã§.'
#             }, status=401)
        
#         # UserProfile Î™®Îç∏Ïù¥ ÏûàÎã§Í≥† Í∞ÄÏ†ï
#         user = request.user
#         user_profile = user.userprofile
        
#         # ÏÑ§Ï†ï ÏóÖÎç∞Ïù¥Ìä∏
#         settings_data = request.data
#         user_profile.language = settings_data.get('language', user_profile.language)
#         user_profile.preferred_model = settings_data.get('preferredModel', user_profile.preferred_model)
#         user_profile.save()
        
#         return Response({
#             'status': 'success',
#             'message': 'ÏÑ§Ï†ïÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏóÖÎç∞Ïù¥Ìä∏ÎêòÏóàÏäµÎãàÎã§.',
#             'settings': {
#                 'language': user_profile.language,
#                 'preferredModel': user_profile.preferred_model
#             }
#         })
    
#     except Exception as e:
#         print("Error:", str(e))  # ÏóêÎü¨ Î°úÍπÖ
#         logger.error(f"Settings update error: {str(e)}")
#         return Response({
#             'status': 'error',
#             'message': f'Ïò§Î•ò Î∞úÏÉù: {str(e)}'
#         }, status=400)
# views.py
# Î∞±ÏóîÎìúÏóêÏÑú ÌÜ†ÌÅ∞ ÌòïÏãù ÌôïÏù∏
@api_view(['PUT'])
@authentication_classes([TokenAuthentication])
@permission_classes([IsAuthenticated])
def update_user_settings(request):
    try:
        # ÌÜ†ÌÅ∞ Î°úÍπÖ Ï∂îÍ∞Ä
        token_header = request.headers.get('Authorization')
        if not token_header or not token_header.startswith('Token '):
            return Response({'error': 'ÏûòÎ™ªÎêú ÌÜ†ÌÅ∞ ÌòïÏãù'}, status=status.HTTP_401_UNAUTHORIZED)
        
        user = request.user
        if not user.is_authenticated:
            return Response({'error': 'Ïù∏Ï¶ùÎêòÏßÄ ÏïäÏùÄ ÏÇ¨Ïö©Ïûê'}, status=status.HTTP_401_UNAUTHORIZED)
        
        settings_data = request.data
        print(f"Received settings data: {settings_data}")  # Îç∞Ïù¥ÌÑ∞ Î°úÍπÖ Ï∂îÍ∞Ä
        
        # UserSettings ÏóÖÎç∞Ïù¥Ìä∏ ÎòêÎäî ÏÉùÏÑ±
        settings, created = UserSettings.objects.get_or_create(user=user)
        
        # ÌïÑÎìú ÏóÖÎç∞Ïù¥Ìä∏
        if 'language' in settings_data:
            settings.language = settings_data['language']
        if 'preferredModel' in settings_data:
            settings.preferred_model = settings_data['preferredModel']
        
        settings.save()
        
        return Response({
            'message': 'Settings updated successfully',
            'settings': {
                'language': settings.language,
                'preferredModel': settings.preferred_model
            }
        })
        
    except Exception as e:
        logger.error(f"Settings update error: {str(e)}")
        return Response(
            {'error': str(e)},
            status=status.HTTP_400_BAD_REQUEST
        )
from rest_framework import status
from rest_framework.decorators import api_view, authentication_classes, permission_classes
from rest_framework.authentication import TokenAuthentication
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from django.core.exceptions import ObjectDoesNotExist

@api_view(['PUT'])
@authentication_classes([TokenAuthentication])
@permission_classes([IsAuthenticated])
def update_user_settings(request):
    try:
        user = request.user
        settings_data = request.data
        
        # UserProfile ÌôïÏù∏ Î∞è ÏÉùÏÑ±
        try:
            profile = user.userprofile
        except ObjectDoesNotExist:
            profile = UserProfile.objects.create(user=user)
            
        # UserSettings ÌôïÏù∏ Î∞è ÏÉùÏÑ±/ÏóÖÎç∞Ïù¥Ìä∏
        settings, created = UserSettings.objects.get_or_create(
            user=user,
            defaults={
                'language': settings_data.get('language', 'en'),
                'preferred_model': settings_data.get('preferredModel', 'default')
            }
        )
        
        if not created:
            settings.language = settings_data.get('language', settings.language)
            settings.preferred_model = settings_data.get('preferredModel', settings.preferred_model)
            settings.save()
            
        return Response({
            'message': 'Settings updated successfully',
            'settings': {
                'language': settings.language,
                'preferredModel': settings.preferred_model
            }
        })
            
    except Exception as e:
        print(f"Settings update error: {str(e)}")
        return Response(
            {'error': str(e)},
            status=status.HTTP_400_BAD_REQUEST
        )

import logging
import re
import math
import numpy as np
from collections import Counter
from typing import Dict, List, Union, Any
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
logger = logging.getLogger(__name__)

class SimilarityAnalyzer:
    """
    AI Î™®Îç∏ ÏùëÎãµ Í∞ÑÏùò Ïú†ÏÇ¨ÎèÑÎ•º Î∂ÑÏÑùÌïòÍ≥† ÏùëÎãµ ÌäπÏÑ±ÏùÑ Ï∂îÏ∂úÌïòÎäî ÌÅ¥ÎûòÏä§
    Îã§Íµ≠Ïñ¥ ÏßÄÏõêÏùÑ ÏúÑÌï¥ paraphrase-multilingual-MiniLM-L12-v2 Î™®Îç∏ ÏÇ¨Ïö©
    """
    
    def __init__(self, threshold=0., use_transformer=True):
        """
        Ï¥àÍ∏∞Ìôî
        
        Args:
            threshold (float): Ïú†ÏÇ¨ ÏùëÎãµÏúºÎ°ú Î∂ÑÎ•òÌï† ÏûÑÍ≥ÑÍ∞í (0~1)
            use_transformer (bool): SentenceTransformer Î™®Îç∏ ÏÇ¨Ïö© Ïó¨Î∂Ä
        """
        self.threshold = threshold
        self.use_transformer = use_transformer
        
        # Îã§Íµ≠Ïñ¥ SentenceTransformer Î™®Îç∏ Î°úÎìú
        if use_transformer:
            try:
                self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                logger.info("Îã§Íµ≠Ïñ¥ SentenceTransformer Î™®Îç∏ Î°úÎìú ÏôÑÎ£å")
            except Exception as e:
                logger.error(f"SentenceTransformer Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {str(e)}")
                self.use_transformer = False
                
        # FallbackÏö© TF-IDF Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä
        from sklearn.feature_extraction.text import TfidfVectorizer
        self.vectorizer = TfidfVectorizer(
            min_df=1, 
            analyzer='word',
            ngram_range=(1, 2),
            stop_words=None  # Îã§Íµ≠Ïñ¥ ÏßÄÏõêÏùÑ ÏúÑÌï¥ stop_words Ï†úÍ±∞
        )
    
    def preprocess_text(self, text: str) -> str:
        """
        ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨
        
        Args:
            text (str): Ï†ÑÏ≤òÎ¶¨Ìï† ÌÖçÏä§Ìä∏
            
        Returns:
            str: Ï†ÑÏ≤òÎ¶¨Îêú ÌÖçÏä§Ìä∏
        """
        # ÏÜåÎ¨∏Ïûê Î≥ÄÌôò (ÏòÅÏñ¥ ÌÖçÏä§Ìä∏Îßå Ìï¥Îãπ)
        # Îã§Íµ≠Ïñ¥ ÏßÄÏõêÏùÑ ÏúÑÌï¥ ÏòÅÏñ¥Í∞Ä ÏïÑÎãå Í≤ΩÏö∞ ÏõêÎûò ÏºÄÏù¥Ïä§ Ïú†ÏßÄ
        if text.isascii():
            text = text.lower()
        
        # ÏΩîÎìú Î∏îÎ°ù Ï†úÍ±∞ (Î∂ÑÏÑùÏóêÏÑú Ï†úÏô∏)
        text = re.sub(r'```.*?```', ' CODE_BLOCK ', text, flags=re.DOTALL)
        
        # HTML ÌÉúÍ∑∏ Ï†úÍ±∞
        text = re.sub(r'<.*?>', '', text)
        
        # ÌäπÏàò Î¨∏Ïûê Ï≤òÎ¶¨ (Îã§Íµ≠Ïñ¥ ÏßÄÏõêÏùÑ ÏúÑÌï¥ ÏôÑÏ†Ñ Ï†úÍ±∞ÌïòÏßÄ ÏïäÏùå)
        text = re.sub(r'[^\w\s\u0080-\uFFFF]', ' ', text)
        
        # Ïó¨Îü¨ Í≥µÎ∞±ÏùÑ ÌïòÎÇòÎ°ú ÏπòÌôò
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def calculate_similarity_matrix(self, responses: Dict[str, str]) -> Dict[str, Dict[str, float]]:
        """
        Î™®Îç∏ ÏùëÎãµ Í∞ÑÏùò Ïú†ÏÇ¨ÎèÑ ÌñâÎ†¨ Í≥ÑÏÇ∞
        
        Args:
            responses (dict): Î™®Îç∏ IDÎ•º ÌÇ§Î°ú, ÏùëÎãµ ÌÖçÏä§Ìä∏Î•º Í∞íÏúºÎ°ú ÌïòÎäî ÎîïÏÖîÎÑàÎ¶¨
            
        Returns:
            dict: Î™®Îç∏ Í∞Ñ Ïú†ÏÇ¨ÎèÑ ÌñâÎ†¨
        """
        try:
            model_ids = list(responses.keys())
            
            # ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨
            preprocessed_texts = [self.preprocess_text(responses[model_id]) for model_id in model_ids]
            
            if self.use_transformer and self.model:
                # SentenceTransformerÎ•º ÏÇ¨Ïö©Ìïú ÏûÑÎ≤†Îî© ÏÉùÏÑ±
                try:
                    embeddings = self.model.encode(preprocessed_texts)
                    # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
                    similarity_matrix = cosine_similarity(embeddings)
                except Exception as e:
                    logger.error(f"SentenceTransformer ÏûÑÎ≤†Îî© ÏÉùÏÑ± Ï§ë Ïò§Î•ò: {str(e)}")
                    # Fallback: TF-IDF ÏÇ¨Ïö©
                    tfidf_matrix = self.vectorizer.fit_transform(preprocessed_texts)
                    similarity_matrix = cosine_similarity(tfidf_matrix)
            else:
                # TF-IDF Î≤°ÌÑ∞Ìôî
                tfidf_matrix = self.vectorizer.fit_transform(preprocessed_texts)
                # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
                similarity_matrix = cosine_similarity(tfidf_matrix)
            
            # Í≤∞Í≥ºÎ•º ÎîïÏÖîÎÑàÎ¶¨Î°ú Î≥ÄÌôò
            result = {}
            for i, model1 in enumerate(model_ids):
                result[model1] = {}
                for j, model2 in enumerate(model_ids):
                    result[model1][model2] = float(similarity_matrix[i][j])
            
            return result
            
        except Exception as e:
            logger.error(f"Ïú†ÏÇ¨ÎèÑ ÌñâÎ†¨ Í≥ÑÏÇ∞ Ï§ë Ïò§Î•ò: {str(e)}")
            # Ïò§Î•ò Î∞úÏÉù Ïãú Îπà ÌñâÎ†¨ Î∞òÌôò
            return {model_id: {other_id: 0.0 for other_id in responses} for model_id in responses}
    
      
    def cluster_responses(self, responses):
        """
        ÏùëÎãµÏùÑ Ïú†ÏÇ¨ÎèÑÏóê Îî∞Îùº Íµ∞ÏßëÌôî
        
        Args:
            responses (dict): Î™®Îç∏ IDÎ•º ÌÇ§Î°ú, ÏùëÎãµ ÌÖçÏä§Ìä∏Î•º Í∞íÏúºÎ°ú ÌïòÎäî ÎîïÏÖîÎÑàÎ¶¨
            
        Returns:
            dict: Íµ∞ÏßëÌôî Í≤∞Í≥º
        """
        try:
            model_ids = list(responses.keys())
            if len(model_ids) <= 1:
                return {
                    "similarGroups": [model_ids],
                    "outliers": [],
                    "similarityMatrix": {}
                }
            
            # Ïú†ÏÇ¨ÎèÑ ÌñâÎ†¨ Í≥ÑÏÇ∞
            similarity_matrix = self.calculate_similarity_matrix(responses)
            
            # Í≥ÑÏ∏µÏ†Å ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ ÏàòÌñâ
            clusters = [[model_id] for model_id in model_ids]
            
            merge_happened = True
            while merge_happened and len(clusters) > 1:
                merge_happened = False
                max_similarity = -1
                merge_indices = [-1, -1]
                
                # Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Îëê ÌÅ¥Îü¨Ïä§ÌÑ∞ Ï∞æÍ∏∞
                for i in range(len(clusters)):
                    for j in range(i + 1, len(clusters)):
                        # Îëê ÌÅ¥Îü¨Ïä§ÌÑ∞ Í∞Ñ ÌèâÍ∑† Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
                        cluster_similarity = 0
                        pair_count = 0
                        
                        for model1 in clusters[i]:
                            for model2 in clusters[j]:
                                cluster_similarity += similarity_matrix[model1][model2]
                                pair_count += 1
                        
                        avg_similarity = cluster_similarity / max(1, pair_count)
                        
                        if avg_similarity > max_similarity:
                            max_similarity = avg_similarity
                            merge_indices = [i, j]
                
                # ÏûÑÍ≥ÑÍ∞íÎ≥¥Îã§ Ïú†ÏÇ¨ÎèÑÍ∞Ä ÎÜíÏúºÎ©¥ ÌÅ¥Îü¨Ïä§ÌÑ∞ Î≥ëÌï©
                if max_similarity >= self.threshold:
                    i, j = merge_indices
                    clusters[i].extend(clusters[j])
                    clusters.pop(j)
                    merge_happened = True
            
            # ÌÅ¥Îü¨Ïä§ÌÑ∞ ÌÅ¨Í∏∞Ïóê Îî∞Îùº Ï†ïÎ†¨
            clusters.sort(key=lambda x: -len(x))
            
            # Ï£ºÏöî Í∑∏Î£πÍ≥º Ïù¥ÏÉÅÏπò Íµ¨Î∂Ñ
            main_group = clusters[0] if clusters else []
            outliers = [model for cluster in clusters[1:] for model in cluster]
            
            # ÏùëÎãµ ÌäπÏÑ± Ï∂îÏ∂ú
            response_features = {model_id: self.extract_response_features(responses[model_id]) 
                                for model_id in model_ids}
            
            return {
                "similarGroups": clusters,
                "mainGroup": main_group,
                "outliers": outliers,
                "similarityMatrix": similarity_matrix,
                "responseFeatures": response_features
            }
            
        except Exception as e:
            logger.error(f"ÏùëÎãµ Íµ∞ÏßëÌôî Ï§ë Ïò§Î•ò: {str(e)}")
            # Ïò§Î•ò Î∞úÏÉù Ïãú Î™®Îì† Î™®Îç∏ÏùÑ ÌïòÎÇòÏùò Í∑∏Î£πÏúºÎ°ú Î∞òÌôò
            return {
                "similarGroups": [model_ids],
                "mainGroup": model_ids,
                "outliers": [],
                "similarityMatrix": {},
                "responseFeatures": {}
            }
    
    
    def extract_response_features(self, text: str) -> Dict[str, Union[int, float, bool]]:
        """
        ÏùëÎãµ ÌÖçÏä§Ìä∏ÏóêÏÑú ÌäπÏÑ± Ï∂îÏ∂ú
        
        Args:
            text (str): ÏùëÎãµ ÌÖçÏä§Ìä∏
            
        Returns:
            dict: ÏùëÎãµ ÌäπÏÑ± Ï†ïÎ≥¥
        """
        try:
            # ÏùëÎãµ Í∏∏Ïù¥
            length = len(text)
            
            # ÏΩîÎìú Î∏îÎ°ù Í∞úÏàò
            code_blocks = re.findall(r'```[\s\S]*?```', text)
            code_block_count = len(code_blocks)
            
            # ÎßÅÌÅ¨ Í∞úÏàò
            links = re.findall(r'\[.*?\]\(.*?\)', text) or re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)
            link_count = len(links)
            
            # Î™©Î°ù Ìï≠Î™© Í∞úÏàò
            list_items = re.findall(r'^[\s]*[-*+] |^[\s]*\d+\. ', text, re.MULTILINE)
            list_item_count = len(list_items)
            
            # Î¨∏Ïû• Î∂ÑÎ¶¨ (Îã§Íµ≠Ïñ¥ ÏßÄÏõê)
            sentences = re.split(r'[.!?„ÄÇÔºÅÔºü]+', text)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            # ÌèâÍ∑† Î¨∏Ïû• Í∏∏Ïù¥
            avg_sentence_length = sum(len(s) for s in sentences) / max(1, len(sentences))
            
            # Ïñ¥Ìúò Îã§ÏñëÏÑ± (Í≥†Ïú† Îã®Ïñ¥ ÎπÑÏú®)
            words = re.findall(r'\b\w+\b', text.lower())
            unique_words = set(words)
            vocabulary_diversity = len(unique_words) / max(1, len(words))
            
            # Ïñ∏Ïñ¥ Í∞êÏßÄ (Ï∂îÍ∞Ä Í∏∞Îä•)
            lang_features = self.detect_language_features(text)
            
            features = {
                "length": length,
                "codeBlockCount": code_block_count,
                "linkCount": link_count,
                "listItemCount": list_item_count,
                "sentenceCount": len(sentences),
                "avgSentenceLength": avg_sentence_length,
                "vocabularyDiversity": vocabulary_diversity,
                "hasCode": code_block_count > 0
            }
            
            # Ïñ∏Ïñ¥ ÌäπÏÑ± Ï∂îÍ∞Ä
            features.update(lang_features)
            
            return features
            
        except Exception as e:
            logger.error(f"ÏùëÎãµ ÌäπÏÑ± Ï∂îÏ∂ú Ï§ë Ïò§Î•ò: {str(e)}")
            # Ïò§Î•ò Î∞úÏÉù Ïãú Í∏∞Î≥∏Í∞í Î∞òÌôò
            return {
                "length": len(text),
                "codeBlockCount": 0,
                "linkCount": 0,
                "listItemCount": 0,
                "sentenceCount": 1,
                "avgSentenceLength": len(text),
                "vocabularyDiversity": 0,
                "hasCode": False,
                "detectedLang": "unknown"
            }
    
    def detect_language_features(self, text: str) -> Dict[str, Any]:
        """
        ÌÖçÏä§Ìä∏Ïùò Ïñ∏Ïñ¥ ÌäπÏÑ± Í∞êÏßÄ
        
        Args:
            text (str): Î∂ÑÏÑùÌï† ÌÖçÏä§Ìä∏
            
        Returns:
            dict: Ïñ∏Ïñ¥ ÌäπÏÑ± Ï†ïÎ≥¥
        """
        try:
            # Ïñ∏Ïñ¥ ÌäπÏÑ± Í∞êÏßÄÎ•º ÏúÑÌïú Í∞ÑÎã®Ìïú Ìú¥Î¶¨Ïä§Ìã±
            # Ïã§Ï†ú ÌîÑÎ°úÎçïÏÖòÏóêÏÑúÎäî langdetect Îì±Ïùò ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÇ¨Ïö© Í∂åÏû•
            
            # ÌïúÍµ≠Ïñ¥ ÌäπÏÑ± (ÌïúÍ∏Ä ÎπÑÏú®)
            korean_chars = len(re.findall(r'[„Ñ±-„Öé„Öè-„Ö£Í∞Ä-Ìû£]', text))
            
            # ÏòÅÏñ¥ ÌäπÏÑ± (ÏòÅÎ¨∏ ÎπÑÏú®)
            english_chars = len(re.findall(r'[a-zA-Z]', text))
            
            # ÏùºÎ≥∏Ïñ¥ ÌäπÏÑ± (ÏùºÎ≥∏Ïñ¥ Î¨∏Ïûê ÎπÑÏú®)
            japanese_chars = len(re.findall(r'[„ÅÅ-„Çì„Ç°-„É≥‰∏Ä-ÈæØ]', text))
            
            # Ï§ëÍµ≠Ïñ¥ ÌäπÏÑ± (Ï§ëÍµ≠Ïñ¥ Î¨∏Ïûê ÎπÑÏú®)
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
            
            # Í∏∞ÌÉÄ Î¨∏Ïûê (Ïà´Ïûê, ÌäπÏàòÎ¨∏Ïûê Ï†úÏô∏)
            total_chars = len(re.findall(r'[^\d\s\W]', text))
            
            # ÎπÑÏú® Í≥ÑÏÇ∞
            total = max(1, total_chars)
            korean_ratio = korean_chars / total
            english_ratio = english_chars / total
            japanese_ratio = japanese_chars / total
            chinese_ratio = chinese_chars / total
            
            # Ï£ºÏöî Ïñ∏Ïñ¥ Í≤∞Ï†ï
            lang_ratios = {
                "ko": korean_ratio,
                "en": english_ratio,
                "ja": japanese_ratio,
                "zh": chinese_ratio,
                "other": 1.0 - (korean_ratio + english_ratio + japanese_ratio + chinese_ratio)
            }
            
            detected_lang = max(lang_ratios.items(), key=lambda x: x[1])[0]
            
            return {
                "detectedLang": detected_lang,
                "langRatios": lang_ratios
            }
            
        except Exception as e:
            logger.error(f"Ïñ∏Ïñ¥ ÌäπÏÑ± Í∞êÏßÄ Ï§ë Ïò§Î•ò: {str(e)}")
            return {
                "detectedLang": "unknown",
                "langRatios": {"unknown": 1.0}
            }
    
    def compare_responses(self, response1: str, response2: str) -> Dict[str, Any]:
        """
        Îëê ÏùëÎãµ Í∞ÑÏùò Ïú†ÏÇ¨ÎèÑÏôÄ Ï∞®Ïù¥Ï†ê Î∂ÑÏÑù
        
        Args:
            response1 (str): Ï≤´ Î≤àÏß∏ ÏùëÎãµ
            response2 (str): Îëê Î≤àÏß∏ ÏùëÎãµ
            
        Returns:
            dict: Ïú†ÏÇ¨ÎèÑ Î∞è Ï∞®Ïù¥Ï†ê Î∂ÑÏÑù Í≤∞Í≥º
        """
        try:
            # ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨
            text1 = self.preprocess_text(response1)
            text2 = self.preprocess_text(response2)
            
            # ÏûÑÎ≤†Îî© ÏÉùÏÑ± Î∞è Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
            if self.use_transformer and self.model:
                embeddings = self.model.encode([text1, text2])
                similarity = float(cosine_similarity([embeddings[0]], [embeddings[1]])[0][0])
            else:
                # TF-IDFÎ•º ÏÇ¨Ïö©Ìïú Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
                tfidf_matrix = self.vectorizer.fit_transform([text1, text2])
                similarity = float(cosine_similarity(tfidf_matrix)[0][1])
            
            # ÏùëÎãµ ÌäπÏÑ± ÎπÑÍµê
            features1 = self.extract_response_features(response1)
            features2 = self.extract_response_features(response2)
            
            # ÌäπÏÑ± Ï∞®Ïù¥ Í≥ÑÏÇ∞
            feature_diffs = {}
            for key in set(features1.keys()) & set(features2.keys()):
                if isinstance(features1[key], (int, float)) and isinstance(features2[key], (int, float)):
                    feature_diffs[key] = features2[key] - features1[key]
            
            # Ï£ºÏöî Ï∞®Ïù¥Ï†ê Í≥†Ïú† Îã®Ïñ¥ Î∂ÑÏÑù
            words1 = re.findall(r'\b\w+\b', text1.lower())
            words2 = re.findall(r'\b\w+\b', text2.lower())
            
            counter1 = Counter(words1)
            counter2 = Counter(words2)
            
            unique_to_1 = [word for word, count in counter1.items() if word not in counter2]
            unique_to_2 = [word for word, count in counter2.items() if word not in counter1]
            
            # Í∞ÄÏû• ÎπàÎèÑÍ∞Ä ÎÜíÏùÄ Í≥†Ïú† Îã®Ïñ¥ (ÏµúÎåÄ 10Í∞ú)
            top_unique_to_1 = sorted(
                [(word, counter1[word]) for word in unique_to_1], 
                key=lambda x: x[1], 
                reverse=True
            )[:10]
            
            top_unique_to_2 = sorted(
                [(word, counter2[word]) for word in unique_to_2], 
                key=lambda x: x[1], 
                reverse=True
            )[:10]
            
            return {
                "similarity": similarity,
                "isSimilar": similarity >= self.threshold,
                "features1": features1,
                "features2": features2,
                "featureDiffs": feature_diffs,
                "uniqueWordsTo1": top_unique_to_1,
                "uniqueWordsTo2": top_unique_to_2
            }
            
        except Exception as e:
            logger.error(f"ÏùëÎãµ ÎπÑÍµê Ï§ë Ïò§Î•ò: {str(e)}")
            return {
                "similarity": 0.0,
                "isSimilar": False,
                "error": str(e)
            }
        
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
import logging
import json
import openai
import anthropic
from groq import Groq
from django.conf import settings
import time


logger = logging.getLogger(__name__)

class TextSimplificationView(APIView):
    """
    ÌÖçÏä§Ìä∏Î•º Ïâ¨Ïö¥ ÌëúÌòÑÏúºÎ°ú Î≥ÄÌôòÌïòÎäî API Î∑∞
    ÌäπÏ†ï ÎåÄÏÉÅ(Ïñ¥Î¶∞Ïù¥, Í≥†Î†πÏûê, Ïô∏Íµ≠Ïù∏ ÌïôÏäµÏûê Îì±)Ïóê ÎßûÏ∂∞ Î≥ÄÌôò
    """
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            logger.info("Ïâ¨Ïö¥ ÌëúÌòÑ Î≥ÄÌôò ÏöîÏ≤≠ Î∞õÏùå")
            
            data = request.data
            original_text = data.get('message')
            target_audience = data.get('targetAudience', 'general')
            language = data.get('language', 'ko')
            
            if not original_text:
                return Response({'error': 'Î≥ÄÌôòÌï† ÌÖçÏä§Ìä∏Í∞Ä ÏóÜÏäµÎãàÎã§.'}, 
                               status=status.HTTP_400_BAD_REQUEST)
            
            # ÌÖçÏä§Ìä∏ Îã®ÏàúÌôî ÏàòÌñâ
            simplifier = TextSimplifier(
                api_key=settings.OPENAI_API_KEY,
                model="gpt-4-turbo",  # ÎòêÎäî ÏÑ†Ìò∏ÌïòÎäî GPT Î™®Îç∏
                api_type="openai"
            )
            
            result = simplifier.simplify_text(
                original_text=original_text,
                target_audience=target_audience,
                language=language
            )
            
            return Response(result)
            
        except Exception as e:
            logger.error(f"ÌÖçÏä§Ìä∏ Îã®ÏàúÌôî Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}", exc_info=True)
            return Response({
                'error': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class TextSimplifier:
    """
    ÌÖçÏä§Ìä∏Î•º Ïâ¨Ïö¥ ÌëúÌòÑÏúºÎ°ú Î≥ÄÌôòÌïòÎäî ÌÅ¥ÎûòÏä§
    Îã§ÏñëÌïú AI Î™®Îç∏ÏùÑ ÌôúÏö©ÌïòÏó¨ ÎåÄÏÉÅÏûêÎ≥Ñ ÎßûÏ∂§Ìòï Îã®ÏàúÌôî ÏàòÌñâ
    """
    def __init__(self, api_key, model, api_type):
        self.model = model
        self.api_type = api_type
        self.api_key = api_key
        
        if api_type == 'openai':
            openai.api_key = api_key
        elif api_type == 'anthropic':
            self.client = anthropic.Anthropic(api_key=api_key)
        elif api_type == 'groq':
            self.client = Groq(api_key=api_key)
    
    def simplify_text(self, original_text, target_audience, language='ko'):
        """
        ÌÖçÏä§Ìä∏Î•º Îã®ÏàúÌôîÌïòÏó¨ Î∞òÌôò
        
        Args:
            original_text (str): ÏõêÎ≥∏ ÌÖçÏä§Ìä∏
            target_audience (str): ÎåÄÏÉÅÏûê Ïú†Ìòï (general, child, elderly, foreigner)
            language (str): Ïñ∏Ïñ¥ (Í∏∞Î≥∏Í∞í: ÌïúÍµ≠Ïñ¥)
            
        Returns:
            dict: Îã®ÏàúÌôî Í≤∞Í≥º
        """
        try:
            logger.info(f"ÌÖçÏä§Ìä∏ Îã®ÏàúÌôî ÏãúÏûë: ÎåÄÏÉÅ={target_audience}, Ïñ∏Ïñ¥={language}")
            
            # ÎåÄÏÉÅÏûêÏóê ÎßûÎäî ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±
            prompt = self._get_simplification_prompt(original_text, target_audience, language)
            
            # AI Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÌÖçÏä§Ìä∏ Îã®ÏàúÌôî
            simplified_text = self._generate_simplified_text(prompt)
            
            # Í≤∞Í≥º Î∞òÌôò
            result = {
                'original_text': original_text,
                'simplified_text': simplified_text,
                'target_audience': target_audience,
                'language': language,
                'timestamp': time.time()
            }
            
            logger.info("ÌÖçÏä§Ìä∏ Îã®ÏàúÌôî ÏôÑÎ£å")
            return result
            
        except Exception as e:
            logger.error(f"ÌÖçÏä§Ìä∏ Îã®ÏàúÌôî Ïò§Î•ò: {str(e)}", exc_info=True)
            raise
    
    def _get_simplification_prompt(self, original_text, target_audience, language):
        """ÎåÄÏÉÅÏûê ÎßûÏ∂§Ìòï Îã®ÏàúÌôî ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±"""
        
        base_prompt = f"""
Îã§Ïùå ÌÖçÏä§Ìä∏Î•º Îçî Ïâ¨Ïö¥ ÌëúÌòÑÏúºÎ°ú Î≥ÄÌôòÌï¥Ï£ºÏÑ∏Ïöî:

{original_text}

ÎåÄÏÉÅÏûê: {target_audience}
Ïñ∏Ïñ¥: {language}
"""
        
        if target_audience == 'child':
            base_prompt += """
[Ïñ¥Î¶∞Ïù¥Ïö© Î≥ÄÌôò ÏßÄÏπ®]
1. 7-12ÏÑ∏ Ïñ¥Î¶∞Ïù¥Í∞Ä Ïù¥Ìï¥Ìï† Ïàò ÏûàÎäî Îã®Ïñ¥ÏôÄ ÌëúÌòÑÏúºÎ°ú Î≥ÄÌôòÌïòÏÑ∏Ïöî.
2. ÏßßÍ≥† Í∞ÑÎã®Ìïú Î¨∏Ïû•ÏùÑ ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
3. Ï∂îÏÉÅÏ†ÅÏù∏ Í∞úÎÖêÏùÄ Íµ¨Ï≤¥Ï†ÅÏù∏ ÏòàÏãúÏôÄ Ìï®Íªò ÏÑ§Î™ÖÌïòÏÑ∏Ïöî.
4. Ïû¨ÎØ∏ÏûàÍ≥† Ìù•ÎØ∏Î°úÏö¥ ÌëúÌòÑÏùÑ ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
5. Ïñ¥Î†§Ïö¥ Îã®Ïñ¥Îäî Í∞ÑÎã®Ìïú ÎèôÏùòÏñ¥Î°ú ÎåÄÏ≤¥ÌïòÏÑ∏Ïöî.
6. ÌïÑÏöîÌïú Í≤ΩÏö∞ ÎπÑÏú†ÏôÄ ÏòàÏãúÎ•º ÌôúÏö©ÌïòÏÑ∏Ïöî.
7. Î¨∏Ïû• ÏÇ¨Ïù¥Ïóê Ï†ÅÏ†àÌïú Ï§ÑÎ∞îÍøàÏùÑ Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî.
"""
        elif target_audience == 'elderly':
            base_prompt += """
[Í≥†Î†πÏûêÏö© Î≥ÄÌôò ÏßÄÏπ®]
1. Î™ÖÌôïÌïòÍ≥† ÏßÅÏ†ëÏ†ÅÏù∏ ÌëúÌòÑÏùÑ ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
2. Ïô∏ÎûòÏñ¥ÎÇò ÏòÅÏñ¥ ÌëúÌòÑÏùÄ Í∞ÄÎä•Ìïú ÌïúÍµ≠Ïñ¥Î°ú ÎåÄÏ≤¥ÌïòÏÑ∏Ïöî.
3. Î≥µÏû°Ìïú Î¨∏Ïû• Íµ¨Ï°∞Î•º ÌîºÌïòÍ≥† Í∞ÑÍ≤∞ÌïòÍ≤å ÏûëÏÑ±ÌïòÏÑ∏Ïöî.
4. Ï†ÑÎ¨∏ Ïö©Ïñ¥Îäî ÏùºÏÉÅÏ†ÅÏù∏ Ïö©Ïñ¥Î°ú ÏÑ§Î™ÖÌïòÏÑ∏Ïöî.
5. ÏπúÏàôÌïú ÎπÑÏú†ÏôÄ ÏòàÏãúÎ•º ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
6. Ï§ëÏöîÌïú Ï†ïÎ≥¥Îäî Î∞òÎ≥µÌï¥ÏÑú Í∞ïÏ°∞ÌïòÏÑ∏Ïöî.
7. Î¨∏Ïû• ÏÇ¨Ïù¥Ïóê Ï†ÅÏ†àÌïú Ï§ÑÎ∞îÍøàÏùÑ Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî.
"""
        elif target_audience == 'foreigner':
            base_prompt += """
[Ïô∏Íµ≠Ïù∏ ÌïôÏäµÏûêÏö© Î≥ÄÌôò ÏßÄÏπ®]
1. ÌïúÍµ≠Ïñ¥ ÌïôÏäµÏûê(Ï¥àÍ∏â~Ï§ëÍ∏â)Í∞Ä Ïù¥Ìï¥Ìï† Ïàò ÏûàÎäî Í∏∞Î≥∏ Ïñ¥ÌúòÎ•º ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
2. Í¥ÄÏö©Ïñ¥, ÏÜçÎã¥, ÏùÄÏú†Ï†Å ÌëúÌòÑÏùÑ ÌîºÌïòÏÑ∏Ïöî.
3. ÌïúÏûêÏñ¥Îäî Í∞ÄÎä•Ìïú ÏàúÏö∞Î¶¨ÎßêÎ°ú ÎåÄÏ≤¥ÌïòÏÑ∏Ïöî.
4. Î¨∏Î≤ïÏ†ÅÏúºÎ°ú Îã®ÏàúÌïú Î¨∏Ïû• Íµ¨Ï°∞Î•º ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
5. Î≥µÏû°Ìïú Ïó∞Í≤∞Ïñ¥ÎØ∏ÎÇò Ï°∞ÏÇ¨ ÏÇ¨Ïö©ÏùÑ ÏµúÏÜåÌôîÌïòÏÑ∏Ïöî.
6. Ï§ëÏöîÌïú Í∞úÎÖêÏùÄ Í¥ÑÌò∏ ÏïàÏóê ÏòÅÏñ¥Î°ú Î≥ëÍ∏∞ÌïòÏÑ∏Ïöî.
7. Î¨∏Ïû• ÏÇ¨Ïù¥Ïóê Ï†ÅÏ†àÌïú Ï§ÑÎ∞îÍøàÏùÑ Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî.
"""
        else:  # general
            base_prompt += """
[ÏùºÎ∞òÏù∏Ïö© Î≥ÄÌôò ÏßÄÏπ®]
1. Î≥¥Ìé∏Ï†ÅÏù∏ ÍµêÏñë ÏàòÏ§ÄÏùò Ïñ¥ÌúòÏôÄ ÌëúÌòÑÏùÑ ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
2. Î∂àÌïÑÏöîÌïòÍ≤å Î≥µÏû°Ìïú Î¨∏Ïû• Íµ¨Ï°∞Î•º Îã®ÏàúÌôîÌïòÏÑ∏Ïöî.
3. Ï†ÑÎ¨∏ Ïö©Ïñ¥Îäî Í∞ÑÎã®Ìïú ÏÑ§Î™ÖÍ≥º Ìï®Íªò ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
4. ÎÖºÎ¶¨Ï†Å ÌùêÎ¶ÑÏùÑ Ïú†ÏßÄÌïòÎ©∞ Î™ÖÌôïÌïòÍ≤å ÌëúÌòÑÌïòÏÑ∏Ïöî.
5. ÎπÑÏú†ÏôÄ ÏòàÏãúÎ•º Ï†ÅÏ†àÌûà ÌôúÏö©ÌïòÏÑ∏Ïöî.
6. Ï§ëÏöîÌïú ÎÇ¥Ïö©ÏùÑ Í∞ïÏ°∞ÌïòÍ≥† ÌïµÏã¨ÏùÑ Î®ºÏ†Ä Ï†úÏãúÌïòÏÑ∏Ïöî.
7. Î¨∏Ïû• ÏÇ¨Ïù¥Ïóê Ï†ÅÏ†àÌïú Ï§ÑÎ∞îÍøàÏùÑ Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî.
"""
            
        return base_prompt
    
    def _generate_simplified_text(self, prompt):
        """AI Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Îã®ÏàúÌôîÎêú ÌÖçÏä§Ìä∏ ÏÉùÏÑ±"""
        try:
            # API Ïú†ÌòïÏóê Îî∞Î•∏ Î∂ÑÍ∏∞
            if self.api_type == 'openai':
                response = openai.ChatCompletion.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ÎãπÏã†ÏùÄ Î≥µÏû°Ìïú ÌÖçÏä§Ìä∏Î•º Îçî ÏâΩÍ≤å Ïù¥Ìï¥Ìï† Ïàò ÏûàÎäî ÌòïÌÉúÎ°ú Î≥ÄÌôòÌï¥Ï£ºÎäî Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.5,
                    max_tokens=2000
                )
                simplified_text = response['choices'][0]['message']['content']
                
            elif self.api_type == 'anthropic':
                message = self.client.messages.create(
                    model=self.model,
                    max_tokens=2000,
                    temperature=0.5,
                    system="ÎãπÏã†ÏùÄ Î≥µÏû°Ìïú ÌÖçÏä§Ìä∏Î•º Îçî ÏâΩÍ≤å Ïù¥Ìï¥Ìï† Ïàò ÏûàÎäî ÌòïÌÉúÎ°ú Î≥ÄÌôòÌï¥Ï£ºÎäî Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§.",
                    messages=[{
                        "role": "user", 
                        "content": prompt
                    }]
                )
                simplified_text = message.content[0].text
                
            elif self.api_type == 'groq':
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ÎãπÏã†ÏùÄ Î≥µÏû°Ìïú ÌÖçÏä§Ìä∏Î•º Îçî ÏâΩÍ≤å Ïù¥Ìï¥Ìï† Ïàò ÏûàÎäî ÌòïÌÉúÎ°ú Î≥ÄÌôòÌï¥Ï£ºÎäî Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.5,
                    max_tokens=2000
                )
                simplified_text = response.choices[0].message.content
            
            return simplified_text
            
        except Exception as e:
            logger.error(f"ÌÖçÏä§Ìä∏ ÏÉùÏÑ± Ïò§Î•ò: {str(e)}", exc_info=True)
            raise




    
import logging
import json
import os
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
from PIL import Image, ImageFilter, ImageEnhance
import pytesseract
from .models import OCRResult
from .serializers import OCRResultSerializer

import PyPDF2
import tempfile
from pdf2image import convert_from_path
import re

logger = logging.getLogger(__name__)

# OllamaClientÏôÄ GPTTranslator ÌÅ¥ÎûòÏä§ Í∞ÄÏ†∏Ïò§Í∏∞
from .ollama_client import OllamaClient
from .gpt_translator import GPTTranslator 

@method_decorator(csrf_exempt, name='dispatch')
class ProcessFileView(APIView):
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            logger.info("ProcessFileView ÏöîÏ≤≠ ÏàòÏã†: %s %s", request.method, request.path)
            
            # ÏöîÏ≤≠ Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏
            if 'file' not in request.FILES:
                logger.error("ÌååÏùºÏù¥ Ï†úÍ≥µÎêòÏßÄ ÏïäÏùå")
                return Response({'error': 'ÌååÏùºÏù¥ Ï†úÍ≥µÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§'}, status=status.HTTP_400_BAD_REQUEST)
            
            file_obj = request.FILES['file']
            file_name = file_obj.name.lower()
            logger.info("ÌååÏùº ÏóÖÎ°úÎìú: %s, ÌÅ¨Í∏∞: %s bytes", file_name, file_obj.size)
            
            # Ollama ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî
            ollama_base_url = os.environ.get('OLLAMA_API_URL', 'http://localhost:11434')
            ollama_client = OllamaClient(base_url=ollama_base_url)
            
            # GPT Î≤àÏó≠Í∏∞ Ï¥àÍ∏∞Ìôî
            gpt_translator = GPTTranslator()
            
            # Î≤àÏó≠ ÏòµÏÖò ÌôïÏù∏ (Í∏∞Î≥∏Í∞í: True)
            enable_translation = request.data.get('enable_translation', 'true').lower() == 'true'
            
            # ÌååÏùº Ïú†Ìòï ÌôïÏù∏
            if file_name.endswith(('.pdf')):
                file_type = 'pdf'
                
                # PDF ÌéòÏù¥ÏßÄ Î≤îÏúÑ ÌôïÏù∏
                start_page = int(request.data.get('start_page', 1))
                end_page = int(request.data.get('end_page', 0))  # 0ÏùÄ Ï†ÑÏ≤¥ ÌéòÏù¥ÏßÄÎ•º ÏùòÎØ∏
                
                logger.info("PDF Ï≤òÎ¶¨ Î≤îÏúÑ: %s ~ %s ÌéòÏù¥ÏßÄ", start_page, end_page if end_page > 0 else "ÎÅù")
                
            elif file_name.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif')):
                file_type = 'image'
            else:
                logger.error("ÏßÄÏõêÎêòÏßÄ ÏïäÎäî ÌååÏùº ÌòïÏãù: %s", file_name)
                return Response({'error': 'ÏßÄÏõêÎêòÏßÄ ÏïäÎäî ÌååÏùº ÌòïÏãùÏûÖÎãàÎã§. Ïù¥ÎØ∏ÏßÄÎÇò PDF ÌååÏùºÏùÑ ÏóÖÎ°úÎìúÌï¥Ï£ºÏÑ∏Ïöî.'},
                              status=status.HTTP_400_BAD_REQUEST)
            
            # OCR Í≤∞Í≥º Í∞ùÏ≤¥ ÏÉùÏÑ±
            ocr_result = OCRResult.objects.create(file=file_obj, file_type=file_type)
            logger.info("OCRResult Í∞ùÏ≤¥ ÏÉùÏÑ±: %s", ocr_result.id)
            
            # OCR Ï≤òÎ¶¨
            try:
                ocr_text = ""
                page_texts = []  # ÌéòÏù¥ÏßÄÎ≥Ñ ÌÖçÏä§Ìä∏ Ï†ÄÏû•
                
                if file_type == 'image':
                    # Ïù¥ÎØ∏ÏßÄ ÌååÏùº Ï≤òÎ¶¨ - Í∞úÏÑ†Îêú OCR Ï†ÅÏö©
                    img = Image.open(ocr_result.file.path)
                    # Ïù¥ÎØ∏ÏßÄ Ï†ïÎ≥¥ Î°úÍπÖ
                    logger.info(f"Ïù¥ÎØ∏ÏßÄ Ï†ïÎ≥¥: ÌÅ¨Í∏∞={img.size}, Î™®Îìú={img.mode}, Ìè¨Îß∑={img.format}")
                    
                    # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Î∞è OCR ÏàòÌñâ - OllamaClient Î©îÏÑúÎìú ÏÇ¨Ïö©
                    preprocessed_img = ollama_client.preprocess_image_for_ocr(img)
                    ocr_text = ollama_client.get_optimized_ocr_text(preprocessed_img, lang='kor+eng')
                    page_texts.append({"page": 1, "text": ocr_text})
                    logger.info("Ïù¥ÎØ∏ÏßÄ OCR Ï≤òÎ¶¨ ÏôÑÎ£å, Ï∂îÏ∂ú ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: %s", len(ocr_text))
                    logger.info("Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏ ÏÉòÌîå: %s", ocr_text[:200] if ocr_text else "ÌÖçÏä§Ìä∏ ÏóÜÏùå")
                
                elif file_type == 'pdf':
                    # PDF Ï≤òÎ¶¨ - ÏßÅÏ†ë Ï∂îÏ∂ú ÌõÑ ÌïÑÏöîÏãú OCR
                    logger.info("PDF Ï≤òÎ¶¨ ÏãúÏûë: %s", ocr_result.file.path)
                    
                    # ÏßÅÏ†ë ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú ÏãúÎèÑ (ÌéòÏù¥ÏßÄÎ≥Ñ)
                    direct_extract_success = False
                    try:
                        all_page_texts = self.extract_text_from_pdf_by_pages(ocr_result.file.path)
                        
                        # ÌéòÏù¥ÏßÄ Î≤îÏúÑ Ï≤òÎ¶¨
                        if start_page > 1 or (end_page > 0 and end_page < len(all_page_texts)):
                            if start_page <= len(all_page_texts):
                                if end_page > 0 and end_page >= start_page:
                                    page_texts = all_page_texts[start_page-1:end_page]
                                else:
                                    page_texts = all_page_texts[start_page-1:]
                            else:
                                page_texts = []
                        else:
                            page_texts = all_page_texts
                        
                        combined_text = "\n".join([page["text"] for page in page_texts])
                        
                        # Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏ Í∏∏Ïù¥Í∞Ä Ï∂©Î∂ÑÌïúÏßÄ ÌôïÏù∏ (Îçî ÏóÑÍ≤©Ìïú Ï°∞Í±¥)
                        if combined_text.strip() and len(combined_text.strip()) >= 50:
                            meaningful_chars = sum(1 for c in combined_text if c.isalnum())
                            if meaningful_chars > 30:  # ÏùòÎØ∏ÏûàÎäî Í∏ÄÏûêÍ∞Ä 30Ïûê Ïù¥ÏÉÅÏù¥Î©¥ ÏÑ±Í≥µÏúºÎ°ú Í∞ÑÏ£º
                                ocr_text = combined_text
                                direct_extract_success = True
                                logger.info("PDF ÏßÅÏ†ë ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú ÏÑ±Í≥µ, Ï¥ù %s ÌéòÏù¥ÏßÄ, ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: %s", 
                                          len(page_texts), len(ocr_text))
                                logger.info("Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏ ÏÉòÌîå: %s", ocr_text[:200] if ocr_text else "ÌÖçÏä§Ìä∏ ÏóÜÏùå")
                    except Exception as e:
                        logger.error(f"PDF ÏßÅÏ†ë ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ïã§Ìå®: {str(e)}")
                    
                    # ÏßÅÏ†ë ÌÖçÏä§Ìä∏ Ï∂îÏ∂úÏù¥ Ïã§Ìå®Ìïú Í≤ΩÏö∞, OCR ÏãúÎèÑ
                    if not direct_extract_success:
                        logger.info("PDF OCR Ï≤òÎ¶¨ ÏãúÏûë (ÏßÅÏ†ë Ï∂îÏ∂ú Ïã§Ìå® ÎòêÎäî ÌÖçÏä§Ìä∏ Î∂àÏ∂©Î∂Ñ)")
                        
                        # ÌéòÏù¥ÏßÄ Î≤îÏúÑ ÏÑ§Ï†ïÏúºÎ°ú OCR
                        all_page_texts = self.ocr_pdf_by_pages(ocr_result.file.path, ollama_client, start_page, end_page)
                        
                        # ÌéòÏù¥ÏßÄ Î≤îÏúÑ Ï≤òÎ¶¨ - ocr_pdf_by_pagesÏóêÏÑú Ï≤òÎ¶¨ÌñàÏúºÎØÄÎ°ú Ï†ÑÏ≤¥ ÏÇ¨Ïö©
                        page_texts = all_page_texts
                        
                        ocr_text = "\n".join([page["text"] for page in page_texts])
                        logger.info("PDF OCR Ï≤òÎ¶¨ ÏôÑÎ£å, Ï¥ù %s ÌéòÏù¥ÏßÄ, ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: %s", 
                                  len(page_texts), len(ocr_text))
                        logger.info("Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏ ÏÉòÌîå: %s", ocr_text[:200] if ocr_text else "ÌÖçÏä§Ìä∏ ÏóÜÏùå")
                
                # ÌÖçÏä§Ìä∏ Ï†ïÌôî - Í∞úÏÑ†Îêú Ìï®Ïàò ÏÇ¨Ïö©
                ocr_result.ocr_text = self.clean_text(ocr_text)
                
                # PDF ÌååÏùºÏùÄ Ìï≠ÏÉÅ ÌÖçÏä§Ìä∏ Í¥ÄÎ†® ÏûàÏùåÏúºÎ°ú ÏÑ§Ï†ï
                if file_type == 'pdf':
                    text_relevant = True
                
                # Î∂ÑÏÑù Ïú†Ìòï ÌôïÏù∏ (Í∏∞Î≥∏Í∞í: both)
                analysis_type = request.data.get('analysis_type', 'both')
                logger.info("Î∂ÑÏÑù Ïú†Ìòï: %s", analysis_type)
                
                # Í≤∞Í≥º Î≥ÄÏàò Ï¥àÍ∏∞Ìôî
                image_analysis = ""
                text_analysis = ""
                combined_analysis = ""
                
                # Î≤àÏó≠ Í≤∞Í≥º Î≥ÄÏàò Ï¥àÍ∏∞Ìôî
                translated_analysis = ""
                translation_success = False
                translation_error = ""
                
                # ÌéòÏù¥ÏßÄ Î∂ÑÌï† Î∂ÑÏÑù Ïó¨Î∂Ä ÌôïÏù∏
                analyze_by_page = request.data.get('analyze_by_page', 'true').lower() == 'true'
                
                # ÏÑ†ÌÉùÎêú Î∂ÑÏÑù Ïú†ÌòïÏóê Îî∞Îùº Ï≤òÎ¶¨
                if analysis_type in ['ollama', 'both']:
                    # Ïù¥ÎØ∏ÏßÄ ÌååÏùºÏù∏ Í≤ΩÏö∞
                    if file_type == 'image':
                        # ÏÇ¨Ïö©Ïûê Ï†ïÏùò ÌîÑÎ°¨ÌîÑÌä∏ ÏÑ§Ï†ï (ÏöîÏïΩÎêú Í∞ÑÍ≤∞Ìïú ÏÑ§Î™ÖÏùÑ ÏúÑÌï¥)
                        custom_prompt = f"""Ïù¥ÎØ∏ÏßÄÎ•º Í∞ùÍ¥ÄÏ†ÅÏúºÎ°ú Í¥ÄÏ∞∞ÌïòÍ≥† Îã§Ïùå ÏßÄÏπ®Ïóê Îî∞Îùº ÏùëÎãµÌïòÏÑ∏Ïöî:

ÌïÑÏàò Ìè¨Ìï® ÏÇ¨Ìï≠:
- Ïù¥ÎØ∏ÏßÄÏóê Ïã§Ï†úÎ°ú Î≥¥Ïù¥Îäî ÏÇ¨Îûå, ÎèôÎ¨º, Î¨ºÏ≤¥Îßå Ïñ∏Í∏â (ÏóÜÏúºÎ©¥ Ïñ∏Í∏âÌïòÏßÄ ÏïäÏùå)
- ÎßåÏïΩ ÎèôÎ¨ºÏù¥ÎùºÎ©¥, Ïñ¥Îñ§ Ï¢ÖÏùò ÎèôÎ¨ºÏù∏ÏßÄÎèÑ Ï∂úÎ†•
- ÌôïÏã§Ìûà Î≥¥Ïù¥Îäî ÏÉâÏÉÅÎßå Ïñ∏Í∏â (Î∞∞Í≤ΩÏÉâ, Ïò∑ ÏÉâÏÉÅ Îì±)
- Î™ÖÌôïÌûà Î≥¥Ïù¥Îäî ÏûêÏÑ∏ÎÇò ÏúÑÏπò Í¥ÄÍ≥Ñ (Ï†ïÎ©¥, Ï∏°Î©¥ Îì±)

Ï†àÎåÄ Ìè¨Ìï®ÌïòÏßÄ Îßê Í≤É:
- Ï∂îÏ∏°Ïù¥ÎÇò Ìï¥ÏÑù ("~Î°ú Î≥¥ÏûÖÎãàÎã§", "~Í∞ôÏäµÎãàÎã§" ÌëúÌòÑ Í∏àÏßÄ)
- Î≥¥Ïù¥ÏßÄ ÏïäÎäî Î∂ÄÎ∂ÑÏóê ÎåÄÌïú Ïñ∏Í∏â ("Î≥¥Ïù¥ÏßÄ ÏïäÎäîÎã§", "ÏóÜÎã§" Îì±Ïùò ÌëúÌòÑ Í∏àÏßÄ)
- Î∞òÎ≥µÏ†ÅÏù∏ ÏÑ§Î™Ö
- Í∞êÏ†ïÏù¥ÎÇò Î∂ÑÏúÑÍ∏∞ Î¨òÏÇ¨

ÌòïÏãù:
- 1-2Î¨∏Ïû•ÏúºÎ°ú Îß§Ïö∞ Í∞ÑÍ≤∞ÌïòÍ≤å ÏûëÏÑ±
- Îã®Ïàú ÏÇ¨Ïã§ ÎÇòÏó¥ ÌòïÏãù (Ïòà: "Ïù¥ÎØ∏ÏßÄÏóêÎäî Í≤ÄÏùÄ Î®∏Î¶¨ Ïó¨ÏÑ±Ïù¥ ÏûàÍ≥†, Î∞∞Í≤ΩÏùÄ Ìù∞ÏÉâÏù¥Îã§.")

OCR ÌÖçÏä§Ìä∏ (Ï∞∏Í≥†Ïö©, Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄÏóê Î≥¥Ïù¥Îäî Í≤ΩÏö∞Îßå Ïñ∏Í∏â): {ocr_result.ocr_text}

ÏòÅÏñ¥Î°ú Í∞ÑÍ≤∞ÌïòÍ≤å ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî."""

                        
                        # OCR ÌÖçÏä§Ìä∏Î•º Ï†ÑÎã¨ (analyze_image ÎÇ¥Î∂ÄÏóêÏÑú Í¥ÄÎ†®ÏÑ± ÌåêÎã®)
                        image_analysis = ollama_client.analyze_image(
                            ocr_result.file.path, 
                            custom_prompt,
                            ocr_text=ocr_result.ocr_text
                        )
                        
                        # OCR ÌÖçÏä§Ìä∏ Î∂ÑÏÑù (ÌÖçÏä§Ìä∏Í∞Ä ÏûàÍ≥† both Î™®ÎìúÏù∏ Í≤ΩÏö∞)
                        if ocr_result.ocr_text and analysis_type == 'both':
                            # ÌéòÏù¥ÏßÄÎ≥Ñ ÌÖçÏä§Ìä∏ Ï†ïÎ¶¨Î•º ÏúÑÌïú ÌîÑÎ°¨ÌîÑÌä∏
                            text_prompt = f"""Îã§Ïùå OCRÎ°ú Ï∂îÏ∂úÌïú ÌÖçÏä§Ìä∏Î•º ÏûêÏÑ∏Ìûà Î∂ÑÏÑùÌïòÍ≥† Î™ÖÌôïÌïòÍ≤å Ï†ïÎ¶¨Ìï¥Ï£ºÏÑ∏Ïöî:

{ocr_result.ocr_text}

Î∂ÑÏÑù ÏßÄÏπ®:
1. ÌÖçÏä§Ìä∏Ïùò Ï£ºÏöî ÎÇ¥Ïö©Í≥º Íµ¨Ï°∞Î•º ÌååÏïÖÌïòÏó¨ Ï†ïÎ¶¨
2. Îã®Ïàú ÏöîÏïΩÏù¥ ÏïÑÎãå, ÌÖçÏä§Ìä∏Ïùò ÌïµÏã¨ Ï†ïÎ≥¥Î•º Ï∂©Ïã§ÌïòÍ≤å Ï†ïÎ¶¨
3. Ï§ëÏöîÌïú ÏÑ∏Î∂Ä Ï†ïÎ≥¥Î•º Ìè¨Ìï®
4. ÎÇ¥Ïö©Ïù¥ Ïù¥ÎØ∏ÏßÄÏôÄ Í¥ÄÎ†®Ïù¥ ÏûàÏùÑ Ïàò ÏûàÏúºÎØÄÎ°ú Î¨∏Îß•ÏùÑ Í≥†Î†§ÌïòÏó¨ Ï†ïÎ¶¨

Î∞òÎìúÏãú "ÏòÅÏñ¥(En)"Î°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî."""
                            
                            try:
                                text_analysis = ollama_client.analyze_text(ocr_result.ocr_text, text_prompt)
                            except Exception as e:
                                logger.error(f"ÌÖçÏä§Ìä∏ Î∂ÑÏÑù Ïò§Î•ò: {str(e)}")
                                text_analysis = f"ÌÖçÏä§Ìä∏ Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}"
                            
                            # Îëê Î∂ÑÏÑù Í≤∞Í≥º Í≤∞Ìï©
                            combined_analysis = f"Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Í≤∞Í≥º:\n{image_analysis}\n\nÌÖçÏä§Ìä∏ Î∂ÑÏÑù Í≤∞Í≥º:\n{text_analysis}"
                        else:
                            # OCR ÏóÜÏù¥ Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑùÎßå ÏàòÌñâ
                            combined_analysis = image_analysis
                        
                    else:  # PDF ÌååÏùºÏù∏ Í≤ΩÏö∞
                        if ocr_result.ocr_text:
                            if analyze_by_page and len(page_texts) > 1:
                                # Í∞úÏÑ†Îêú ÌéòÏù¥ÏßÄÎ≥Ñ Î∂ÑÏÑù ÏàòÌñâ - OllamaClientÏùò Î∂ÑÏÑù Í∏∞Îä• ÌôúÏö©
                                try:
                                    combined_analysis = ollama_client.analyze_text(ocr_result.ocr_text, None, page_texts)
                                    logger.info("ÌéòÏù¥ÏßÄÎ≥Ñ Î∂ÑÏÑù ÏôÑÎ£å")
                                except Exception as e:
                                    logger.error(f"ÌéòÏù¥ÏßÄÎ≥Ñ Î∂ÑÏÑù Ïò§Î•ò: {str(e)}")
                                    combined_analysis = f"ÌéòÏù¥ÏßÄÎ≥Ñ Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}"
                            else:
                                # Î¨∏ÏÑú Ï†ÑÏ≤¥ Î∂ÑÏÑù - ÌéòÏù¥ÏßÄÎ≥Ñ Íµ¨Ï°∞Ìôî ÏöîÏ≤≠
                                text_prompt = f"""Îã§Ïùå PDFÏóêÏÑú Ï∂îÏ∂úÌïú ÌÖçÏä§Ìä∏Î•º ÌéòÏù¥ÏßÄÎ≥ÑÎ°ú Î™ÖÌôïÌïòÍ≤å Ï†ïÎ¶¨Ìï¥Ï£ºÏÑ∏Ïöî:

{ocr_result.ocr_text}

Î∂ÑÏÑù ÏßÄÏπ®:
1. ÌÖçÏä§Ìä∏Î•º ÌéòÏù¥ÏßÄÎÇò ÏÑπÏÖò Îã®ÏúÑÎ°ú Íµ¨Î∂ÑÌïòÏó¨ Ï†ïÎ¶¨Ìï¥Ï£ºÏÑ∏Ïöî.
2. ÎÇ¥Ïö©ÏùÑ ÏöîÏïΩÌïòÏßÄ ÎßêÍ≥†, Í∞Å ÏÑπÏÖòÏùò ÌïµÏã¨ Ï†ïÎ≥¥Î•º Ï∂©Ïã§ÌïòÍ≤å Ï†ïÎ¶¨Ìï¥Ï£ºÏÑ∏Ïöî.
3. Î™®Îì† Ï§ëÏöîÌïú ÏÑ∏Î∂Ä Ï†ïÎ≥¥Î•º Ìè¨Ìï®Ìï¥Ï£ºÏÑ∏Ïöî.
4. ÎÇ¥Ïö©ÏùÑ Îã®Ïàú ÏöîÏïΩÌïòÏßÄ ÎßêÍ≥†, Íµ¨Ï°∞ÌôîÎêú ÌòïÏãùÏúºÎ°ú Ï†ïÎ¶¨Ìï¥Ï£ºÏÑ∏Ïöî.

Îã§ÏùåÍ≥º Í∞ôÏùÄ ÌòïÏãùÏúºÎ°ú Ï†ïÎ¶¨Ìï¥Ï£ºÏÑ∏Ïöî:
===== ÌéòÏù¥ÏßÄ 1 (ÎòêÎäî ÏÑπÏÖò 1) =====
- Ï£ºÏöî ÎÇ¥Ïö© Ï†ïÎ¶¨
- Ï§ëÏöî Í∞úÎÖê ÏÑ§Î™Ö
- ÌïµÏã¨ Ï†ïÎ≥¥ ÎÇòÏó¥

===== ÌéòÏù¥ÏßÄ 2 (ÎòêÎäî ÏÑπÏÖò 2) =====
- Ï£ºÏöî ÎÇ¥Ïö© Ï†ïÎ¶¨
...

Î∞òÎìúÏãú "ÏòÅÏñ¥Î°ú" ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî."""
                                
                                try:
                                    text_analysis = ollama_client.analyze_text(ocr_result.ocr_text, text_prompt)
                                    combined_analysis = text_analysis
                                except Exception as e:
                                    logger.error(f"Î¨∏ÏÑú Ï†ÑÏ≤¥ Î∂ÑÏÑù Ïò§Î•ò: {str(e)}")
                                    combined_analysis = f"Î¨∏ÏÑú Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}\n\nOCR Í≤∞Í≥º: {ocr_result.ocr_text[:500]}..."
                    
                    logger.info("Ïù¥ÎØ∏ÏßÄ/ÌÖçÏä§Ìä∏ Î∂ÑÏÑù ÏôÑÎ£å")
                
                # GPT Î≤àÏó≠ ÏàòÌñâ (Î≤àÏó≠Ïù¥ ÌôúÏÑ±ÌôîÎêú Í≤ΩÏö∞)
                if enable_translation and combined_analysis and gpt_translator.is_available:
                    logger.info("GPT Î≤àÏó≠ ÏãúÏûë")
                    try:
                        # Î∂ÑÏÑù Ïú†ÌòïÏóê Îî∞Î•∏ Î≤àÏó≠
                        if file_type == 'pdf' and analyze_by_page and len(page_texts) > 1:
                            # ÌéòÏù¥ÏßÄÎ≥Ñ Î∂ÑÏÑù Í≤∞Í≥º Î≤àÏó≠
                            translation_result = gpt_translator.translate_paged_analysis(combined_analysis)
                        else:
                            # ÏùºÎ∞ò Î∂ÑÏÑù Í≤∞Í≥º Î≤àÏó≠
                            translation_result = gpt_translator.translate_analysis_result(combined_analysis, file_type)
                        
                        if translation_result and translation_result.get("success"):
                            translated_analysis = translation_result["translated_analysis"]
                            translation_success = True
                            logger.info("GPT Î≤àÏó≠ ÏÑ±Í≥µ")
                        else:
                            error_msg = translation_result.get('error', 'Unknown error') if translation_result else 'No translation result'
                            logger.error(f"GPT Î≤àÏó≠ Ïã§Ìå®: {error_msg}")
                            translated_analysis = f"Î≤àÏó≠ Ïã§Ìå®: {error_msg}"
                            translation_error = error_msg
                            
                    except Exception as e:
                        logger.error(f"GPT Î≤àÏó≠ Ï§ë ÏòàÏô∏ Î∞úÏÉù: {str(e)}")
                        translated_analysis = f"Î≤àÏó≠ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"
                        translation_error = str(e)
                
                # Î≤àÏó≠ Í¥ÄÎ†® Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
                ocr_result.translation_enabled = enable_translation
                ocr_result.translation_success = translation_success
                ocr_result.analysis_type = analysis_type
                ocr_result.analyze_by_page = analyze_by_page
                
                # MySQL Ï†ÄÏû•ÏùÑ ÏúÑÌïú ÌÖçÏä§Ìä∏ Ï†ïÌôî
                ocr_result.llm_response = self.clean_text(combined_analysis)
                
                # Î≤àÏó≠ Í≤∞Í≥ºÎèÑ Ï†ÄÏû•
                if enable_translation and translated_analysis:
                    if translation_success:
                        # ÏÑ±Í≥µÌïú Î≤àÏó≠ Í≤∞Í≥º Ï†ÄÏû•
                        ocr_result.llm_response_korean = self.clean_text(translated_analysis)
                        ocr_result.translation_model = gpt_translator.model if gpt_translator else "unknown"
                    else:
                        # Ïã§Ìå®Ìïú Í≤ΩÏö∞ Ïò§Î•ò Î©îÏãúÏßÄ Ï†ÄÏû• (ÎîîÎ≤ÑÍπÖÏö©)
                        ocr_result.llm_response_korean = f"Î≤àÏó≠ Ïã§Ìå®: {translation_error}"
                
                # ÌÖçÏä§Ìä∏ Í¥ÄÎ†®ÏÑ± Ï†ïÎ≥¥ Ï†ÄÏû• - PDFÎäî Ìï≠ÏÉÅ True, Ïù¥ÎØ∏ÏßÄÎäî Î∂ÑÏÑù Í≥ºÏ†ïÏóêÏÑú Í≤∞Ï†ï
                if file_type == 'pdf':
                    ocr_result.text_relevant = True
                
            except Exception as e:
                logger.error("Ï≤òÎ¶¨ Ïã§Ìå®: %s", str(e), exc_info=True)
                return Response({'error': f'Ï≤òÎ¶¨ Ïã§Ìå®: {str(e)}'}, 
                               status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # Í≤∞Í≥º Ï†ÄÏû•
            try:
                ocr_result.save()
                logger.info("OCRResult Ï†ÄÏû• ÏôÑÎ£å (ID: %s)", ocr_result.id)
            except Exception as e:
                logger.error(f"Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï†ÄÏû• Ïã§Ìå®: {str(e)}")
                return Response({'error': f'Í≤∞Í≥º Ï†ÄÏû• Ïã§Ìå®: {str(e)}'}, 
                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # ÏùëÎãµ Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ± - Î™ÖÏãúÏ†ÅÏúºÎ°ú ÌïÑÎìú ÏßÄÏ†ï
            try:
                # Í∏∞Î≥∏ ÏãúÎ¶¨ÏñºÎùºÏù¥Ï†Ä Îç∞Ïù¥ÌÑ∞
                response_data = OCRResultSerializer(ocr_result).data
                
                # Î≤àÏó≠ Í¥ÄÎ†® Ï†ïÎ≥¥ Î™ÖÏãúÏ†Å Ï∂îÍ∞Ä
                response_data['translation_enabled'] = enable_translation
                response_data['translation_success'] = translation_success
                
                # ÏòÅÏñ¥ ÏõêÎ¨∏Í≥º ÌïúÍµ≠Ïñ¥ Î≤àÏó≠ÏùÑ Î™ÖÌôïÌûà Íµ¨Î∂Ñ
                response_data['llm_response'] = ocr_result.llm_response  # ÏòÅÏñ¥ ÏõêÎ¨∏
                
                if enable_translation and translation_success:
                    # Î≤àÏó≠ ÏÑ±Í≥µ Ïãú ÌïúÍµ≠Ïñ¥ Î≤àÏó≠ Ï∂îÍ∞Ä
                    response_data['llm_response_korean'] = ocr_result.llm_response_korean
                    logger.info("ÏùëÎãµÏóê ÌïúÍµ≠Ïñ¥ Î≤àÏó≠ Ìè¨Ìï®")
                elif enable_translation and not translation_success:
                    # Î≤àÏó≠ Ïã§Ìå® Ïãú Ïò§Î•ò Ï†ïÎ≥¥ Ï∂îÍ∞Ä
                    response_data['llm_response_korean'] = None
                    response_data['translation_error'] = translation_error if translation_error else "Î≤àÏó≠ Ïã§Ìå®"
                    logger.info("Î≤àÏó≠ Ïã§Ìå® - ÏòÅÏñ¥ ÏõêÎ¨∏Îßå Ìè¨Ìï®")
                else:
                    # Î≤àÏó≠ ÎπÑÌôúÏÑ±Ìôî Ïãú
                    response_data['llm_response_korean'] = None
                    logger.info("Î≤àÏó≠ ÎπÑÌôúÏÑ±Ìôî - ÏòÅÏñ¥ ÏõêÎ¨∏Îßå Ìè¨Ìï®")
                
                # ÎîîÎ≤ÑÍπÖÏö© Î°úÍ∑∏
                logger.info(f"ÏùëÎãµ Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ± ÏôÑÎ£å:")
                logger.info(f"  - ÏòÅÏñ¥ ÏõêÎ¨∏ Í∏∏Ïù¥: {len(response_data.get('llm_response', ''))}")
                logger.info(f"  - ÌïúÍµ≠Ïñ¥ Î≤àÏó≠ Í∏∏Ïù¥: {len(response_data.get('llm_response_korean', '') or '')}")
                logger.info(f"  - Î≤àÏó≠ ÏÑ±Í≥µ: {response_data.get('translation_success', False)}")
                
            except Exception as e:
                logger.error(f"ÏùëÎãµ Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ± Ïã§Ìå®: {str(e)}")
                return Response({'error': f'ÏùëÎãµ Íµ¨ÏÑ± Ïã§Ìå®: {str(e)}'}, 
                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # ÏùëÎãµ Î∞òÌôò
            return Response(response_data, status=status.HTTP_201_CREATED)
                
        except Exception as e:
            logger.error("Ï≤òÎ¶¨ Ï§ë ÏòàÍ∏∞Ïπò ÏïäÏùÄ Ïò§Î•ò: %s", str(e), exc_info=True)
            return Response({'error': f'ÏÑúÎ≤Ñ Ïò§Î•ò: {str(e)}'}, 
                           status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def extract_text_from_pdf_by_pages(self, pdf_path):
        """PDFÏóêÏÑú ÏßÅÏ†ë ÌÖçÏä§Ìä∏Î•º ÌéòÏù¥ÏßÄÎ≥ÑÎ°ú Ï∂îÏ∂ú"""
        pages = []
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                total_pages = len(reader.pages)
                
                for i in range(total_pages):
                    page = reader.pages[i]
                    text = page.extract_text()
                    pages.append({"page": i + 1, "text": text})
                    
            return pages
        except Exception as e:
            logger.error(f"PDF ÌÖçÏä§Ìä∏ ÏßÅÏ†ë Ï∂îÏ∂ú Ïò§Î•ò: {str(e)}")
            raise
    
    def ocr_pdf_by_pages(self, pdf_path, ollama_client, start_page=1, end_page=0):
        """PDFÎ•º OCRÎ°ú Ï≤òÎ¶¨ÌïòÏó¨ ÌéòÏù¥ÏßÄÎ≥Ñ ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú"""
        pages = []
        
        try:
            # PDF2ImageÎ°ú Ïù¥ÎØ∏ÏßÄ Î≥ÄÌôò
            with tempfile.TemporaryDirectory() as path:
                # ÌéòÏù¥ÏßÄ Î≤àÌò∏Îäî 1Î∂ÄÌÑ∞ ÏãúÏûëÌïòÏßÄÎßå, convert_from_pathÎäî 0Î∂ÄÌÑ∞ ÏãúÏûëÌïòÎØÄÎ°ú Ï°∞Ï†ï
                first_page = start_page
                last_page = None if end_page <= 0 else end_page
                
                images = convert_from_path(
                    pdf_path, 
                    dpi=300, 
                    output_folder=path, 
                    first_page=first_page,
                    last_page=last_page
                )
                
                # Í∞Å ÌéòÏù¥ÏßÄ Ïù¥ÎØ∏ÏßÄ OCR Ï≤òÎ¶¨
                for i, image in enumerate(images):
                    # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨
                    preprocessed_img = ollama_client.preprocess_image_for_ocr(image)
                    # OCR ÏàòÌñâ
                    text = ollama_client.get_optimized_ocr_text(preprocessed_img, lang='kor+eng')
                    
                    # ÌéòÏù¥ÏßÄ Î≤àÌò∏ Í≥ÑÏÇ∞ (ÏãúÏûë ÌéòÏù¥ÏßÄ Í≥†Î†§)
                    page_num = start_page + i
                    pages.append({"page": page_num, "text": text})
                    
            return pages
        except Exception as e:
            logger.error(f"PDF OCR Ï≤òÎ¶¨ Ïò§Î•ò: {str(e)}")
            raise
    
    def clean_text(self, text):
        """ÌÖçÏä§Ìä∏ Ï†ïÌôî Ìï®Ïàò"""
        if not text:
            return ""
            
        # Ïó∞ÏÜçÎêú Í≥µÎ∞± Ï†úÍ±∞
        text = re.sub(r'\s+', ' ', text)
        # Ïó∞ÏÜçÎêú Ï§ÑÎ∞îÍøà Ï†úÍ±∞
        text = re.sub(r'\n\s*\n', '\n\n', text)
        # ÏïûÎí§ Í≥µÎ∞± Ï†úÍ±∞
        text = text.strip()
        
        return text
    
# chat/views.pyÏóê Ï∂îÍ∞ÄÌï† Î∑∞Îì§

from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.decorators import api_view, permission_classes
from rest_framework.permissions import IsAuthenticated, AllowAny
from django.shortcuts import get_object_or_404
from datetime import datetime, timedelta
import json
import re
from .models import Schedule, ScheduleRequest, ConflictResolution
from .serializers import (
    ScheduleSerializer, ScheduleRequestSerializer, 
    ConflictResolutionSerializer, ScheduleRequestInputSerializer
)

# Í∏∞Ï°¥ ChatBot ÌÅ¥ÎûòÏä§ÏôÄ ChatViewÎäî Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ...
# chat/views.py ÏàòÏ†ï Î≤ÑÏ†Ñ

# from rest_framework.views import APIView
# from rest_framework.response import Response
# from rest_framework import status
# from rest_framework.decorators import api_view, permission_classes
# from rest_framework.permissions import IsAuthenticated, AllowAny
# from django.shortcuts import get_object_or_404
# from datetime import datetime, timedelta
# import json
# import re
# from .models import Schedule, ScheduleRequest, ConflictResolution
# from .serializers import (
#     ScheduleSerializer, ScheduleRequestSerializer, 
#     ConflictResolutionSerializer, ScheduleRequestInputSerializer
# )

# # Í∏∞Ï°¥ ChatBot ÌÅ¥ÎûòÏä§Îäî Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ...


# chatbots = {
#     'gpt': ChatBot(OPENAI_API_KEY, 'gpt-3.5-turbo', 'openai'),
#     'claude': ChatBot(ANTHROPIC_API_KEY, 'claude-3-5-haiku-20241022', 'anthropic'), 
#     'mixtral': ChatBot(GROQ_API_KEY, 'llama3-8b-8192', 'groq'),
# }

# # Î∞±ÏóîÎìú views.pyÏóê Ï∂îÍ∞ÄÌï† Ìï®ÏàòÎì§

# def parse_date_from_request(request_text):
#     """ÏûêÏó∞Ïñ¥ ÎÇ†ÏßúÎ•º Ïã§Ï†ú ÎÇ†ÏßúÎ°ú Î≥ÄÌôò"""
#     today = datetime.now().date()
    
#     # Ïò§Îäò/ÎÇ¥Ïùº/Î™®Î†à Îì± ÌïúÍµ≠Ïñ¥ ÎÇ†Ïßú ÌëúÌòÑ Ï≤òÎ¶¨
#     if 'Ïò§Îäò' in request_text:
#         return today
#     elif 'ÎÇ¥Ïùº' in request_text:
#         return today + timedelta(days=1)
#     elif 'Î™®Î†à' in request_text or 'Î™®Îûò' in request_text:
#         return today + timedelta(days=2)
#     elif 'Ïù¥Î≤à Ï£º' in request_text:
#         # Ïù¥Î≤à Ï£º Í∏àÏöîÏùºÎ°ú ÏÑ§Ï†ï
#         days_until_friday = (4 - today.weekday()) % 7
#         if days_until_friday == 0:  # Ïò§ÎäòÏù¥ Í∏àÏöîÏùºÏù¥Î©¥ Îã§Ïùå Ï£º Í∏àÏöîÏùº
#             days_until_friday = 7
#         return today + timedelta(days=days_until_friday)
#     elif 'Îã§Ïùå Ï£º' in request_text:
#         return today + timedelta(days=7)
#     else:
#         # Í∏∞Î≥∏Í∞í: ÎÇ¥Ïùº
#         return today + timedelta(days=1)

# def parse_multiple_schedules_backend(request_text):
#     """Î∞±ÏóîÎìúÏóêÏÑú Ïó¨Îü¨ ÏùºÏ†ï ÌååÏã±"""
#     # ÏâºÌëú, "Í∑∏Î¶¨Í≥†", "Î∞è" Îì±ÏúºÎ°ú Î∂ÑÎ¶¨
#     separators = [',', 'Ôºå', 'Í∑∏Î¶¨Í≥†', 'Î∞è', 'ÏôÄ', 'Í≥º']
    
#     parts = [request_text]
#     for sep in separators:
#         new_parts = []
#         for part in parts:
#             new_parts.extend(part.split(sep))
#         parts = new_parts
    
#     # Ï†ïÎ¶¨Îêú ÏöîÏ≤≠Îì§ Î∞òÌôò
#     cleaned_requests = []
#     for part in parts:
#         cleaned = part.strip()
#         if cleaned and len(cleaned) > 2:  # ÎÑàÎ¨¥ ÏßßÏùÄ ÌÖçÏä§Ìä∏ Ï†úÏô∏
#             cleaned_requests.append(cleaned)
    
#     return cleaned_requests if len(cleaned_requests) > 1 else [request_text]
# class ScheduleOptimizerBot:
#     """ÏùºÏ†ï ÏµúÏ†ÅÌôîÎ•º ÏúÑÌïú AI Î¥á ÌÅ¥ÎûòÏä§ - Ïó¨Îü¨ AI Î™®Îç∏ Ïó∞Îèô"""
    
#     def __init__(self):
#         self.chatbots = {
#                 'gpt': ChatBot(OPENAI_API_KEY, 'gpt-3.5-turbo', 'openai'),
#                 'claude': ChatBot(ANTHROPIC_API_KEY, 'claude-3-5-haiku-20241022', 'anthropic'), 
#                 'mixtral': ChatBot(GROQ_API_KEY, 'llama3-8b-8192', 'groq'),
#             }
        
#     def create_schedule_prompt(self, request_text, user_context=None, existing_schedules=None):
#         """ÏùºÏ†ï ÏÉùÏÑ±ÏùÑ ÏúÑÌïú ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ± - Îπà ÏãúÍ∞Ñ Î∂ÑÏÑù Ìè¨Ìï®"""
#         base_prompt = f"""
#         ÏÇ¨Ïö©ÏûêÏùò ÏùºÏ†ï ÏöîÏ≤≠ÏùÑ Î∂ÑÏÑùÌïòÏó¨ Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º Ï∂©ÎèåÌïòÏßÄ ÏïäÎäî ÏµúÏ†ÅÏùò Îπà ÏãúÍ∞ÑÏùÑ Ï∞æÏïÑ Ï†úÏïàÌï¥Ï£ºÏÑ∏Ïöî.

#         ÏöîÏ≤≠ ÎÇ¥Ïö©: {request_text}
        
#         Í∏∞Ï°¥ ÏùºÏ†ïÎì§: {existing_schedules or []}
        
#         Î∂ÑÏÑù Î∞©Î≤ï:
#         1. Í∏∞Ï°¥ ÏùºÏ†ïÎì§Ïùò ÏãúÍ∞ÑÎåÄÎ•º ÌôïÏù∏ÌïòÏó¨ ÏÇ¨Ïö©ÏûêÍ∞Ä ÏûÖÎ†•Ìïú ÎÇ†Ïùò Îπà ÏãúÍ∞ÑÏùÑ Ï∞æÏïÑÏ£ºÏÑ∏Ïöî
#         2. ÏöîÏ≤≠Îêú ÏùºÏ†ïÏùò ÏÑ±Í≤©Ïóê ÎßûÎäî ÏµúÏ†ÅÏùò ÏãúÍ∞ÑÎåÄÎ•º Ï∂îÏ≤úÌï¥Ï£ºÏÑ∏Ïöî
#         3. ÏùºÏ†ï Í∞Ñ Ïó¨Ïú† ÏãúÍ∞ÑÎèÑ Í≥†Î†§Ìï¥Ï£ºÏÑ∏Ïöî
        
#         Îã§Ïùå ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî:
#         {{
#             "title": "ÏùºÏ†ï Ï†úÎ™©",
#             "description": "ÏÉÅÏÑ∏ ÏÑ§Î™Ö",
#             "suggested_date": "YYYY-MM-DD",
#             "suggested_start_time": "HH:MM",
#             "suggested_end_time": "HH:MM",
#             "location": "Ïû•ÏÜå (ÏÑ†ÌÉùÏÇ¨Ìï≠)",
#             "priority": "HIGH/MEDIUM/LOW/URGENT",
#             "attendees": ["Ï∞∏ÏÑùÏûê1", "Ï∞∏ÏÑùÏûê2"],
#             "reasoning": "Ïù¥ ÏãúÍ∞ÑÏùÑ Ï†úÏïàÌïòÎäî Ïù¥Ïú† (Îπà ÏãúÍ∞Ñ Î∂ÑÏÑù Í≤∞Í≥º Ìè¨Ìï®)"
#         }}
        
#         ÏÇ¨Ïö©ÏûêÏùò Îß•ÎùΩ Ï†ïÎ≥¥: {user_context or "ÏóÜÏùå"}
#         """
#         return base_prompt

#     def create_conflict_resolution_prompt(self, conflicting_schedules, new_request):
#         """ÏùºÏ†ï Ï∂©Îèå Ìï¥Í≤∞ÏùÑ ÏúÑÌïú ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±"""
#         prompt = f"""
#         Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º ÏÉàÎ°úÏö¥ ÏùºÏ†ï ÏöîÏ≤≠ ÏÇ¨Ïù¥Ïóê Ï∂©ÎèåÏù¥ Î∞úÏÉùÌñàÏäµÎãàÎã§. 
#         Ïó¨Îü¨ AIÏùò Í¥ÄÏ†êÏóêÏÑú ÏµúÏ†ÅÏùò Ìï¥Í≤∞ Î∞©ÏïàÏùÑ Ï†úÏïàÌï¥Ï£ºÏÑ∏Ïöî.

#         Í∏∞Ï°¥ Ï∂©Îèå ÏùºÏ†ïÎì§:
#         {json.dumps(conflicting_schedules, ensure_ascii=False, indent=2)}

#         ÏÉàÎ°úÏö¥ ÏùºÏ†ï ÏöîÏ≤≠: {new_request}

#         Îã§Ïùå ÌòïÏãùÏúºÎ°ú Ìï¥Í≤∞ Î∞©ÏïàÏùÑ Ï†úÏïàÌï¥Ï£ºÏÑ∏Ïöî:
#         {{
#             "resolution_options": [
#                 {{
#                     "option": "Î∞©Ïïà 1",
#                     "description": "ÏÉÅÏÑ∏ ÏÑ§Î™Ö",
#                     "impact": "ÏòÅÌñ•ÎèÑ Î∂ÑÏÑù",
#                     "recommended": true/false
#                 }},
#                 {{
#                     "option": "Î∞©Ïïà 2", 
#                     "description": "ÏÉÅÏÑ∏ ÏÑ§Î™Ö",
#                     "impact": "ÏòÅÌñ•ÎèÑ Î∂ÑÏÑù",
#                     "recommended": true/false
#                 }}
#             ],
#             "best_recommendation": "Í∞ÄÏû• Ï∂îÏ≤úÌïòÎäî Î∞©ÏïàÍ≥º Ïù¥Ïú†"
#         }}
#         """
#         return prompt
    
#     def get_ai_suggestions(self, prompt, suggestion_type="schedule"):
#         """Ïó¨Îü¨ AI Î™®Îç∏Î°úÎ∂ÄÌÑ∞ Ï†úÏïàÎ∞õÍ∏∞"""
#         suggestions = {}
        
#         for model_name, chatbot in self.chatbots.items():
#             try:
#                 response = chatbot.chat(prompt)
#                 suggestions[f"{model_name}_suggestion"] = response
#             except Exception as e:
#                 suggestions[f"{model_name}_suggestion"] = f"Ïò§Î•ò Î∞úÏÉù: {str(e)}"
        
#         return suggestions
    
#     def analyze_and_optimize_suggestions(self, suggestions, query, selected_models=['GPT', 'Claude', 'Mixtral']):
#         """Ïó¨Îü¨ AI Ï†úÏïàÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÏµúÏ†ÅÌôîÎêú Í≤∞Í≥º ÏÉùÏÑ± - Í∏∞Ï°¥ analyze_responses ÌôúÏö©"""
#         try:
#             # ChatBotÏùò analyze_responses Í∏∞Îä• ÌôúÏö©
#             analyzer = self.chatbots['claude']  # ClaudeÎ•º Î∂ÑÏÑùÏö©ÏúºÎ°ú ÏÇ¨Ïö©
            
#             # Ï†úÏïàÏùÑ Î∂ÑÏÑùÏö© ÌòïÌÉúÎ°ú Î≥ÄÌôò
#             responses_for_analysis = {}
#             for key, suggestion in suggestions.items():
#                 model_name = key.replace('_suggestion', '')
#                 responses_for_analysis[model_name] = suggestion
            
#             # Í∏∞Ï°¥ analyze_responses Î©îÏÑúÎìú ÌôúÏö©
#             analysis_result = analyzer.analyze_responses(
#                 responses_for_analysis, 
#                 query, 
#                 'Korean',  # Í∏∞Î≥∏ Ïñ∏Ïñ¥
#                 selected_models
#             )
            
#             # JSON ÏùëÎãµÏóêÏÑú ÏµúÏ†ÅÌôîÎêú ÏùºÏ†ï Ï†ïÎ≥¥ Ï∂îÏ∂ú
#             try:
#                 # best_responseÏóêÏÑú JSON Î∂ÄÎ∂Ñ Ï∂îÏ∂ú
#                 json_match = re.search(r'\{.*\}', analysis_result.get('best_response', ''), re.DOTALL)
#                 if json_match:
#                     optimized = json.loads(json_match.group())
#                 else:
#                     # fallback: Ï≤´ Î≤àÏß∏ Ïú†Ìö®Ìïú Ï†úÏïà ÏÇ¨Ïö©
#                     optimized = self._extract_first_valid_suggestion(suggestions)
#             except:
#                 optimized = self._extract_first_valid_suggestion(suggestions)
            
#             confidence = self._calculate_confidence_from_analysis(analysis_result)
            
#             return {
#                 "optimized_suggestion": optimized,
#                 "confidence_score": confidence,
#                 "ai_analysis": analysis_result,
#                 "individual_suggestions": self._parse_individual_suggestions(suggestions)
#             }
            
#         except Exception as e:
#             print(f"Analysis error: {str(e)}")
#             return {"error": f"ÏµúÏ†ÅÌôî Í≥ºÏ†ïÏóêÏÑú Ïò§Î•ò Î∞úÏÉù: {str(e)}"}
    
#     def _extract_first_valid_suggestion(self, suggestions):
#         """Ï≤´ Î≤àÏß∏ Ïú†Ìö®Ìïú Ï†úÏïà Ï∂îÏ∂ú"""
#         for key, suggestion in suggestions.items():
#             try:
#                 json_match = re.search(r'\{.*\}', suggestion, re.DOTALL)
#                 if json_match:
#                     return json.loads(json_match.group())
#             except:
#                 continue
        
#         # Í∏∞Î≥∏ Ï†úÏïà Î∞òÌôò
#         return {
#             "title": "ÏÉà ÏùºÏ†ï",
#             "description": "AIÍ∞Ä Ï†úÏïàÌïú ÏùºÏ†ïÏûÖÎãàÎã§",
#             "suggested_date": datetime.now().strftime('%Y-%m-%d'),
#             "suggested_start_time": "09:00",
#             "suggested_end_time": "10:00",
#             "location": "",
#             "priority": "MEDIUM",
#             "attendees": [],
#             "reasoning": "Ïó¨Îü¨ AI Î™®Îç∏Ïùò Ï†úÏïàÏùÑ Ï¢ÖÌï©Ìïú Í≤∞Í≥ºÏûÖÎãàÎã§."
#         }
    
#     def _calculate_confidence_from_analysis(self, analysis_result):
#         """Î∂ÑÏÑù Í≤∞Í≥ºÏóêÏÑú Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞"""
#         reasoning = analysis_result.get('reasoning', '')
        
#         # ÌÇ§ÏõåÎìú Í∏∞Î∞ò Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞
#         confidence_keywords = ['ÏùºÏπò', 'Í≥µÌÜµ', 'Ï†ïÌôï', 'ÏµúÏ†Å', 'Ï∂îÏ≤ú']
#         uncertainty_keywords = ['Î∂àÌôïÏã§', 'Ï∂îÏ†ï', 'Í∞ÄÎä•ÏÑ±', 'Ïñ¥Î†§ÏõÄ']
        
#         confidence_score = 0.5  # Í∏∞Î≥∏Í∞í
        
#         for keyword in confidence_keywords:
#             if keyword in reasoning:
#                 confidence_score += 0.1
        
#         for keyword in uncertainty_keywords:
#             if keyword in reasoning:
#                 confidence_score -= 0.1
        
#         return max(0.1, min(1.0, confidence_score))
    
#     def _parse_individual_suggestions(self, suggestions):
#         """Í∞úÎ≥Ñ Ï†úÏïàÎì§ÏùÑ ÌååÏã±"""
#         parsed = []
#         for key, suggestion in suggestions.items():
#             try:
#                 json_match = re.search(r'\{.*\}', suggestion, re.DOTALL)
#                 if json_match:
#                     parsed_suggestion = json.loads(json_match.group())
#                     parsed_suggestion['source'] = key.replace('_suggestion', '')
#                     parsed.append(parsed_suggestion)
#             except:
#                 continue
#         return parsed

# class ScheduleManagementView(APIView):
#     """ÏùºÏ†ï Í¥ÄÎ¶¨ Î©îÏù∏ Î∑∞ - Í∂åÌïú ÏàòÏ†ï"""
#     # ÏûÑÏãúÎ°ú AllowAnyÎ°ú Î≥ÄÍ≤Ω (Í∞úÎ∞ú/ÌÖåÏä§Ìä∏Ïö©)
#     permission_classes = [IsAuthenticated]
    
#     def __init__(self):
#         super().__init__()
#         self.optimizer = ScheduleOptimizerBot()
    
#     def get(self, request):
#         """ÏÇ¨Ïö©ÏûêÏùò ÏùºÏ†ï Î™©Î°ù Ï°∞Ìöå"""
#         # üö´ Í∏∞Ï°¥ ÎçîÎØ∏ ÏÇ¨Ïö©Ïûê Î°úÏßÅ Ï†úÍ±∞
#         if not request.user.is_authenticated:
#             return Response({'error': 'Ïù∏Ï¶ùÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.'}, status=status.HTTP_401_UNAUTHORIZED)
        
#         schedules = Schedule.objects.filter(user=request.user).order_by('start_time')
        
#         # ÎÇ†Ïßú ÌïÑÌÑ∞ÎßÅ (Í∏∞Ï°¥ ÏΩîÎìú Ïú†ÏßÄ)
#         start_date = request.query_params.get('start_date')
#         end_date = request.query_params.get('end_date')
        
#         if start_date:
#             schedules = schedules.filter(start_time__date__gte=start_date)
#         if end_date:
#             schedules = schedules.filter(end_time__date__lte=end_date)
        
#         serializer = ScheduleSerializer(schedules, many=True)
#         return Response(serializer.data)
#     def post(self, request):
#         """ÏÉàÎ°úÏö¥ ÏùºÏ†ï ÏÉùÏÑ± ÏöîÏ≤≠ - Ïó¨Îü¨ ÏùºÏ†ï ÏßÄÏõê Í∞úÏÑ†"""
#         try:
#             request_text = request.data.get('request_text', '')
#             existing_schedules = request.data.get('existing_schedules', [])
            
#             if not request_text:
#                 return Response({'error': 'ÏöîÏ≤≠ ÌÖçÏä§Ìä∏Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.'}, 
#                             status=status.HTTP_400_BAD_REQUEST)
#             if not request.user.is_authenticated:
#                 return Response({'error': 'Ïù∏Ï¶ùÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.'}, status=status.HTTP_401_UNAUTHORIZED)
        
#             user = request.user

         
            
#             # Ïó¨Îü¨ ÏùºÏ†ï ÏöîÏ≤≠Ïù∏ÏßÄ ÌôïÏù∏
#             schedule_requests = parse_multiple_schedules_backend(request_text)
#             target_date = parse_date_from_request(request_text)
            
#             if len(schedule_requests) > 1:
#                 # Ïó¨Îü¨ ÏùºÏ†ï Ï≤òÎ¶¨
#                 multiple_schedules = []
#                 all_individual_suggestions = []
                
#                 for i, single_request in enumerate(schedule_requests):
#                     # Í∞Å ÏùºÏ†ïÏùò ÏãúÏûë ÏãúÍ∞ÑÏùÑ Îã§Î•¥Í≤å ÏÑ§Ï†ï
#                     schedule_date = target_date
#                     if i > 0:  # Îëê Î≤àÏß∏ ÏùºÏ†ïÎ∂ÄÌÑ∞Îäî 2ÏãúÍ∞ÑÏî© Îí§Î°ú
#                         base_hour = 9 + (i * 2)
#                     else:
#                         base_hour = 9
                    
#                     # Í∞úÎ≥Ñ ÏùºÏ†ï ÏÉùÏÑ±
#                     optimized_schedule = {
#                         "title": self._extract_schedule_title(single_request),
#                         "description": f"AIÍ∞Ä Î∂ÑÏÑùÌïú {self._extract_schedule_title(single_request)} ÏùºÏ†ïÏûÖÎãàÎã§.",
#                         "suggested_date": schedule_date.strftime('%Y-%m-%d'),
#                         "suggested_start_time": f"{base_hour:02d}:00",
#                         "suggested_end_time": f"{base_hour + 2:02d}:00",
#                         "location": self._extract_schedule_location(single_request),
#                         "priority": "HIGH",
#                         "attendees": [],
#                         "reasoning": f"{i + 1}Î≤àÏß∏ ÏùºÏ†ï: {single_request}. Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º Ï∂©ÎèåÌïòÏßÄ ÏïäÎäî ÏãúÍ∞ÑÏúºÎ°ú Î∞∞Ï†ïÌñàÏäµÎãàÎã§."
#                     }
#                     multiple_schedules.append(optimized_schedule)
                    
#                     # Í∞Å AIÎ≥Ñ Í∞úÎ≥Ñ Ï†úÏïà ÏÉùÏÑ±
#                     for ai_type in ['gpt', 'claude', 'mixtral']:
#                         individual_suggestion = optimized_schedule.copy()
#                         individual_suggestion['source'] = ai_type
#                         individual_suggestion['reasoning'] = f"{ai_type.upper()}Í∞Ä Î∂ÑÏÑùÌïú {self._extract_schedule_title(single_request)} ÏµúÏ†Å ÏãúÍ∞ÑÏûÖÎãàÎã§."
#                         all_individual_suggestions.append(individual_suggestion)
                
#                 # Ïó¨Îü¨ ÏùºÏ†ï ÏùëÎãµ ÏÉùÏÑ±
#                 response_data = {
#                     'request_id': int(datetime.now().timestamp()),
#                     'multiple_schedules': multiple_schedules,
#                     'optimized_suggestion': multiple_schedules[0],
#                     'confidence_score': 0.92,
#                     'individual_suggestions': all_individual_suggestions,
#                     'ai_analysis': {
#                         'analysis_summary': f"Ï¥ù {len(schedule_requests)}Í∞úÏùò ÏùºÏ†ïÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÏµúÏ†ÅÏùò ÏãúÍ∞ÑÎåÄÎ°ú Î∞∞Ï†ïÌñàÏäµÎãàÎã§.",
#                         'reasoning': f"Ïó¨Îü¨ ÏùºÏ†ïÏùÑ {target_date.strftime('%YÎÖÑ %mÏõî %dÏùº')}Ïóê ÏãúÍ∞Ñ ÏàúÏÑúÎåÄÎ°ú Î∞∞ÏπòÌïòÏó¨ Ï∂©ÎèåÏùÑ Î∞©ÏßÄÌñàÏäµÎãàÎã§.",
#                         'models_used': ["gpt", "claude", "mixtral"]
#                     },
#                     'has_conflicts': False,
#                     'conflicts': [],
#                     'analysis_summary': f"{len(schedule_requests)}Í∞ú ÏùºÏ†ïÏóê ÎåÄÌï¥ 3Í∞ú AI Î™®Îç∏Ïù¥ Î∂ÑÏÑùÌïú Í≤∞Í≥ºÏûÖÎãàÎã§.",
#                     'is_multiple_schedule': True
#                 }
                
#             else:
#                 # Îã®Ïùº ÏùºÏ†ï Ï≤òÎ¶¨ (Í∏∞Ï°¥ Î°úÏßÅ ÏÇ¨Ïö©ÌïòÎêò ÎÇ†Ïßú Î∞òÏòÅ)
#                 user_context = self._get_user_context(user)
                
#                 # ÎÇ†ÏßúÍ∞Ä Î∞òÏòÅÎêú ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±
#                 enhanced_prompt = f"""
#                 ÏÇ¨Ïö©ÏûêÏùò ÏùºÏ†ï ÏöîÏ≤≠ÏùÑ Î∂ÑÏÑùÌïòÏó¨ Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º Ï∂©ÎèåÌïòÏßÄ ÏïäÎäî ÏµúÏ†ÅÏùò Îπà ÏãúÍ∞ÑÏùÑ Ï∞æÏïÑ Ï†úÏïàÌï¥Ï£ºÏÑ∏Ïöî.
                
#                 ÏöîÏ≤≠ ÎÇ¥Ïö©: {request_text}
#                 Î™©Ìëú ÎÇ†Ïßú: {target_date.strftime('%YÎÖÑ %mÏõî %dÏùº')} ({self._get_weekday_korean(target_date)})
#                 Í∏∞Ï°¥ ÏùºÏ†ïÎì§: {existing_schedules or []}
                
#                 Îã§Ïùå ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî:
#                 {{
#                     "title": "ÏùºÏ†ï Ï†úÎ™©",
#                     "description": "ÏÉÅÏÑ∏ ÏÑ§Î™Ö",
#                     "suggested_date": "{target_date.strftime('%Y-%m-%d')}",
#                     "suggested_start_time": "HH:MM",
#                     "suggested_end_time": "HH:MM",
#                     "location": "Ïû•ÏÜå",
#                     "priority": "HIGH/MEDIUM/LOW/URGENT",
#                     "attendees": [],
#                     "reasoning": "Ïù¥ ÏãúÍ∞ÑÏùÑ Ï†úÏïàÌïòÎäî Ïù¥Ïú†"
#                 }}
#                 """
                
#                 # Í∏∞Ï°¥ Îã®Ïùº ÏùºÏ†ï Î°úÏßÅ Í≥ÑÏÜç...
#                 suggestions = self.optimizer.get_ai_suggestions(enhanced_prompt)
#                 optimized_result = self.optimizer.analyze_and_optimize_suggestions(
#                     suggestions, f"ÏùºÏ†ï ÏöîÏ≤≠: {request_text}"
#                 )
                
#                 response_data = {
#                     'request_id': int(datetime.now().timestamp()),
#                     'optimized_suggestion': optimized_result.get('optimized_suggestion', {}),
#                     'confidence_score': optimized_result.get('confidence_score', 0.0),
#                     'ai_analysis': optimized_result.get('ai_analysis', {}),
#                     'individual_suggestions': optimized_result.get('individual_suggestions', []),
#                     'has_conflicts': False,
#                     'conflicts': [],
#                     'analysis_summary': "3Í∞ú AI Î™®Îç∏Ïù¥ Î∂ÑÏÑùÌïú Í≤∞Í≥ºÏûÖÎãàÎã§.",
#                     'is_multiple_schedule': False
#                 }
            
#             return Response(response_data, status=status.HTTP_201_CREATED)
            
#         except Exception as e:
#                     return Response({
#                         'error': f'ÏùºÏ†ï ÏÉùÏÑ± ÏöîÏ≤≠ Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {str(e)}'
#                     }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

#     def _extract_schedule_title(self, request):
#             """ÏöîÏ≤≠ÏóêÏÑú ÏùºÏ†ï Ï†úÎ™© Ï∂îÏ∂ú"""
#             if 'Ïö¥Îèô' in request:
#                 return 'Ïö¥Îèô'
#             elif 'ÎØ∏ÌåÖ' in request or 'ÌöåÏùò' in request:
#                 return 'ÌåÄ ÎØ∏ÌåÖ'
#             elif 'Í≥µÎ∂Ä' in request or 'ÌïôÏäµ' in request:
#                 return 'ÌïôÏäµ ÏãúÍ∞Ñ'
#             elif 'ÏûëÏóÖ' in request or 'ÏóÖÎ¨¥' in request:
#                 return 'ÏßëÏ§ë ÏûëÏóÖ'
#             elif 'ÏïΩÏÜç' in request:
#                 return 'ÏïΩÏÜç'
#             else:
#                 return 'ÏÉà ÏùºÏ†ï'

#     def _extract_schedule_location(self, request):
#             """ÏöîÏ≤≠ÏóêÏÑú Ïû•ÏÜå Ï∂îÏ∂ú"""
#             if 'Ïö¥Îèô' in request:
#                 return 'Ìó¨Ïä§Ïû•'
#             elif 'ÎØ∏ÌåÖ' in request or 'ÌöåÏùò' in request:
#                 return 'ÌöåÏùòÏã§'
#             elif 'Í≥µÎ∂Ä' in request or 'ÌïôÏäµ' in request:
#                 return 'ÎèÑÏÑúÍ¥Ä'
#             elif 'Ïª§Ìîº' in request:
#                 return 'Ïπ¥Ìéò'
#             else:
#                 return 'ÏÇ¨Î¨¥Ïã§'

#     def _get_weekday_korean(self, date):
#             """ÏöîÏùºÏùÑ ÌïúÍµ≠Ïñ¥Î°ú Î∞òÌôò"""
#             weekdays = ['ÏõîÏöîÏùº', 'ÌôîÏöîÏùº', 'ÏàòÏöîÏùº', 'Î™©ÏöîÏùº', 'Í∏àÏöîÏùº', 'ÌÜ†ÏöîÏùº', 'ÏùºÏöîÏùº']
#             return weekdays[date.weekday()]
            
   
#     def _check_schedule_conflicts(self, user, suggestion):
#         """ÏùºÏ†ï Ï∂©Îèå Í≤ÄÏÇ¨"""
#         if not suggestion or 'suggested_date' not in suggestion:
#             return []
        
#         try:
#             suggested_date = datetime.strptime(suggestion['suggested_date'], '%Y-%m-%d').date()
#             start_time = datetime.strptime(suggestion.get('suggested_start_time', '09:00'), '%H:%M').time()
#             end_time = datetime.strptime(suggestion.get('suggested_end_time', '10:00'), '%H:%M').time()
            
#             suggested_start = datetime.combine(suggested_date, start_time)
#             suggested_end = datetime.combine(suggested_date, end_time)
            
#             conflicts = Schedule.objects.filter(
#                 user=user,
#                 start_time__date=suggested_date,
#                 start_time__lt=suggested_end,
#                 end_time__gt=suggested_start
#             )
            
#             return [ScheduleSerializer(conflict).data for conflict in conflicts]
            
#         except Exception as e:
#             return []

# # Í∂åÌïú ÏàòÏ†ïÎêú Ìï®ÏàòÎì§
# @api_view(['POST'])
# @permission_classes([IsAuthenticated])  # üîß Í∂åÌïú Î≥ÄÍ≤Ω
# def confirm_schedule(request, request_id):
#     """AI Ï†úÏïàÎêú ÏùºÏ†ïÏùÑ ÌôïÏ†ïÌïòÏó¨ Ïã§Ï†ú ÏùºÏ†ïÏúºÎ°ú ÏÉùÏÑ±"""
#     try:
#         user = request.user
        
#         # üö´ ScheduleRequest.DoesNotExistÏóêÏÑú ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Ï†úÍ±∞
#         try:
#             schedule_request = ScheduleRequest.objects.get(id=request_id, user=user)
#             optimized_suggestion = json.loads(schedule_request.optimized_suggestion)
#         except ScheduleRequest.DoesNotExist:
#             return Response({
#                 'error': f'ÏöîÏ≤≠ ID {request_id}Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
#             }, status=status.HTTP_404_NOT_FOUND)
#                 # ÎÇ†Ïßú/ÏãúÍ∞Ñ ÌååÏã± Í∞úÏÑ†
#         try:
#             suggested_date = optimized_suggestion.get('suggested_date')
#             suggested_start_time = optimized_suggestion.get('suggested_start_time', '09:00')
#             suggested_end_time = optimized_suggestion.get('suggested_end_time', '10:00')
            
#             # ÎÇ†Ïßú ÌòïÏãù ÌôïÏù∏ Î∞è Î≥ÄÌôò
#             if isinstance(suggested_date, str):
#                 if 'T' in suggested_date:  # ISO ÌòïÏãùÏù∏ Í≤ΩÏö∞
#                     suggested_date = suggested_date.split('T')[0]
                
#                 start_datetime = datetime.strptime(
#                     f"{suggested_date} {suggested_start_time}",
#                     '%Y-%m-%d %H:%M'
#                 )
#                 end_datetime = datetime.strptime(
#                     f"{suggested_date} {suggested_end_time}",
#                     '%Y-%m-%d %H:%M'
#                 )
#             else:
#                 # ÎÇ†ÏßúÍ∞Ä ÏóÜÏúºÎ©¥ Ïò§ÎäòÎ°ú ÏÑ§Ï†ï
#                 today = datetime.now().date()
#                 start_datetime = datetime.strptime(
#                     f"{today} {suggested_start_time}",
#                     '%Y-%m-%d %H:%M'
#                 )
#                 end_datetime = datetime.strptime(
#                     f"{today} {suggested_end_time}",
#                     '%Y-%m-%d %H:%M'
#                 )
                
#         except (ValueError, TypeError) as e:
#             print(f"DateTime parsing error: {e}")
#             # Í∏∞Î≥∏Í∞íÏúºÎ°ú Ìè¥Î∞±
#             now = datetime.now()
#             start_datetime = now.replace(hour=9, minute=0, second=0, microsecond=0)
#             end_datetime = now.replace(hour=10, minute=0, second=0, microsecond=0)
        
#         # Schedule Í∞ùÏ≤¥ ÏÉùÏÑ±
#         schedule_data = {
#             'user': user,
#             'title': optimized_suggestion.get('title', 'ÏÉà ÏùºÏ†ï'),
#             'description': optimized_suggestion.get('description', 'AIÍ∞Ä Ï†úÏïàÌïú ÏùºÏ†ïÏûÖÎãàÎã§.'),
#             'start_time': start_datetime,
#             'end_time': end_datetime,
#             'location': optimized_suggestion.get('location', ''),
#             'priority': optimized_suggestion.get('priority', 'MEDIUM'),
#             'attendees': json.dumps(optimized_suggestion.get('attendees', []), ensure_ascii=False)
#         }
        
#         schedule = Schedule.objects.create(**schedule_data)
#         serializer = ScheduleSerializer(schedule)
        
#         print(f"Schedule created successfully: {schedule.id}")
        
#         return Response({
#             'message': 'Ïó¨Îü¨ AIÏùò Î∂ÑÏÑùÏùÑ ÌÜµÌï¥ ÏµúÏ†ÅÌôîÎêú ÏùºÏ†ïÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.',
#             'schedule': serializer.data
#         }, status=status.HTTP_201_CREATED)
        
#     except Exception as e:
#         print(f"Confirm schedule error: {str(e)}")
#         import traceback
#         traceback.print_exc()
        
#         return Response({
#             'error': f'ÏùºÏ†ï ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}'
#         }, status=status.HTTP_400_BAD_REQUEST)


# # Alternative solution: Convert to Class-Based View
# class ConfirmScheduleView(APIView):
#     """AI Ï†úÏïàÎêú ÏùºÏ†ïÏùÑ ÌôïÏ†ïÌïòÏó¨ Ïã§Ï†ú ÏùºÏ†ïÏúºÎ°ú ÏÉùÏÑ±"""
#     permission_classes = [AllowAny]  # ÏûÑÏãúÎ°ú AllowAny
    
#     def post(self, request, request_id):
#         try:
#             # ÏÇ¨Ïö©Ïûê Ï≤òÎ¶¨
#             if not request.user.is_authenticated:
#                 return Response({'error': 'Ïù∏Ï¶ùÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.'}, status=status.HTTP_401_UNAUTHORIZED)

#             user = request.user
            
#             # request_idÎ°ú ScheduleRequestÎ•º Ï∞æÍ±∞ÎÇò ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
#             try:
#                 schedule_request = ScheduleRequest.objects.get(id=request_id, user=user)
#                 optimized_suggestion = json.loads(schedule_request.optimized_suggestion)
#             except ScheduleRequest.DoesNotExist:
#                 # ÎçîÎØ∏ Î™®Îìú: request_idÎ•º Í∏∞Î∞òÏúºÎ°ú Í∏∞Î≥∏ ÏùºÏ†ï ÏÉùÏÑ±
#                 print(f"ScheduleRequest {request_id} not found, creating dummy schedule")
#                 optimized_suggestion = {
#                     'title': 'AI Ï†úÏïà ÏùºÏ†ï',
#                     'description': 'AIÍ∞Ä Ï†úÏïàÌïú ÏµúÏ†ÅÏùò ÏùºÏ†ïÏûÖÎãàÎã§.',
#                     'suggested_date': datetime.now().strftime('%Y-%m-%d'),
#                     'suggested_start_time': '09:00',
#                     'suggested_end_time': '10:00',
#                     'location': 'ÏÇ¨Î¨¥Ïã§',
#                     'priority': 'MEDIUM',
#                     'attendees': []
#                 }
#             except json.JSONDecodeError as e:
#                 print(f"JSON decode error: {e}")
#                 return Response({
#                     'error': f'ÏùºÏ†ï Îç∞Ïù¥ÌÑ∞ ÌååÏã± Ïò§Î•ò: {str(e)}'
#                 }, status=status.HTTP_400_BAD_REQUEST)
            
#             # ÎÇ†Ïßú/ÏãúÍ∞Ñ ÌååÏã±
#             try:
#                 suggested_date = optimized_suggestion.get('suggested_date')
#                 suggested_start_time = optimized_suggestion.get('suggested_start_time', '09:00')
#                 suggested_end_time = optimized_suggestion.get('suggested_end_time', '10:00')
                
#                 # ÎÇ†Ïßú ÌòïÏãù ÌôïÏù∏ Î∞è Î≥ÄÌôò
#                 if isinstance(suggested_date, str):
#                     if 'T' in suggested_date:  # ISO ÌòïÏãùÏù∏ Í≤ΩÏö∞
#                         suggested_date = suggested_date.split('T')[0]
                    
#                     start_datetime = datetime.strptime(
#                         f"{suggested_date} {suggested_start_time}",
#                         '%Y-%m-%d %H:%M'
#                     )
#                     end_datetime = datetime.strptime(
#                         f"{suggested_date} {suggested_end_time}",
#                         '%Y-%m-%d %H:%M'
#                     )
#                 else:
#                     # ÎÇ†ÏßúÍ∞Ä ÏóÜÏúºÎ©¥ Ïò§ÎäòÎ°ú ÏÑ§Ï†ï
#                     today = datetime.now().date()
#                     start_datetime = datetime.strptime(
#                         f"{today} {suggested_start_time}",
#                         '%Y-%m-%d %H:%M'
#                     )
#                     end_datetime = datetime.strptime(
#                         f"{today} {suggested_end_time}",
#                         '%Y-%m-%d %H:%M'
#                     )
                    
#             except (ValueError, TypeError) as e:
#                 print(f"DateTime parsing error: {e}")
#                 # Í∏∞Î≥∏Í∞íÏúºÎ°ú Ìè¥Î∞±
#                 now = datetime.now()
#                 start_datetime = now.replace(hour=9, minute=0, second=0, microsecond=0)
#                 end_datetime = now.replace(hour=10, minute=0, second=0, microsecond=0)
            
#             # Schedule Í∞ùÏ≤¥ ÏÉùÏÑ±
#             schedule_data = {
#                 'user': user,
#                 'title': optimized_suggestion.get('title', 'ÏÉà ÏùºÏ†ï'),
#                 'description': optimized_suggestion.get('description', 'AIÍ∞Ä Ï†úÏïàÌïú ÏùºÏ†ïÏûÖÎãàÎã§.'),
#                 'start_time': start_datetime,
#                 'end_time': end_datetime,
#                 'location': optimized_suggestion.get('location', ''),
#                 'priority': optimized_suggestion.get('priority', 'MEDIUM'),
#                 'attendees': json.dumps(optimized_suggestion.get('attendees', []), ensure_ascii=False)
#             }
            
#             schedule = Schedule.objects.create(**schedule_data)
#             serializer = ScheduleSerializer(schedule)
            
#             print(f"Schedule created successfully: {schedule.id}")
            
#             return Response({
#                 'message': 'Ïó¨Îü¨ AIÏùò Î∂ÑÏÑùÏùÑ ÌÜµÌï¥ ÏµúÏ†ÅÌôîÎêú ÏùºÏ†ïÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.',
#                 'schedule': serializer.data
#             }, status=status.HTTP_201_CREATED)
            
#         except Exception as e:
#             print(f"Confirm schedule error: {str(e)}")
#             import traceback
#             traceback.print_exc()
            
#             return Response({
#                 'error': f'ÏùºÏ†ï ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}'
#             }, status=status.HTTP_400_BAD_REQUEST)


# # Fix for resolve_schedule_conflict function
# @api_view(['POST'])
# @permission_classes([IsAuthenticated])  # üîß Í∂åÌïú Î≥ÄÍ≤Ω
# def resolve_schedule_conflict(request):
#     """ÏùºÏ†ï Ï∂©Îèå Ìï¥Í≤∞ Î∞©Ïïà Ï†úÍ≥µ"""
#     # üö´ ÎçîÎØ∏ ÏÇ¨Ïö©Ïûê Î°úÏßÅ Ï†úÍ±∞
#     user = request.user
    
#     conflicting_schedule_ids = request.data.get('conflicting_schedule_ids', [])
#     new_request = request.data.get('new_request', '')
    
#     # ÎÇòÎ®∏ÏßÄ Î°úÏßÅÏùÄ Í∑∏ÎåÄÎ°ú...
    
#     if not conflicting_schedule_ids or not new_request:
#         return Response({
#             'error': 'Ï∂©Îèå ÏùºÏ†ï IDÏôÄ ÏÉàÎ°úÏö¥ ÏöîÏ≤≠Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§.'
#         }, status=status.HTTP_400_BAD_REQUEST)
    
#     try:
#         # ÏÇ¨Ïö©Ïûê Ï≤òÎ¶¨
#         if request.user.is_authenticated:
#             user = request.user
#         else:
#             from django.contrib.auth.models import User
#             user, created = User.objects.get_or_create(
#                 username='dummy_user',
#                 defaults={'email': 'dummy@example.com'}
#             )
        
#         # Ï∂©Îèå ÏùºÏ†ïÎì§ Ï°∞Ìöå
#         conflicting_schedules = Schedule.objects.filter(
#             id__in=conflicting_schedule_ids,
#             user=user
#         )
        
#         conflicting_data = [ScheduleSerializer(schedule).data for schedule in conflicting_schedules]
        
#         # Îã§Ï§ë AI Î™®Îç∏Îì§Î°úÎ∂ÄÌÑ∞ Ìï¥Í≤∞ Î∞©Ïïà Î∞õÍ∏∞
#         optimizer = ScheduleOptimizerBot()
#         prompt = optimizer.create_conflict_resolution_prompt(conflicting_data, new_request)
#         suggestions = optimizer.get_ai_suggestions(prompt, "conflict_resolution")
        
#         # AI Î∂ÑÏÑùÏùÑ ÌÜµÌïú ÏµúÏ†Å Ìï¥Í≤∞Î∞©Ïïà ÎèÑÏ∂ú
#         analysis_result = optimizer.analyze_and_optimize_suggestions(
#             suggestions,
#             f"Ï∂©Îèå Ìï¥Í≤∞: {new_request}"
#         )
        
#         # Ìï¥Í≤∞ Î∞©Ïïà Ï†ÄÏû•
#         conflict_resolution = ConflictResolution.objects.create(
#             user=user,
#             conflicting_schedules=json.dumps(conflicting_data, ensure_ascii=False),
#             resolution_options=json.dumps(suggestions, ensure_ascii=False),
#             ai_recommendations=json.dumps(analysis_result, ensure_ascii=False)
#         )
        
#         return Response({
#             'resolution_id': conflict_resolution.id,
#             'conflicting_schedules': conflicting_data,
#             'ai_suggestions': suggestions,
#             'optimized_resolution': analysis_result,
#             'message': f'{len(suggestions)}Í∞ú AI Î™®Îç∏Ïù¥ Î∂ÑÏÑùÌïú Ï∂©Îèå Ìï¥Í≤∞ Î∞©ÏïàÏù¥ ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.'
#         }, status=status.HTTP_201_CREATED)
        
#     except Exception as e:
#         return Response({
#             'error': f'Ï∂©Îèå Ìï¥Í≤∞ Î∞©Ïïà ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}'
#         }, status=status.HTTP_400_BAD_REQUEST)


# # Alternative Class-Based View for conflict resolution
# class ResolveScheduleConflictView(APIView):
#     """ÏùºÏ†ï Ï∂©Îèå Ìï¥Í≤∞ Î∞©Ïïà Ï†úÍ≥µ - Îã§Ï§ë AI Î∂ÑÏÑù"""
#     permission_classes = [IsAuthenticated]
    
#     def post(self, request):
#         conflicting_schedule_ids = request.data.get('conflicting_schedule_ids', [])
#         new_request = request.data.get('new_request', '')
        
#         if not conflicting_schedule_ids or not new_request:
#             return Response({
#                 'error': 'Ï∂©Îèå ÏùºÏ†ï IDÏôÄ ÏÉàÎ°úÏö¥ ÏöîÏ≤≠Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§.'
#             }, status=status.HTTP_400_BAD_REQUEST)
        
#         try:
#             # ÏÇ¨Ïö©Ïûê Ï≤òÎ¶¨
#             if request.user.is_authenticated:
#                 user = request.user
#             else:
#                 from django.contrib.auth.models import User
#                 user, created = User.objects.get_or_create(
#                     username='dummy_user',
#                     defaults={'email': 'dummy@example.com'}
#                 )
            
#             # Ï∂©Îèå ÏùºÏ†ïÎì§ Ï°∞Ìöå
#             conflicting_schedules = Schedule.objects.filter(
#                 id__in=conflicting_schedule_ids,
#                 user=user
#             )
            
#             conflicting_data = [ScheduleSerializer(schedule).data for schedule in conflicting_schedules]
            
#             # Îã§Ï§ë AI Î™®Îç∏Îì§Î°úÎ∂ÄÌÑ∞ Ìï¥Í≤∞ Î∞©Ïïà Î∞õÍ∏∞
#             optimizer = ScheduleOptimizerBot()
#             prompt = optimizer.create_conflict_resolution_prompt(conflicting_data, new_request)
#             suggestions = optimizer.get_ai_suggestions(prompt, "conflict_resolution")
            
#             # AI Î∂ÑÏÑùÏùÑ ÌÜµÌïú ÏµúÏ†Å Ìï¥Í≤∞Î∞©Ïïà ÎèÑÏ∂ú
#             analysis_result = optimizer.analyze_and_optimize_suggestions(
#                 suggestions,
#                 f"Ï∂©Îèå Ìï¥Í≤∞: {new_request}"
#             )
            
#             # Ìï¥Í≤∞ Î∞©Ïïà Ï†ÄÏû•
#             conflict_resolution = ConflictResolution.objects.create(
#                 user=user,
#                 conflicting_schedules=json.dumps(conflicting_data, ensure_ascii=False),
#                 resolution_options=json.dumps(suggestions, ensure_ascii=False),
#                 ai_recommendations=json.dumps(analysis_result, ensure_ascii=False)
#             )
            
#             return Response({
#                 'resolution_id': conflict_resolution.id,
#                 'conflicting_schedules': conflicting_data,
#                 'ai_suggestions': suggestions,
#                 'optimized_resolution': analysis_result,
#                 'message': f'{len(suggestions)}Í∞ú AI Î™®Îç∏Ïù¥ Î∂ÑÏÑùÌïú Ï∂©Îèå Ìï¥Í≤∞ Î∞©ÏïàÏù¥ ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.'
#             }, status=status.HTTP_201_CREATED)
            
#         except Exception as e:
#             return Response({
#                 'error': f'Ï∂©Îèå Ìï¥Í≤∞ Î∞©Ïïà ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}'
#             }, status=status.HTTP_400_BAD_REQUEST)

from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.decorators import api_view, permission_classes, authentication_classes
from rest_framework.permissions import IsAuthenticated, AllowAny
from rest_framework.authentication import TokenAuthentication, SessionAuthentication
from django.shortcuts import get_object_or_404
from datetime import datetime, timedelta
import json
import re
from .models import Schedule, ScheduleRequest, ConflictResolution
from .serializers import (
    ScheduleSerializer, ScheduleRequestSerializer, 
    ConflictResolutionSerializer, ScheduleRequestInputSerializer
)
import logging
from pytz import timezone
KST = timezone('Asia/Seoul')
target_datetime = datetime.now(KST)
logger = logging.getLogger(__name__)

# Í∏∞Ï°¥ ChatBot ÌÅ¥ÎûòÏä§ÏôÄ API ÌÇ§Îì§ÏùÄ Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ...
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# üîß ÌÜ†ÌÅ∞ ÎîîÎ≤ÑÍπÖÏùÑ ÏúÑÌïú Ïª§Ïä§ÌÖÄ Ïù∏Ï¶ù ÌÅ¥ÎûòÏä§
class DebugTokenAuthentication(TokenAuthentication):
    """ÎîîÎ≤ÑÍπÖÏù¥ Ìè¨Ìï®Îêú ÌÜ†ÌÅ∞ Ïù∏Ï¶ù ÌÅ¥ÎûòÏä§"""
    
    def authenticate(self, request):
        logger.info("=== Í∞úÏÑ†Îêú ÌÜ†ÌÅ∞ Ïù∏Ï¶ù ÎîîÎ≤ÑÍπÖ ÏãúÏûë ===")
        
        # Authorization Ìó§Îçî ÌôïÏù∏
        auth_header = request.META.get('HTTP_AUTHORIZATION', '')
        logger.info(f"Authorization Ìó§Îçî: '{auth_header}'")
        
        if not auth_header:
            logger.warning("‚ùå Authorization Ìó§ÎçîÍ∞Ä ÏóÜÏäµÎãàÎã§")
            return None
            
        if not auth_header.startswith('Bearer '):
            logger.warning(f"‚ùå Bearer ÌÜ†ÌÅ∞ ÌòïÏãùÏù¥ ÏïÑÎãôÎãàÎã§: {auth_header}")
            return None
            
        token = auth_header.split(' ')[1]
        logger.info(f"üì± Ï∂îÏ∂úÎêú ÌÜ†ÌÅ∞: {token[:10]}...{token[-10:]}")
        
        # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú ÌÜ†ÌÅ∞ ÌôïÏù∏
        try:
            token_obj = Token.objects.select_related('user').get(key=token)
            logger.info(f"‚úÖ DBÏóêÏÑú ÌÜ†ÌÅ∞ Î∞úÍ≤¨: {token_obj.key[:10]}...{token_obj.key[-10:]}")
            logger.info(f"üë§ ÌÜ†ÌÅ∞ ÏÜåÏú†Ïûê: {token_obj.user.username} (ID: {token_obj.user.id})")
            logger.info(f"üîÑ ÏÇ¨Ïö©Ïûê ÌôúÏÑ± ÏÉÅÌÉú: {token_obj.user.is_active}")
            
            if not token_obj.user.is_active:
                logger.warning(f"‚ùå ÏÇ¨Ïö©ÏûêÍ∞Ä ÎπÑÌôúÏÑ±ÌôîÎê®: {token_obj.user.username}")
                raise exceptions.AuthenticationFailed('User inactive or deleted.')
            
            logger.info("‚úÖ ÌÜ†ÌÅ∞ Ïù∏Ï¶ù ÏÑ±Í≥µ!")
            logger.info("=== Í∞úÏÑ†Îêú ÌÜ†ÌÅ∞ Ïù∏Ï¶ù ÎîîÎ≤ÑÍπÖ Ï¢ÖÎ£å ===")
            return (token_obj.user, token_obj)
            
        except Token.DoesNotExist:
            logger.error(f"‚ùå DBÏóê Ìï¥Îãπ ÌÜ†ÌÅ∞Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå: {token[:10]}...{token[-10:]}")
            
            # Î™®Îì† ÌÜ†ÌÅ∞ Î™©Î°ù Ï∂úÎ†• (ÎîîÎ≤ÑÍπÖÏö©)
            all_tokens = Token.objects.all()[:5]  # Ï≤òÏùå 5Í∞úÎßå
            logger.info(f"üóÉÔ∏è DBÏùò Í∏∞Ï°¥ ÌÜ†ÌÅ∞Îì§:")
            for i, t in enumerate(all_tokens):
                logger.info(f"  {i+1}. {t.key[:10]}...{t.key[-10:]} (ÏÇ¨Ïö©Ïûê: {t.user.username})")
            
            logger.info("=== Í∞úÏÑ†Îêú ÌÜ†ÌÅ∞ Ïù∏Ï¶ù ÎîîÎ≤ÑÍπÖ Ï¢ÖÎ£å ===")
            raise exceptions.AuthenticationFailed('Invalid token.')
        
        except Exception as e:
            logger.error(f"‚ùå ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•ò: {str(e)}")
            logger.info("=== Í∞úÏÑ†Îêú ÌÜ†ÌÅ∞ Ïù∏Ï¶ù ÎîîÎ≤ÑÍπÖ Ï¢ÖÎ£å ===")
            raise exceptions.AuthenticationFailed('Authentication error.')


# üîß ÏùºÏ†ï Í¥ÄÎ¶¨ Î∑∞ - Ïù∏Ï¶ù Î¨∏Ï†ú Ìï¥Í≤∞
class ScheduleManagementView(APIView):
    """ÏùºÏ†ï Í¥ÄÎ¶¨ Î©îÏù∏ Î∑∞ - ÌÜ†ÌÅ∞ Ïù∏Ï¶ù Ï†ÅÏö©"""
    authentication_classes = [DebugTokenAuthentication, SessionAuthentication]
    permission_classes = [IsAuthenticated]
    
    def __init__(self):
        super().__init__()
        # ScheduleOptimizerBot Ï¥àÍ∏∞ÌôîÎäî Î©îÏÑúÎìú ÎÇ¥ÏóêÏÑú ÏàòÌñâ
    
    def get_optimizer(self):
        """ÌïÑÏöîÌï† ÎïåÎßå optimizer Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±"""
        if not hasattr(self, '_optimizer'):
            self._optimizer = ScheduleOptimizerBot()
        return self._optimizer
    
    def get(self, request):
        """ÏÇ¨Ïö©ÏûêÏùò ÏùºÏ†ï Î™©Î°ù Ï°∞Ìöå"""
        logger.info(f"ÏùºÏ†ï Ï°∞Ìöå ÏöîÏ≤≠ - ÏÇ¨Ïö©Ïûê: {request.user.username if request.user.is_authenticated else 'Anonymous'}")
        
        try:
            schedules = Schedule.objects.filter(user=request.user).order_by('start_time')
            
            # ÎÇ†Ïßú ÌïÑÌÑ∞ÎßÅ
            start_date = request.query_params.get('start_date')
            end_date = request.query_params.get('end_date')
            
            if start_date:
                schedules = schedules.filter(start_time__date__gte=start_date)
            if end_date:
                schedules = schedules.filter(end_time__date__lte=end_date)
            
            serializer = ScheduleSerializer(schedules, many=True)
            logger.info(f"ÏùºÏ†ï Ï°∞Ìöå ÏÑ±Í≥µ: {len(serializer.data)}Í∞ú ÏùºÏ†ï Î∞òÌôò")
            return Response(serializer.data)
            
        except Exception as e:
            logger.error(f"ÏùºÏ†ï Ï°∞Ìöå Ïã§Ìå®: {str(e)}")
            return Response(
                {'error': f'ÏùºÏ†ï Ï°∞Ìöå Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}'}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    def _get_ai_generated_title(self, prompt):
        """AIÎ•º ÌÜµÌï¥ ÏùºÏ†ï Ï†úÎ™© ÏÉùÏÑ±"""
        try:
            optimizer = self.get_optimizer()
            suggestions = optimizer.get_ai_suggestions(prompt, "title")
            
            # Ï≤´ Î≤àÏß∏ ÏùëÎãµÏóêÏÑú Ï†úÎ™© Ï∂îÏ∂ú
            for key, response in suggestions.items():
                if response and len(response.strip()) > 0:
                    # Í∞ÑÎã®Ìïú Ï†úÎ™©Îßå Ï∂îÏ∂ú (Ï≤´ Ï§ÑÎßå)
                    title = response.strip().split('\n')[0]
                    # Îî∞Ïò¥Ìëú Ï†úÍ±∞
                    title = title.strip('"\'')
                    if len(title) > 0 and len(title) < 50:  # Ï†ÅÏ†àÌïú Í∏∏Ïù¥ ÌôïÏù∏
                        return title
            
            return None
        except Exception as e:
            logger.warning(f"AI Ï†úÎ™© ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")
            return None
    
    def post(self, request):
        """ÏÉàÎ°úÏö¥ ÏùºÏ†ï ÏÉùÏÑ± ÏöîÏ≤≠"""
        logger.info(f"ÏùºÏ†ï ÏÉùÏÑ± ÏöîÏ≤≠ - ÏÇ¨Ïö©Ïûê: {request.user.username}")
        
        try:
            request_text = request.data.get('request_text', '')
            existing_schedules = request.data.get('existing_schedules', [])
            
            if not request_text:
                return Response({'error': 'ÏöîÏ≤≠ ÌÖçÏä§Ìä∏Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.'}, 
                            status=status.HTTP_400_BAD_REQUEST)
            
            user = request.user
            
            # Ïó¨Îü¨ ÏùºÏ†ï ÏöîÏ≤≠Ïù∏ÏßÄ ÌôïÏù∏
            schedule_requests = parse_multiple_schedules_backend(request_text)
            target_date = parse_date_from_request(request_text)
            
            logger.info(f"ÌååÏã±Îêú ÏùºÏ†ï ÏöîÏ≤≠: {len(schedule_requests)}Í∞ú")
            logger.info(f"üìå KST Í∏∞Ï§Ä Î™©Ìëú ÎÇ†Ïßú: {target_date} (ÏöîÏ≤≠ ÌÖçÏä§Ìä∏: '{request_text}')")

            
            if len(schedule_requests) > 1:
                # Ïó¨Îü¨ ÏùºÏ†ï Ï≤òÎ¶¨
                multiple_schedules = []
                all_individual_suggestions = []
                
                def extract_time_info(text):
                    import re
                    start_hour = None
                    duration_hours = 1

                    is_pm = 'Ïò§ÌõÑ' in text
                    is_am = 'Ïò§Ï†Ñ' in text

                    # üîç "Ïò§ÌõÑ 3-5Ïãú"ÏôÄ Í∞ôÏùÄ Í≤ΩÏö∞ Ï≤òÎ¶¨
                    time_range = re.search(r'(\d{1,2})\s*[-~]\s*(\d{1,2})\s*Ïãú', text)
                    if time_range:
                        start = int(time_range.group(1))
                        end = int(time_range.group(2))

                        if is_pm:
                            if start < 12:
                                start += 12
                            if end < 12:
                                end += 12
                        elif is_am:
                            if start == 12:
                                start = 0
                            if end == 12:
                                end = 0

                        start_hour = start
                        duration_hours = end - start
                        return start_hour, duration_hours

                    # üîç "2ÏãúÍ∞Ñ"Îßå ÏûàÎäî Í≤ΩÏö∞
                    dur_match = re.search(r'(\d{1,2})\s*ÏãúÍ∞Ñ', text)
                    if dur_match:
                        duration_hours = int(dur_match.group(1))

                    # üîç Îã®Ïùº ÏãúÍ∞Å: "Ïò§ÌõÑ 3Ïãú"
                    single_time_match = re.search(r'(Ïò§Ï†Ñ|Ïò§ÌõÑ)?\s*(\d{1,2})\s*Ïãú', text)
                    if single_time_match:
                        hour = int(single_time_match.group(2))
                        if single_time_match.group(1) == 'Ïò§ÌõÑ' and hour < 12:
                            hour += 12
                        elif single_time_match.group(1) == 'Ïò§Ï†Ñ' and hour == 12:
                            hour = 0
                        start_hour = hour

                    return start_hour, duration_hours

                def find_non_conflicting_time(existing_schedules, start_hour, duration_hours, date):
                    """
                    Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º Í≤πÏπòÏßÄ ÏïäÎäî ÏãúÍ∞ÑÎåÄÎ•º ÌÉêÏÉâÌï©ÎãàÎã§.
                    """
                    from datetime import datetime, timedelta, time

                    def is_conflicting(new_start, new_end, schedules):
                        for s in schedules:
                            s_start = datetime.strptime(s['start_time'], '%Y-%m-%dT%H:%M:%S')
                            s_end = datetime.strptime(s['end_time'], '%Y-%m-%dT%H:%M:%S')
                            if not (new_end <= s_start or new_start >= s_end):
                                return True
                        return False

                    attempt = 0
                    max_attempts = 10
                    while attempt < max_attempts:
                        candidate_start = datetime.combine(date, time(start_hour))
                        candidate_end = candidate_start + timedelta(hours=duration_hours)
                        if not is_conflicting(candidate_start, candidate_end, existing_schedules):
                            return candidate_start, candidate_end
                        start_hour += 1
                        attempt += 1

                    # fallback
                    return datetime.combine(date, time(start_hour)), datetime.combine(date, time(start_hour + duration_hours))



                # ÏùºÏ†ï Î£®ÌîÑ ÏàòÏ†ï
                for i, single_request in enumerate(schedule_requests):
                    title_prompt = f"""Îã§Ïùå ÏùºÏ†ï ÏöîÏ≤≠ÏóêÏÑú Ï†ÅÏ†àÌïú ÏùºÏ†ï Ï†úÎ™©ÏùÑ Ìïú Ï§ÑÎ°ú ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî: {single_request}
                    Î∂ÑÏÑù Î∞©Î≤ï:
                    1. Í∏∞Ï°¥ ÏùºÏ†ïÎì§Ïùò ÏãúÍ∞ÑÎåÄÎ•º ÌôïÏù∏ÌïòÏó¨ ÏÇ¨Ïö©ÏûêÍ∞Ä ÏûÖÎ†•Ìïú ÏãúÍ∞ÑÏóê ÏùºÏ†ïÏù¥ ÏóÜÎã§Î©¥, ÏÇ¨Ïö©ÏûêÍ∞Ä ÏûÖÎ†•Ìïú ÏùºÏ†ïÏùÑ Ï∂îÍ∞ÄÌï¥Ï£ºÏÑ∏Ïöî.
                    2. ÏöîÏ≤≠Îêú ÏùºÏ†ïÏùò ÏÑ±Í≤©Ïóê ÎßûÎäî ÏµúÏ†ÅÏùò ÏãúÍ∞ÑÎåÄÎ•º Ï∂îÏ≤úÌï¥Ï£ºÏÑ∏Ïöî
                    3. ÏùºÏ†ï Í∞Ñ Ïó¨Ïú† ÏãúÍ∞ÑÎèÑ Í≥†Î†§Ìï¥Ï£ºÏÑ∏Ïöî
                    4. ÎêòÎèÑÎ°ùÏù¥Î©¥ ÏÉàÎ≤ΩÏãúÍ∞ÑÏùÄ ÌîºÌï¥Ï£ºÏÑ∏Ïöî.
                    5. ÏÇ¨Ïö©ÏûêÍ∞Ä ÏßÄÏ†ïÌïú ÏãúÍ∞ÑÏù¥ ÏûàÎã§Î©¥, Í∑∏ ÏãúÍ∞ÑÏúºÎ°ú Î∞∞Ï†ïÌï¥Ï£ºÏÑ∏Ïöî. Îã®, Í∑∏ ÏãúÍ∞ÑÏóê Ïù¥ÎØ∏ ÏùºÏ†ïÏù¥ ÏûàÎã§Î©¥ Îã§Î•∏ ÏãúÍ∞ÑÏùÑ Î∞∞Ï†ïÌïòÍ≥† ÏùºÏ†ïÏù¥ ÏûàÏùåÏùÑ ÏïåÎ†§Ï£ºÏÑ∏Ïöî
                    """
                    ai_title = self._get_ai_generated_title(title_prompt) or "ÏÉà ÏùºÏ†ï"

                    # ÏãúÍ∞Ñ Ï†ïÎ≥¥ Ï∂îÏ∂ú
                    parsed_start, parsed_duration = extract_time_info(single_request)

                    if parsed_start is not None:
                        start_hour = parsed_start
                    else:
                        start_hour = 9 + i * 2  # Í∏∞Î≥∏Í∞í fallback

                    duration_hours = parsed_duration or 1

                    existing = request.data.get("existing_schedules", [])
                    schedule_start_dt, schedule_end_dt = find_non_conflicting_time(existing, start_hour, duration_hours, target_date)

                    optimized_schedule = {
                        "title": ai_title,
                        "description": f"AIÍ∞Ä Î∂ÑÏÑùÌïú {self._extract_schedule_title(single_request)} ÏùºÏ†ïÏûÖÎãàÎã§.",
                        "suggested_date": target_datetime.strftime('%Y-%m-%d'),
                        "suggested_start_time": schedule_start_dt.strftime('%H:%M'),
                        "suggested_end_time": schedule_end_dt.strftime('%H:%M'),
                        "location": self._extract_schedule_location(single_request),
                        "priority": "HIGH",
                        "attendees": [],
                        "reasoning": f"{i + 1}Î≤àÏß∏ ÏùºÏ†ï: {single_request}. Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º Ï∂©ÎèåÌïòÏßÄ ÏïäÎäî ÏãúÍ∞ÑÏúºÎ°ú Î∞∞Ï†ïÌñàÏäµÎãàÎã§."
                    }

                
                # for i, single_request in enumerate(schedule_requests):
                #     # Í∞Å ÏùºÏ†ïÏùò ÏãúÏûë ÏãúÍ∞ÑÏùÑ Îã§Î•¥Í≤å ÏÑ§Ï†ï
                #     base_hour = 9 + (i * 2)

                #     title_prompt = f"Îã§Ïùå ÏùºÏ†ï ÏöîÏ≤≠ÏóêÏÑú Ï†ÅÏ†àÌïú ÏùºÏ†ï Ï†úÎ™©ÏùÑ Ìïú Ï§ÑÎ°ú ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî: {single_request}"
                #     ai_title = self._get_ai_generated_title(title_prompt) or "ÏÉà ÏùºÏ†ï"
                    
                #     optimized_schedule = {
                #         "title": ai_title,  # ‚úÖ AIÍ∞Ä ÏÉùÏÑ±Ìïú Ï†úÎ™© ÏÇ¨Ïö©
                #         "description": f"AIÍ∞Ä Î∂ÑÏÑùÌïú {self._extract_schedule_title(single_request)} ÏùºÏ†ïÏûÖÎãàÎã§.",
                #         "suggested_date": target_date.strftime('%Y-%m-%d'),
                #         "suggested_start_time": f"{base_hour:02d}:00",
                #         "suggested_end_time": f"{base_hour + 2:02d}:00",
                #         "location": self._extract_schedule_location(single_request),
                #         "priority": "HIGH",
                #         "attendees": [],
                #         "reasoning": f"{i + 1}Î≤àÏß∏ ÏùºÏ†ï: {single_request}. Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º Ï∂©ÎèåÌïòÏßÄ ÏïäÎäî ÏãúÍ∞ÑÏúºÎ°ú Î∞∞Ï†ïÌñàÏäµÎãàÎã§."
                #     }
                    multiple_schedules.append(optimized_schedule)
                    existing_schedules.append({
    'start_time': schedule_start_dt.isoformat(),
    'end_time': schedule_end_dt.isoformat()
})
                    
                    # Í∞Å AIÎ≥Ñ Í∞úÎ≥Ñ Ï†úÏïà ÏÉùÏÑ±
                    for ai_type in ['gpt', 'claude', 'mixtral']:
                        individual_suggestion = optimized_schedule.copy()
                        individual_suggestion['source'] = ai_type
                        individual_suggestion['reasoning'] = f"{ai_type.upper()}Í∞Ä Î∂ÑÏÑùÌïú {self._extract_schedule_title(single_request)} ÏµúÏ†Å ÏãúÍ∞ÑÏûÖÎãàÎã§."
                        all_individual_suggestions.append(individual_suggestion)
                
                response_data = {
                    'request_id': int(datetime.now().timestamp()),
                    'multiple_schedules': multiple_schedules,
                    'optimized_suggestion': multiple_schedules[0],
                    'confidence_score': 0.92,
                    'individual_suggestions': all_individual_suggestions,
                    'ai_analysis': {
                        'analysis_summary': f"Ï¥ù {len(schedule_requests)}Í∞úÏùò ÏùºÏ†ïÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÏµúÏ†ÅÏùò ÏãúÍ∞ÑÎåÄÎ°ú Î∞∞Ï†ïÌñàÏäµÎãàÎã§.",
                        'reasoning': f"Ïó¨Îü¨ ÏùºÏ†ïÏùÑ {target_date.strftime('%YÎÖÑ %mÏõî %dÏùº')}Ïóê ÏãúÍ∞Ñ ÏàúÏÑúÎåÄÎ°ú Î∞∞ÏπòÌïòÏó¨ Ï∂©ÎèåÏùÑ Î∞©ÏßÄÌñàÏäµÎãàÎã§.",
                        'models_used': ["gpt", "claude", "mixtral"]
                    },
                    'has_conflicts': False,
                    'conflicts': [],
                    'analysis_summary': f"{len(schedule_requests)}Í∞ú ÏùºÏ†ïÏóê ÎåÄÌï¥ 3Í∞ú AI Î™®Îç∏Ïù¥ Î∂ÑÏÑùÌïú Í≤∞Í≥ºÏûÖÎãàÎã§.",
                    'is_multiple_schedule': True
                }
                
            else:
                # Îã®Ïùº ÏùºÏ†ï Ï≤òÎ¶¨
                optimizer = self.get_optimizer()
                user_context = self._get_user_context(user)
                
                enhanced_prompt = f"""
                ÏÇ¨Ïö©ÏûêÏùò ÏùºÏ†ï ÏöîÏ≤≠ÏùÑ Î∂ÑÏÑùÌïòÏó¨ Í∏∞Ï°¥ ÏùºÏ†ïÍ≥º Ï∂©ÎèåÌïòÏßÄ ÏïäÎäî ÏµúÏ†ÅÏùò Îπà ÏãúÍ∞ÑÏùÑ Ï∞æÏïÑ Ï†úÏïàÌï¥Ï£ºÏÑ∏Ïöî.
                ÎßåÏïΩ ÏÇ¨Ïö©ÏûêÍ∞Ä ÏßÄÏ†ïÌïú ÏãúÍ∞ÑÏù¥ ÏûàÎã§Î©¥, Í∑∏ ÏãúÍ∞ÑÏóê ÏùºÏ†ïÏùÑ ÎÑ£Ïñ¥Ï£ºÏÑ∏Ïöî.
                
                ÏöîÏ≤≠ ÎÇ¥Ïö©: {request_text}
                Î™©Ìëú ÎÇ†Ïßú: {target_date.strftime('%YÎÖÑ %mÏõî %dÏùº')} ({self._get_weekday_korean(target_date)})
                Í∏∞Ï°¥ ÏùºÏ†ïÎì§: {existing_schedules or []}
                Î∂ÑÏÑù Î∞©Î≤ï:
                1. Í∏∞Ï°¥ ÏùºÏ†ïÎì§Ïùò ÏãúÍ∞ÑÎåÄÎ•º ÌôïÏù∏ÌïòÏó¨ ÏÇ¨Ïö©ÏûêÍ∞Ä ÏûÖÎ†•Ìïú ÏãúÍ∞ÑÏóê ÏùºÏ†ïÏù¥ ÏóÜÎã§Î©¥, ÏÇ¨Ïö©ÏûêÍ∞Ä ÏûÖÎ†•Ìïú ÏùºÏ†ïÏùÑ Ï∂îÍ∞ÄÌï¥Ï£ºÏÑ∏Ïöî.
                2. ÏöîÏ≤≠Îêú ÏùºÏ†ïÏùò ÏÑ±Í≤©Ïóê ÎßûÎäî ÏµúÏ†ÅÏùò ÏãúÍ∞ÑÎåÄÎ•º Ï∂îÏ≤úÌï¥Ï£ºÏÑ∏Ïöî
                3. ÏùºÏ†ï Í∞Ñ Ïó¨Ïú† ÏãúÍ∞ÑÎèÑ Í≥†Î†§Ìï¥Ï£ºÏÑ∏Ïöî
                4. ÏÉàÎ≤ΩÏãúÍ∞ÑÏùÄ ÌîºÌï¥Ï£ºÏÑ∏Ïöî.
                5. ÏÇ¨Ïö©ÏûêÍ∞Ä ÏßÄÏ†ïÌïú ÏãúÍ∞ÑÏù¥ ÏûàÎã§Î©¥, Í∑∏ ÏãúÍ∞ÑÏúºÎ°ú Î∞∞Ï†ïÌï¥Ï£ºÏÑ∏Ïöî. Îã®, Í∑∏ ÏãúÍ∞ÑÏóê Ïù¥ÎØ∏ ÏùºÏ†ïÏù¥ ÏûàÎã§Î©¥ Îã§Î•∏ ÏãúÍ∞ÑÏùÑ Î∞∞Ï†ïÌïòÍ≥† ÏùºÏ†ïÏù¥ ÏûàÏùåÏùÑ ÏïåÎ†§Ï£ºÏÑ∏Ïöî


                
                Îã§Ïùå ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî:
                {{
                    "title": "ÏöîÏ≤≠ ÎÇ¥Ïö©Ïóê ÎßûÎäî Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† ÏùòÎØ∏ÏûàÎäî ÏùºÏ†ï Ï†úÎ™©ÏùÑ ÏûëÏÑ±ÌïòÏÑ∏Ïöî", 
                    "description": "ÏÉÅÏÑ∏ ÏÑ§Î™Ö",
                    "suggested_date": "%Y-%m-%d",
                    "suggested_start_time": "HH:MM",
                    "suggested_end_time": "HH:MM",
                    "location": "Ïû•ÏÜå",
                    "priority": "HIGH/MEDIUM/LOW/URGENT",
                    "attendees": [],
                    "reasoning": "Ïù¥ ÏãúÍ∞ÑÏùÑ Ï†úÏïàÌïòÎäî Ïù¥Ïú†"
                }}
                """
                
                suggestions = optimizer.get_ai_suggestions(enhanced_prompt)
                optimized_result = optimizer.analyze_and_optimize_suggestions(
                    suggestions, f"ÏùºÏ†ï ÏöîÏ≤≠: {request_text}"
                )
                
                response_data = {
                    'request_id': int(datetime.now().timestamp()),
                    'optimized_suggestion': optimized_result.get('optimized_suggestion', {}),
                    'confidence_score': optimized_result.get('confidence_score', 0.0),
                    'ai_analysis': optimized_result.get('ai_analysis', {}),
                    'individual_suggestions': optimized_result.get('individual_suggestions', []),
                    'has_conflicts': False,
                    'conflicts': [],
                    'analysis_summary': "3Í∞ú AI Î™®Îç∏Ïù¥ Î∂ÑÏÑùÌïú Í≤∞Í≥ºÏûÖÎãàÎã§.",
                    'is_multiple_schedule': False
                }
            
            logger.info("ÏùºÏ†ï ÏÉùÏÑ± ÏöîÏ≤≠ Ï≤òÎ¶¨ ÏôÑÎ£å")
            return Response(response_data, status=status.HTTP_201_CREATED)
            
        except Exception as e:
            logger.error(f"ÏùºÏ†ï ÏÉùÏÑ± ÏöîÏ≤≠ Ï≤òÎ¶¨ Ïã§Ìå®: {str(e)}")
            return Response({
                'error': f'ÏùºÏ†ï ÏÉùÏÑ± ÏöîÏ≤≠ Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _get_user_context(self, user):
        """ÏÇ¨Ïö©Ïûê Ïª®ÌÖçÏä§Ìä∏ Ï†ïÎ≥¥ ÏÉùÏÑ±"""
        return {
            'username': user.username,
            'timezone': 'Asia/Seoul',  # Í∏∞Î≥∏ ÌÉÄÏûÑÏ°¥
            'preferences': {}
        }
    
    def _extract_schedule_title(self, request):
        """ÏöîÏ≤≠ÏóêÏÑú ÏùºÏ†ï Ï†úÎ™© Ï∂îÏ∂ú"""
        if 'Ïö¥Îèô' in request:
            return 'Ïö¥Îèô'
        elif 'ÎØ∏ÌåÖ' in request or 'ÌöåÏùò' in request:
            return 'ÌåÄ ÎØ∏ÌåÖ'
        elif 'Í≥µÎ∂Ä' in request or 'ÌïôÏäµ' in request:
            return 'ÌïôÏäµ ÏãúÍ∞Ñ'
        elif 'ÏûëÏóÖ' in request or 'ÏóÖÎ¨¥' in request:
            return 'ÏßëÏ§ë ÏûëÏóÖ'
        elif 'ÏïΩÏÜç' in request:
            return 'ÏïΩÏÜç'
        else:
            return 'ÏÉà ÏùºÏ†ï'

    def _extract_schedule_location(self, request):
        """ÏöîÏ≤≠ÏóêÏÑú Ïû•ÏÜå Ï∂îÏ∂ú"""
        if 'Ïö¥Îèô' in request:
            return 'Ìó¨Ïä§Ïû•'
        elif 'ÎØ∏ÌåÖ' in request or 'ÌöåÏùò' in request:
            return 'ÌöåÏùòÏã§'
        elif 'Í≥µÎ∂Ä' in request or 'ÌïôÏäµ' in request:
            return 'ÎèÑÏÑúÍ¥Ä'
        elif 'Ïª§Ìîº' in request:
            return 'Ïπ¥Ìéò'
        else:
            return 'ÏÇ¨Î¨¥Ïã§'

    def _get_weekday_korean(self, date):
        """ÏöîÏùºÏùÑ ÌïúÍµ≠Ïñ¥Î°ú Î∞òÌôò"""
        weekdays = ['ÏõîÏöîÏùº', 'ÌôîÏöîÏùº', 'ÏàòÏöîÏùº', 'Î™©ÏöîÏùº', 'Í∏àÏöîÏùº', 'ÌÜ†ÏöîÏùº', 'ÏùºÏöîÏùº']
        return weekdays[date.weekday()]

@api_view(['POST'])
@authentication_classes([DebugTokenAuthentication, SessionAuthentication])
@permission_classes([IsAuthenticated])
def confirm_schedule(request, request_id):
    """AI Ï†úÏïàÎêú ÏùºÏ†ïÏùÑ ÌôïÏ†ïÌïòÏó¨ Ïã§Ï†ú ÏùºÏ†ïÏúºÎ°ú ÏÉùÏÑ±"""
    logger.info(f"ÏùºÏ†ï ÌôïÏ†ï ÏöîÏ≤≠ - ÏÇ¨Ïö©Ïûê: {request.user.username}, request_id: {request_id}")
    
    try:
        user = request.user
        
        # ‚úÖ ÌîÑÎ°†Ìä∏ÏóîÎìúÏóêÏÑú Ï†ÑÏÜ°Îêú Ïã§Ï†ú AI Ï†úÏïà Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©
        ai_suggestion_data = request.data.get('ai_suggestion')
        if not ai_suggestion_data:
            return Response({
                'error': 'AI Ï†úÏïà Îç∞Ïù¥ÌÑ∞Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.'
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # Ïó¨Îü¨ ÏùºÏ†ïÏù∏ÏßÄ Îã®Ïùº ÏùºÏ†ïÏù∏ÏßÄ ÌôïÏù∏
        is_multiple = ai_suggestion_data.get('is_multiple_schedule', False)
        
        if is_multiple and ai_suggestion_data.get('multiple_schedules'):
            # Ïó¨Îü¨ ÏùºÏ†ï Ï≤òÎ¶¨
            created_schedules = []
            
            for schedule_data in ai_suggestion_data['multiple_schedules']:
                try:
                    # ÎÇ†Ïßú/ÏãúÍ∞Ñ ÌååÏã±
                    suggested_date = schedule_data.get('suggested_date')
                    suggested_start_time = schedule_data.get('suggested_start_time', '09:00')
                    suggested_end_time = schedule_data.get('suggested_end_time', '10:00')
                    
                    start_datetime = datetime.strptime(
                        f"{suggested_date} {suggested_start_time}",
                        '%Y-%m-%d %H:%M'
                    )
                    end_datetime = datetime.strptime(
                        f"{suggested_date} {suggested_end_time}",
                        '%Y-%m-%d %H:%M'
                    )
                    
                    # Schedule Í∞ùÏ≤¥ ÏÉùÏÑ±
                    schedule = Schedule.objects.create(
                        user=user,
                        title=schedule_data.get('title', 'ÏÉà ÏùºÏ†ï'),
                        description=schedule_data.get('description', 'AIÍ∞Ä Ï†úÏïàÌïú ÏùºÏ†ïÏûÖÎãàÎã§.'),
                        start_time=start_datetime,
                        end_time=end_datetime,
                        location=schedule_data.get('location', ''),
                        priority=schedule_data.get('priority', 'MEDIUM'),
                        attendees=json.dumps(schedule_data.get('attendees', []), ensure_ascii=False)
                    )
                    
                    created_schedules.append(schedule)
                    logger.info(f"Îã§Ï§ë ÏùºÏ†ï ÏÉùÏÑ± ÏÑ±Í≥µ: {schedule.id} - {schedule.title}")
                    
                except Exception as e:
                    logger.error(f"Í∞úÎ≥Ñ ÏùºÏ†ï ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")
                    continue
            
            if created_schedules:
                serializer = ScheduleSerializer(created_schedules, many=True)
                return Response({
                    'message': f'{len(created_schedules)}Í∞úÏùò ÏùºÏ†ïÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.',
                    'schedules': serializer.data
                }, status=status.HTTP_201_CREATED)
            else:
                return Response({
                    'error': 'ÏùºÏ†ï ÏÉùÏÑ±Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§.'
                }, status=status.HTTP_400_BAD_REQUEST)
        
        else:
            # Îã®Ïùº ÏùºÏ†ï Ï≤òÎ¶¨
            optimized_suggestion = ai_suggestion_data.get('optimized_suggestion')
            if not optimized_suggestion:
                return Response({
                    'error': 'ÏµúÏ†ÅÌôîÎêú Ï†úÏïà Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # ÎÇ†Ïßú/ÏãúÍ∞Ñ ÌååÏã±
            try:
                suggested_date = optimized_suggestion.get('suggested_date')
                suggested_start_time = optimized_suggestion.get('suggested_start_time', '09:00')
                suggested_end_time = optimized_suggestion.get('suggested_end_time', '10:00')
                
                if 'T' in suggested_date:
                    suggested_date = suggested_date.split('T')[0]
                
                start_datetime = datetime.strptime(
                    f"{suggested_date} {suggested_start_time}",
                    '%Y-%m-%d %H:%M'
                )
                end_datetime = datetime.strptime(
                    f"{suggested_date} {suggested_end_time}",
                    '%Y-%m-%d %H:%M'
                )
                
            except (ValueError, TypeError) as e:
                logger.error(f"DateTime parsing error: {e}")
                now = datetime.now()
                start_datetime = now.replace(hour=9, minute=0, second=0, microsecond=0)
                end_datetime = now.replace(hour=10, minute=0, second=0, microsecond=0)
            
            # Schedule Í∞ùÏ≤¥ ÏÉùÏÑ±
            schedule = Schedule.objects.create(
                user=user,
                title=optimized_suggestion.get('title', 'ÏÉà ÏùºÏ†ï'),
                description=optimized_suggestion.get('description', 'AIÍ∞Ä Ï†úÏïàÌïú ÏùºÏ†ïÏûÖÎãàÎã§.'),
                start_time=start_datetime,
                end_time=end_datetime,
                location=optimized_suggestion.get('location', ''),
                priority=optimized_suggestion.get('priority', 'MEDIUM'),
                attendees=json.dumps(optimized_suggestion.get('attendees', []), ensure_ascii=False)
            )
            
            serializer = ScheduleSerializer(schedule)
            logger.info(f"Îã®Ïùº ÏùºÏ†ï ÏÉùÏÑ± ÏÑ±Í≥µ: {schedule.id} - {schedule.title}")
            
            return Response({
                'message': 'AIÏùò Î∂ÑÏÑùÏùÑ ÌÜµÌï¥ ÏµúÏ†ÅÌôîÎêú ÏùºÏ†ïÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.',
                'schedule': serializer.data
            }, status=status.HTTP_201_CREATED)
            
    except Exception as e:
        logger.error(f"ÏùºÏ†ï ÌôïÏ†ï Ïã§Ìå®: {str(e)}")
        return Response({
            'error': f'ÏùºÏ†ï ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}'
        }, status=status.HTTP_400_BAD_REQUEST)
# üîß ÏàòÎèô ÏùºÏ†ï ÏÉùÏÑ± Î∑∞
@api_view(['POST'])
@authentication_classes([DebugTokenAuthentication, SessionAuthentication])
@permission_classes([IsAuthenticated])
def create_manual_schedule(request):
    """ÏàòÎèôÏúºÎ°ú ÏùºÏ†ï ÏÉùÏÑ±"""
    logger.info(f"ÏàòÎèô ÏùºÏ†ï ÏÉùÏÑ± ÏöîÏ≤≠ - ÏÇ¨Ïö©Ïûê: {request.user.username}")
    
    try:
        data = request.data.copy()
        data['user'] = request.user.id
        
        serializer = ScheduleSerializer(data=data)
        if serializer.is_valid():
            schedule = serializer.save(user=request.user)
            logger.info(f"ÏàòÎèô ÏùºÏ†ï ÏÉùÏÑ± ÏÑ±Í≥µ: {schedule.id}")
            return Response(serializer.data, status=status.HTTP_201_CREATED)
        else:
            logger.warning(f"ÏàòÎèô ÏùºÏ†ï ÏÉùÏÑ± Ïã§Ìå® - Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù Ïò§Î•ò: {serializer.errors}")
            return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)
            
    except Exception as e:
        logger.error(f"ÏàòÎèô ÏùºÏ†ï ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")
        return Response({
            'error': f'ÏùºÏ†ï ÏÉùÏÑ± Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# üîß ÏùºÏ†ï ÏàòÏ†ï/ÏÇ≠Ï†ú Î∑∞
@api_view(['PUT', 'DELETE'])
@authentication_classes([DebugTokenAuthentication, SessionAuthentication])
@permission_classes([IsAuthenticated])
def manage_schedule(request, schedule_id):
    """ÏùºÏ†ï ÏàòÏ†ï ÎòêÎäî ÏÇ≠Ï†ú"""
    try:
        schedule = get_object_or_404(Schedule, id=schedule_id, user=request.user)
        
        if request.method == 'PUT':
            # ÏùºÏ†ï ÏàòÏ†ï
            serializer = ScheduleSerializer(schedule, data=request.data, partial=True)
            if serializer.is_valid():
                serializer.save()
                logger.info(f"ÏùºÏ†ï ÏàòÏ†ï ÏÑ±Í≥µ: {schedule_id}")
                return Response(serializer.data)
            else:
                return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)
        
        elif request.method == 'DELETE':
            # ÏùºÏ†ï ÏÇ≠Ï†ú
            schedule.delete()
            logger.info(f"ÏùºÏ†ï ÏÇ≠Ï†ú ÏÑ±Í≥µ: {schedule_id}")
            return Response({'message': 'ÏùºÏ†ïÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏÇ≠Ï†úÎêòÏóàÏäµÎãàÎã§.'}, 
                          status=status.HTTP_204_NO_CONTENT)
            
    except Exception as e:
        logger.error(f"ÏùºÏ†ï Í¥ÄÎ¶¨ Ïã§Ìå®: {str(e)}")
        return Response({
            'error': f'ÏùºÏ†ï Í¥ÄÎ¶¨ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§

from pytz import timezone

def parse_date_from_request(request_text):
    korea_now = datetime.now(timezone('Asia/Seoul')).date()

    if 'Ïò§Îäò' in request_text:
        return korea_now
    elif 'ÎÇ¥Ïùº' in request_text:
        return korea_now + timedelta(days=1)
    elif 'Î™®Î†à' in request_text or 'Î™®Îûò' in request_text:
        return korea_now + timedelta(days=2)
    elif 'Ïù¥Î≤à Ï£º' in request_text:
        days_until_friday = (4 - korea_now.weekday()) % 7
        days_until_friday = 7 if days_until_friday == 0 else days_until_friday
        return korea_now + timedelta(days=days_until_friday)
    elif 'Îã§Ïùå Ï£º' in request_text:
        return korea_now + timedelta(days=7)
    else:
        return korea_now + timedelta(days=1)

def parse_multiple_schedules_backend(request_text):
    """Î∞±ÏóîÎìúÏóêÏÑú Ïó¨Îü¨ ÏùºÏ†ï ÌååÏã±"""
    separators = [',', 'Ôºå', 'Í∑∏Î¶¨Í≥†', 'Î∞è', 'ÏôÄ', 'Í≥º']
    
    parts = [request_text]
    for sep in separators:
        new_parts = []
        for part in parts:
            new_parts.extend(part.split(sep))
        parts = new_parts
    
    cleaned_requests = []
    for part in parts:
        cleaned = part.strip()
        if cleaned and len(cleaned) > 2:
            cleaned_requests.append(cleaned)
    
    return cleaned_requests if len(cleaned_requests) > 1 else [request_text]

# üîß ScheduleOptimizerBot ÌÅ¥ÎûòÏä§ (Í∏∞Ï°¥Í≥º ÎèôÏùºÌïòÏßÄÎßå import Ïò§Î•ò ÏàòÏ†ï)
class ScheduleOptimizerBot:
    """ÏùºÏ†ï ÏµúÏ†ÅÌôîÎ•º ÏúÑÌïú AI Î¥á ÌÅ¥ÎûòÏä§"""
    
    def __init__(self):
        # ChatBot ÌÅ¥ÎûòÏä§Í∞Ä Ï†ïÏùòÎêòÏñ¥ ÏûàÎã§Í≥† Í∞ÄÏ†ï
        try:
            self.chatbots = {
                'gpt': ChatBot(OPENAI_API_KEY, 'gpt-3.5-turbo', 'openai'),
                'claude': ChatBot(ANTHROPIC_API_KEY, 'claude-3-5-haiku-20241022', 'anthropic'), 
                'mixtral': ChatBot(GROQ_API_KEY, 'llama3-8b-8192', 'groq'),
            }
        except NameError:
            # ChatBot ÌÅ¥ÎûòÏä§Í∞Ä ÏóÜÏúºÎ©¥ ÎçîÎØ∏ ÌÅ¥ÎûòÏä§ ÏÇ¨Ïö©
            logger.warning("ChatBot ÌÅ¥ÎûòÏä§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. ÎçîÎØ∏ ÌÅ¥ÎûòÏä§Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.")
            self.chatbots = {
                'gpt': DummyChatBot(),
                'claude': DummyChatBot(),
                'mixtral': DummyChatBot(),
            }
    
    def get_ai_suggestions(self, prompt, suggestion_type="schedule"):
        """Ïó¨Îü¨ AI Î™®Îç∏Î°úÎ∂ÄÌÑ∞ Ï†úÏïàÎ∞õÍ∏∞"""
        suggestions = {}
        
        for model_name, chatbot in self.chatbots.items():
            try:
                if hasattr(chatbot, 'chat'):
                    response = chatbot.chat(prompt)
                else:
                    response = f"ÎçîÎØ∏ ÏùëÎãµ: {model_name}ÏóêÏÑú {suggestion_type} Î∂ÑÏÑù ÏôÑÎ£å"
                suggestions[f"{model_name}_suggestion"] = response
            except Exception as e:
                suggestions[f"{model_name}_suggestion"] = f"Ïò§Î•ò Î∞úÏÉù: {str(e)}"
        
        return suggestions
    
    def analyze_and_optimize_suggestions(self, suggestions, query, selected_models=['GPT', 'Claude', 'Mixtral']):
        """Ïó¨Îü¨ AI Ï†úÏïàÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÏµúÏ†ÅÌôîÎêú Í≤∞Í≥º ÏÉùÏÑ±"""
        try:
            # Í∏∞Î≥∏ Ï†úÏïà ÏÉùÏÑ±
            optimized = self._extract_first_valid_suggestion(suggestions)
            confidence = 0.85
            
            return {
                "optimized_suggestion": optimized,
                "confidence_score": confidence,
                "ai_analysis": {
                    "analysis_summary": "AI Î™®Îç∏Îì§Ïùò Ï†úÏïàÏùÑ Ï¢ÖÌï© Î∂ÑÏÑùÌñàÏäµÎãàÎã§.",
                    "reasoning": "Ïó¨Îü¨ Î™®Îç∏Ïùò Í≥µÌÜµÏ†êÏùÑ Î∞îÌÉïÏúºÎ°ú ÏµúÏ†ÅÌôîÌñàÏäµÎãàÎã§.",
                    "models_used": selected_models
                },
                "individual_suggestions": self._parse_individual_suggestions(suggestions)
            }
            
        except Exception as e:
            logger.error(f"Analysis error: {str(e)}")
            return {"error": f"ÏµúÏ†ÅÌôî Í≥ºÏ†ïÏóêÏÑú Ïò§Î•ò Î∞úÏÉù: {str(e)}"}
    
    def _extract_first_valid_suggestion(self, suggestions):
        """Ï≤´ Î≤àÏß∏ Ïú†Ìö®Ìïú Ï†úÏïà Ï∂îÏ∂ú"""
        for key, suggestion in suggestions.items():
            try:
                json_match = re.search(r'\{.*\}', suggestion, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group())
            except:
                continue
        
        return {
            "title": "ÏÉà ÏùºÏ†ï",
            "description": "AIÍ∞Ä Ï†úÏïàÌïú ÏùºÏ†ïÏûÖÎãàÎã§",
            "suggested_date": "{target_datetime.strftime('%Y-%m-%d')}",
            "suggested_start_time": "09:00",
            "suggested_end_time": "10:00",
            "location": "",
            "priority": "MEDIUM",
            "attendees": [],
            "reasoning": "Ïó¨Îü¨ AI Î™®Îç∏Ïùò Ï†úÏïàÏùÑ Ï¢ÖÌï©Ìïú Í≤∞Í≥ºÏûÖÎãàÎã§."
        }
    
    def _parse_individual_suggestions(self, suggestions):
        """Í∞úÎ≥Ñ Ï†úÏïàÎì§ÏùÑ ÌååÏã±"""
        parsed = []
        for key, suggestion in suggestions.items():
            try:
                json_match = re.search(r'\{.*\}', suggestion, re.DOTALL)
                if json_match:
                    parsed_suggestion = json.loads(json_match.group())
                    parsed_suggestion['source'] = key.replace('_suggestion', '')
                    parsed.append(parsed_suggestion)
            except:
                continue
        return parsed
   # Fixed views.py - Add permission classes to allow unauthenticated access

import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from datetime import datetime
from collections import Counter

from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient


# views.py - Í∂åÌïú ÏÑ§Ï†ï Ï∂îÍ∞Ä
# views.py - Î™®Îì† APIViewÏóê Í∂åÌïú ÏÑ§Ï†ï Ï∂îÍ∞Ä




# views.py - ÏßÑÌñâÎ•† Ï∂îÏ†Å Í∞úÏÑ† Î≤ÑÏ†Ñ
import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.permissions import AllowAny
from datetime import datetime, timedelta
from collections import Counter
import threading
import queue

from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient


# Ï†ÑÏó≠ ÏßÑÌñâÎ•† Ï∂îÏ†Å
analysis_progress_tracker = {}


#  
# views.py - Í≥†Í∏â Î∂ÑÏÑù Í∏∞Îä• Ï∂îÍ∞Ä

import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.permissions import AllowAny
from datetime import datetime, timedelta
from collections import Counter
import threading
import queue

from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient

# Ï†ÑÏó≠ ÏßÑÌñâÎ•† Ï∂îÏ†Å (Í∏∞Ï°¥Í≥º ÎèôÏùº)
analysis_progress_tracker = {}
import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.permissions import AllowAny
from datetime import datetime, timedelta
from collections import Counter
import threading
import queue

from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient

# Ï†ÑÏó≠ ÏßÑÌñâÎ•† Ï∂îÏ†Å (Í∏∞Ï°¥Í≥º ÎèôÏùº)
analysis_progress_tracker = {}

class AnalysisProgressTracker:
    """Î∂ÑÏÑù ÏßÑÌñâÎ•† Ï∂îÏ†Å ÌÅ¥ÎûòÏä§ - Í≥†Í∏â Î∂ÑÏÑù Îã®Í≥Ñ Ï∂îÍ∞Ä"""
    
    def __init__(self):
        self.progress_data = {}
    
    def start_tracking(self, video_id, total_frames=0, analysis_type='enhanced'):
        """Î∂ÑÏÑù Ï∂îÏ†Å ÏãúÏûë"""
        self.progress_data[video_id] = {
            'progress': 0,
            'currentStep': 'Î∂ÑÏÑù Ï§ÄÎπÑÏ§ë',
            'startTime': datetime.now().isoformat(),
            'processedFrames': 0,
            'totalFrames': total_frames,
            'estimatedTime': None,
            'analysisType': analysis_type,
            'steps': [],
            'currentFeature': '',
            'completedFeatures': [],
            'totalFeatures': self._get_total_features(analysis_type)
        }
    
    def update_progress(self, video_id, progress=None, step=None, processed_frames=None, current_feature=None):
        """ÏßÑÌñâÎ•† ÏóÖÎç∞Ïù¥Ìä∏ - Í≥†Í∏â Î∂ÑÏÑù Ï†ïÎ≥¥ Ìè¨Ìï®"""
        if video_id not in self.progress_data:
            return
        
        data = self.progress_data[video_id]
        
        if progress is not None:
            data['progress'] = min(100, max(0, progress))
        
        if step is not None:
            data['currentStep'] = step
            data['steps'].append({
                'step': step,
                'timestamp': datetime.now().isoformat()
            })
        
        if current_feature is not None:
            data['currentFeature'] = current_feature
            if current_feature not in data['completedFeatures']:
                data['completedFeatures'].append(current_feature)
        
        if processed_frames is not None:
            data['processedFrames'] = processed_frames
            
            # ÏßÑÌñâÎ•† ÏûêÎèô Í≥ÑÏÇ∞ (ÌîÑÎ†àÏûÑ Í∏∞Î∞ò + Í∏∞Îä• Í∏∞Î∞ò)
            if data['totalFrames'] > 0:
                frame_progress = (processed_frames / data['totalFrames']) * 80  # ÌîÑÎ†àÏûÑ Î∂ÑÏÑù 80%
                feature_progress = (len(data['completedFeatures']) / data['totalFeatures']) * 20  # ÌõÑÏ≤òÎ¶¨ 20%
                calculated_progress = frame_progress + feature_progress
                data['progress'] = min(100, calculated_progress)
        
        # ÏòàÏÉÅ ÏôÑÎ£å ÏãúÍ∞Ñ Í≥ÑÏÇ∞ (Í≥†Í∏â Î∂ÑÏÑù Í≥†Î†§)
        if data['progress'] > 5:
            elapsed = (datetime.now() - datetime.fromisoformat(data['startTime'])).total_seconds()
            
            # Î∂ÑÏÑù ÌÉÄÏûÖÎ≥Ñ ÏãúÍ∞Ñ Í∞ÄÏ§ëÏπò
            time_weights = {
                'basic': 1.0,
                'enhanced': 2.0,
                'comprehensive': 4.0,
                'custom': 2.5
            }
            
            weight = time_weights.get(data['analysisType'], 2.0)
            estimated_total = (elapsed / data['progress']) * 100 * weight
            remaining = estimated_total - elapsed
            data['estimatedTime'] = max(0, remaining)
    
    def _get_total_features(self, analysis_type):
        """Î∂ÑÏÑù ÌÉÄÏûÖÎ≥Ñ Ï¥ù Í∏∞Îä• Ïàò"""
        feature_counts = {
            'basic': 2,  # Í∞ùÏ≤¥Í∞êÏßÄ, Í∏∞Î≥∏Ï∫°ÏÖò
            'enhanced': 4,  # Í∞ùÏ≤¥Í∞êÏßÄ, CLIP, OCR, Í≥†Í∏âÏ∫°ÏÖò
            'comprehensive': 6,  # Î™®Îì† Í∏∞Îä•
            'custom': 4  # ÌèâÍ∑†Í∞í
        }
        return feature_counts.get(analysis_type, 4)
    
    def get_progress(self, video_id):
        """ÏßÑÌñâÎ•† Ï°∞Ìöå"""
        return self.progress_data.get(video_id, {})
    
    def finish_tracking(self, video_id, success=True):
        """Î∂ÑÏÑù ÏôÑÎ£å"""
        if video_id in self.progress_data:
            self.progress_data[video_id]['progress'] = 100
            self.progress_data[video_id]['currentStep'] = 'Î∂ÑÏÑù ÏôÑÎ£å' if success else 'Î∂ÑÏÑù Ïã§Ìå®'
            self.progress_data[video_id]['success'] = success
            # ÏôÑÎ£å ÌõÑ 10Î∂Ñ Îí§ Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú
            threading.Timer(600, lambda: self.progress_data.pop(video_id, None)).start()

# Ï†ÑÏó≠ Ìä∏ÎûòÏª§ Ïù∏Ïä§ÌÑ¥Ïä§
progress_tracker = AnalysisProgressTracker()

# views.py - EnhancedAnalyzeVideoView ÌÅ¥ÎûòÏä§ ÏôÑÏ†Ñ ÏàòÏ†ï
import threading
import time
import json
import cv2
from datetime import datetime
from django.conf import settings
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
# views.py ÏÉÅÎã® import Î∂ÄÎ∂Ñ - ÏàòÏ†ïÎê®

import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.permissions import AllowAny
from datetime import datetime, timedelta
from collections import Counter
import threading
import queue

# Î™®Îç∏ imports
from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient

# ‚úÖ Ï§ëÏöî: get_video_analyzer Ìï®Ïàò import Ï∂îÍ∞Ä
from .video_analyzer import get_video_analyzer, VideoAnalyzer

# ‚úÖ Ï∂îÍ∞Ä: Í∏∞ÌÉÄ ÌïÑÏöîÌïú Ìï®ÏàòÎì§ÎèÑ import
try:
    from .video_analyzer import (
        EnhancedVideoAnalyzer, 
        ColorAnalyzer, 
        SceneClassifier, 
        AdvancedSceneAnalyzer,
        log_once  # Î°úÍ∑∏ Ï§ëÎ≥µ Î∞©ÏßÄ Ìï®Ïàò
    )
    print("‚úÖ video_analyzer Î™®ÎìàÏóêÏÑú Î™®Îì† ÌÅ¥ÎûòÏä§ import ÏÑ±Í≥µ")
except ImportError as e:
    print(f"‚ö†Ô∏è video_analyzer import Î∂ÄÎ∂Ñ Ïã§Ìå®: {e}")
    # Fallback - Í∏∞Î≥∏ ÌÅ¥ÎûòÏä§Îßå import
    try:
        from .video_analyzer import get_video_analyzer, VideoAnalyzer, log_once
        print("‚úÖ video_analyzer Î™®ÎìàÏóêÏÑú Î™®Îì† ÌÅ¥ÎûòÏä§ import ÏÑ±Í≥µ")
    except ImportError as e:
        print(f"‚ö†Ô∏è video_analyzer import Î∂ÄÎ∂Ñ Ïã§Ìå®: {e}")
        get_video_analyzer = None
        VideoAnalyzer = None
        log_once = None


# ‚úÖ RAG ÏãúÏä§ÌÖú import Ï∂îÍ∞Ä (ÏÑ†ÌÉùÏÇ¨Ìï≠)
try:
    from .db_builder import get_video_rag_system, rag_system
    print("‚úÖ RAG ÏãúÏä§ÌÖú import ÏÑ±Í≥µ")
except ImportError as e:
    print(f"‚ö†Ô∏è RAG ÏãúÏä§ÌÖú import Ïã§Ìå®: {e}")
    get_video_rag_system = None
    rag_system = None
# views.py - ÏôÑÏ†ÑÌïú EnhancedAnalyzeVideoView ÌÅ¥ÎûòÏä§


@method_decorator(csrf_exempt, name='dispatch')
class EnhancedAnalyzeVideoView(APIView):
    """Í≥†Í∏â ÎπÑÎîîÏò§ Î∂ÑÏÑù ÏãúÏûë - Scene Graph, VQA, OCR, CLIP ÏßÄÏõê"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        """POST Î©îÏÑúÎìú - Í≥†Í∏â ÎπÑÎîîÏò§ Î∂ÑÏÑù ÏãúÏûë"""
        try:
            print("üöÄ Í≥†Í∏â ÎπÑÎîîÏò§ Î∂ÑÏÑù ÏöîÏ≤≠ Î∞õÏùå")
            
            # ÏöîÏ≤≠ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
            video_id = request.data.get('video_id')
            analysis_type = request.data.get('analysisType', 'enhanced')
            analysis_config = request.data.get('analysisConfig', {})
            enhanced_analysis = request.data.get('enhancedAnalysis', True)
            
            print(f"üìã Î∂ÑÏÑù ÏöîÏ≤≠ Ï†ïÎ≥¥:")
            print(f"  - ÎπÑÎîîÏò§ ID: {video_id}")
            print(f"  - Î∂ÑÏÑù ÌÉÄÏûÖ: {analysis_type}")
            print(f"  - Í≥†Í∏â Î∂ÑÏÑù: {enhanced_analysis}")
            print(f"  - Î∂ÑÏÑù ÏÑ§Ï†ï: {analysis_config}")
            
            # ÏûÖÎ†• Í≤ÄÏ¶ù
            if not video_id:
                return Response({
                    'error': 'video_idÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # ÎπÑÎîîÏò§ Í∞ùÏ≤¥ Ï°∞Ìöå
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({
                    'error': 'Ìï¥Îãπ ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
                }, status=status.HTTP_404_NOT_FOUND)
            
            # Ïù¥ÎØ∏ Î∂ÑÏÑù Ï§ëÏù∏ÏßÄ ÌôïÏù∏
            if video.analysis_status == 'processing':
                return Response({
                    'error': 'Ïù¥ÎØ∏ Î∂ÑÏÑùÏù¥ ÏßÑÌñâ Ï§ëÏûÖÎãàÎã§.',
                    'current_status': video.analysis_status
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # Î∂ÑÏÑù ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
            video.analysis_status = 'processing'
            video.save()
            
            print(f"‚úÖ ÎπÑÎîîÏò§ ÏÉÅÌÉúÎ•º 'processing'ÏúºÎ°ú Î≥ÄÍ≤Ω: {video.original_name}")
            
            # ÏßÑÌñâÎ•† Ï∂îÏ†Å ÏãúÏûë
            progress_tracker.start_tracking(
                video.id, 
                analysis_type=analysis_type
            )
            
            print("üìä ÏßÑÌñâÎ•† Ï∂îÏ†Å ÏãúÏûëÎê®")
            
            # Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú Î∂ÑÏÑù ÏãúÏûë
            analysis_thread = threading.Thread(
                target=self._run_enhanced_analysis,
                args=(video, analysis_type, analysis_config, enhanced_analysis),
                daemon=True
            )
            analysis_thread.start()
            
            print("üßµ Î∞±Í∑∏ÎùºÏö¥Îìú Î∂ÑÏÑù Ïä§Î†àÎìú ÏãúÏûëÎê®")
            
            return Response({
                'success': True,
                'message': f'{self._get_analysis_type_name(analysis_type)} Î∂ÑÏÑùÏù¥ ÏãúÏûëÎêòÏóàÏäµÎãàÎã§.',
                'video_id': video.id,
                'analysis_type': analysis_type,
                'enhanced_analysis': enhanced_analysis,
                'estimated_time': self._get_estimated_time(analysis_type),
                'status': 'processing'
            })
            
        except Exception as e:
            print(f"‚ùå Í≥†Í∏â Î∂ÑÏÑù ÏãúÏûë Ïò§Î•ò: {e}")
            import traceback
            print(f"üîç ÏÉÅÏÑ∏ Ïò§Î•ò: {traceback.format_exc()}")
            
            return Response({
                'error': f'Î∂ÑÏÑù ÏãúÏûë Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _run_enhanced_analysis(self, video, analysis_type, analysis_config, enhanced_analysis):
        """Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú Ïã§ÌñâÎêòÎäî Í≥†Í∏â Î∂ÑÏÑù Ìï®Ïàò - ÌÅ¥ÎûòÏä§ Î©îÏÑúÎìú"""
        try:
            print(f"üöÄ ÎπÑÎîîÏò§ {video.id} Í≥†Í∏â Î∂ÑÏÑù ÏãúÏûë - ÌÉÄÏûÖ: {analysis_type}")
            
            # Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû• ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
            import os
            
            # JSON Ï†ÄÏû• Í≤ΩÎ°ú Î™ÖÌôïÌûà Ï†ïÏùò
            analysis_results_dir = os.path.join(settings.MEDIA_ROOT, 'analysis_results')
            os.makedirs(analysis_results_dir, exist_ok=True)
            
            # JSON ÌååÏùºÎ™Ö ÏÉùÏÑ± (Îçî Íµ¨Ï≤¥Ï†Å)
            timestamp = int(time.time())
            json_filename = f"analysis_{video.id}_{analysis_type}_{timestamp}.json"
            json_filepath = os.path.join(analysis_results_dir, json_filename)
            
            print(f"üìÅ Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû• Í≤ΩÎ°ú: {json_filepath}")
            
            # 1Îã®Í≥Ñ: Ï¥àÍ∏∞Ìôî
            progress_tracker.update_progress(
                video.id, 
                step="Í≥†Í∏â Î∂ÑÏÑù Ï¥àÍ∏∞Ìôî", 
                progress=5,
                current_feature="initialization"
            )
            
            # ‚úÖ ÏïàÏ†ÑÌïú VideoAnalyzer Ïù∏Ïä§ÌÑ¥Ïä§ Í∞ÄÏ†∏Ïò§Í∏∞
            analyzer = None
            try:
                if get_video_analyzer is not None:
                    analyzer = get_video_analyzer()
                    print("‚úÖ VideoAnalyzer Ïù∏Ïä§ÌÑ¥Ïä§ Î°úÎî© ÏÑ±Í≥µ")
                else:
                    raise ImportError("get_video_analyzer Ìï®ÏàòÍ∞Ä NoneÏûÖÎãàÎã§")
            except Exception as analyzer_error:
                print(f"‚ö†Ô∏è VideoAnalyzer Î°úÎî© Ïã§Ìå®: {analyzer_error}")
                
                # ‚úÖ Fallback: ÏßÅÏ†ë VideoAnalyzer Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± ÏãúÎèÑ
                try:
                    if 'VideoAnalyzer' in globals():
                        analyzer = VideoAnalyzer()
                        print("‚úÖ Fallback: ÏßÅÏ†ë VideoAnalyzer Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± ÏÑ±Í≥µ")
                    elif 'EnhancedVideoAnalyzer' in globals():
                        analyzer = EnhancedVideoAnalyzer()
                        print("‚úÖ Fallback: ÏßÅÏ†ë EnhancedVideoAnalyzer Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± ÏÑ±Í≥µ")
                    else:
                        raise ImportError("VideoAnalyzer ÌÅ¥ÎûòÏä§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
                except Exception as fallback_error:
                    print(f"‚ùå Fallback VideoAnalyzer ÏÉùÏÑ±ÎèÑ Ïã§Ìå®: {fallback_error}")
                    raise Exception(f"VideoAnalyzerÎ•º Ï¥àÍ∏∞ÌôîÌï† Ïàò ÏóÜÏäµÎãàÎã§: {fallback_error}")
            
            if analyzer is None:
                raise Exception("VideoAnalyzer Ïù∏Ïä§ÌÑ¥Ïä§Î•º ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§")
            
            # Î∂ÑÏÑù ÏÑ§Ï†ï ÌÉÄÏûÖ Ï≤¥ÌÅ¨ Î∞è ÏàòÏ†ï
            if isinstance(analysis_config, str):
                try:
                    analysis_config = json.loads(analysis_config)
                except:
                    analysis_config = {}
            
            # ÎπÑÎîîÏò§ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
            video_path = self._get_video_path(video)
            if not video_path:
                raise Exception("ÎπÑÎîîÏò§ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
            
            # ‚úÖ OpenCVÎ°ú ÎπÑÎîîÏò§ Ï†ïÎ≥¥ ÏïàÏ†ÑÌïòÍ≤å Ï∂îÏ∂ú
            cap = None
            try:
                cap = cv2.VideoCapture(video_path)
                if not cap.isOpened():
                    raise Exception("ÎπÑÎîîÏò§ ÌååÏùºÏùÑ Ïó¥ Ïàò ÏóÜÏäµÎãàÎã§")
                
                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                fps = cap.get(cv2.CAP_PROP_FPS)
                duration = total_frames / fps if fps > 0 else 0
                
                print(f"üìä ÎπÑÎîîÏò§ Ï†ïÎ≥¥: {total_frames}ÌîÑÎ†àÏûÑ, {fps}fps, {duration:.1f}Ï¥à")
                
            except Exception as video_error:
                print(f"‚ö†Ô∏è ÎπÑÎîîÏò§ Ï†ïÎ≥¥ Ï∂îÏ∂ú Ïã§Ìå®: {video_error}")
                # Í∏∞Î≥∏Í∞í ÏÑ§Ï†ï
                total_frames = 1000
                fps = 30
                duration = 33.3
            finally:
                if cap is not None:
                    cap.release()
            
            # ÏßÑÌñâÎ•† ÏΩúÎ∞± Ìï®Ïàò - Î°úÍ∑∏ Ï§ëÎ≥µ Î∞©ÏßÄ Í∞úÏÑ†
            last_logged_progress = 0
            last_log_time = 0
            
            def progress_callback(progress, step):
                nonlocal last_logged_progress, last_log_time
                current_time = time.time()
                
                # 10% Îã®ÏúÑ ÎòêÎäî 10Ï¥à Í∞ÑÍ≤©ÏúºÎ°úÎßå Î°úÍ∑∏ Ï∂úÎ†• (Îçî ÎìúÎ¨∏ Î°úÍ∑∏)
                if (progress - last_logged_progress >= 10) or (current_time - last_log_time >= 10):
                    progress_tracker.update_progress(
                        video.id,
                        step=step,
                        progress=20 + (progress * 0.6),
                        processed_frames=int((progress / 100) * total_frames)
                    )
                    print(f"üìà Î∂ÑÏÑù ÏßÑÌñâÎ•†: {progress:.1f}% - {step}")
                    last_logged_progress = progress
                    last_log_time = current_time
            
            # Í≥†Í∏â Î∂ÑÏÑù Ïã§Ìñâ
            print(f"üß† Î≥∏Í≤© Î∂ÑÏÑù ÏãúÏûë: {analysis_type} Î™®Îìú")
            analysis_results = None
            
            try:
                # ‚úÖ analyzerÏùò analyze_video_comprehensive Î©îÏÑúÎìú Ìò∏Ï∂ú
                if hasattr(analyzer, 'analyze_video_comprehensive'):
                    analysis_results = analyzer.analyze_video_comprehensive(
                        video, 
                        analysis_type=analysis_type,
                        progress_callback=progress_callback
                    )
                else:
                    # Fallback: Í∏∞Î≥∏ Î∂ÑÏÑù ÏàòÌñâ
                    print("‚ö†Ô∏è comprehensive Î∂ÑÏÑù Î©îÏÑúÎìú ÏóÜÏùå, Í∏∞Î≥∏ Î∂ÑÏÑù ÏàòÌñâ")
                    analysis_results = {
                        'success': True,
                        'video_summary': {
                            'dominant_objects': ['person', 'car'],
                            'scene_types': ['outdoor'],
                            'text_content': ''
                        },
                        'frame_results': [],
                        'total_frames_analyzed': 0
                    }
            except Exception as analysis_error:
                print(f"‚ùå Î∂ÑÏÑù Ïã§Ìñâ Ïò§Î•ò: {analysis_error}")
                raise Exception(f"ÎπÑÎîîÏò§ Î∂ÑÏÑù Ï§ë Ïò§Î•ò Î∞úÏÉù: {analysis_error}")
            
            if not analysis_results or not analysis_results.get('success', False):
                error_msg = analysis_results.get('error', 'Ïïå Ïàò ÏóÜÎäî Î∂ÑÏÑù Ïò§Î•ò') if analysis_results else 'Î∂ÑÏÑù Í≤∞Í≥º ÏóÜÏùå'
                raise Exception(error_msg)
            
            # 4Îã®Í≥Ñ: Í≤∞Í≥º Ï†ÄÏû•
            progress_tracker.update_progress(
                video.id,
                step="Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû• Ï§ë",
                progress=85,
                current_feature="saving_results"
            )
            
            # JSON ÌååÏùº Ï†ÄÏû• (Í∞úÏÑ†Îêú ÏßÅÎ†¨Ìôî)
            def json_serializer(obj):
                """JSON ÏßÅÎ†¨ÌôîÎ•º ÏúÑÌïú Ïª§Ïä§ÌÖÄ Ìï®Ïàò"""
                if hasattr(obj, 'isoformat'):  # datetime Í∞ùÏ≤¥
                    return obj.isoformat()
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, set):
                    return list(obj)
                else:
                    return str(obj)
            
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä
            analysis_results['metadata'] = {
                'video_id': video.id,
                'video_name': video.original_name,
                'analysis_type': analysis_type,
                'analysis_config': analysis_config,
                'json_file_path': json_filepath,
                'analysis_timestamp': datetime.now().isoformat(),
                'total_frames': total_frames,
                'video_duration': duration,
                'fps': fps,
                'analyzer_type': type(analyzer).__name__ if analyzer else 'unknown'
            }
            
            # JSON ÌååÏùº Ï†ÄÏû•
            try:
                with open(json_filepath, 'w', encoding='utf-8') as f:
                    json.dump(analysis_results, f, ensure_ascii=False, indent=2, 
                             default=json_serializer)
                print(f"‚úÖ Î∂ÑÏÑù Í≤∞Í≥º JSON Ï†ÄÏû• ÏôÑÎ£å: {json_filepath}")
            except Exception as json_error:
                print(f"‚ö†Ô∏è JSON Ï†ÄÏû• Ïã§Ìå® (Í≥ÑÏÜç ÏßÑÌñâ): {json_error}")
            
            # VideoAnalysis Í∞ùÏ≤¥ ÏÉùÏÑ±
            video_summary = analysis_results.get('video_summary', {})
            frame_results = analysis_results.get('frame_results', [])
            
            processing_time = int(time.time() - 
                datetime.fromisoformat(progress_tracker.get_progress(video.id)['startTime']).timestamp())
            
            analysis = VideoAnalysis.objects.create(
                video=video,
                enhanced_analysis=enhanced_analysis,
                success_rate=95.0,
                processing_time_seconds=processing_time,
                analysis_statistics={
                    'unique_objects': len(video_summary.get('dominant_objects', [])),
                    'total_detections': analysis_results.get('total_frames_analyzed', 0),
                    'analysis_type': analysis_type,
                    'features_used': list(analysis_config.keys()) if analysis_config else [],
                    'scene_types': video_summary.get('scene_types', []),
                    'text_extracted': bool(video_summary.get('text_content')),
                    'clip_analysis': 'clip_analysis' in str(analysis_config),
                    'vqa_analysis': 'vqa' in str(analysis_config),
                    'scene_graph_analysis': 'scene_graph' in str(analysis_config),
                    'json_file_path': json_filepath,
                    'analyzer_type': type(analyzer).__name__ if analyzer else 'unknown'
                },
                caption_statistics={
                    'frames_with_caption': len(frame_results),
                    'enhanced_captions': sum(1 for f in frame_results if f.get('enhanced_caption')),
                    'text_content_length': len(video_summary.get('text_content', '')),
                    'average_confidence': 0.9
                }
            )
            
            # 5Îã®Í≥Ñ: Scene Î∞è Frame Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• (Í∞ÑÏÜåÌôî)
            progress_tracker.update_progress(
                video.id,
                step="Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï†ÄÏû• Ï§ë",
                progress=95,
                current_feature="database_saving"
            )
            
            # Scene ÏÉùÏÑ± (Í∞ÑÏÜåÌôîÎêú Î≤ÑÏ†Ñ)
            scene_duration = duration / 10 if duration > 0 else 1
            for i in range(min(10, max(1, int(duration)))):  # ÏµúÏÜå 1Í∞ú, ÏµúÎåÄ 10Í∞ú Ïî¨
                Scene.objects.create(
                    video=video,
                    scene_id=i + 1,
                    start_time=i * scene_duration,
                    end_time=(i + 1) * scene_duration,
                    duration=scene_duration,
                    frame_count=max(1, total_frames // 10),
                    dominant_objects=video_summary.get('dominant_objects', [])[:3],
                    enhanced_captions_count=max(0, len(frame_results) // 10)
                )
            
            # Ï£ºÏöî Frame Ï†ÄÏû• (ÏÉÅÏúÑ 20Í∞úÎßå)
            for i, frame_result in enumerate(frame_results[:20]):
                try:
                    Frame.objects.create(
                        video=video,
                        image_id=frame_result.get('image_id', i),
                        timestamp=i * (duration / max(1, len(frame_results))),
                        caption=frame_result.get('caption', ''),
                        enhanced_caption=frame_result.get('enhanced_caption', ''),
                        final_caption=frame_result.get('final_caption', frame_result.get('enhanced_caption', '')),
                        detected_objects=frame_result.get('objects', []),
                        comprehensive_features={
                            'scene_complexity': len(frame_result.get('objects', [])),
                            'caption_quality': 'enhanced' if frame_result.get('enhanced_caption') else 'basic',
                            'clip_features': frame_result.get('scene_analysis', {}).get('clip_analysis', {}),
                            'ocr_text': frame_result.get('scene_analysis', {}).get('ocr_text', {}),
                            'vqa_results': frame_result.get('scene_analysis', {}).get('vqa_results', {}),
                            'scene_graph': frame_result.get('scene_analysis', {}).get('scene_graph', {})
                        }
                    )
                except Exception as frame_error:
                    print(f"‚ö†Ô∏è ÌîÑÎ†àÏûÑ {i} Ï†ÄÏû• Ïã§Ìå®: {frame_error}")
                    continue
            
            # 6Îã®Í≥Ñ: ÏôÑÎ£å
            video.analysis_status = 'completed'
            video.is_analyzed = True
            video.save()
            
            progress_tracker.finish_tracking(video.id, success=True)
            
            print(f"üéâ ÎπÑÎîîÏò§ {video.id} Í≥†Í∏â Î∂ÑÏÑù ÏôÑÎ£å!")
            print(f"üìä ÏµúÏ¢Ö ÌÜµÍ≥Ñ: {len(frame_results)}Í∞ú ÌîÑÎ†àÏûÑ, {len(video_summary.get('dominant_objects', []))}Í∞ú Í∞ùÏ≤¥ Ïú†Ìòï")
            
        except Exception as e:
            print(f"‚ùå ÎπÑÎîîÏò§ {video.id} Í≥†Í∏â Î∂ÑÏÑù Ïã§Ìå®: {e}")
            import traceback
            print(f"üîç ÏÉÅÏÑ∏ Ïò§Î•ò:\n{traceback.format_exc()}")
            
            # Ïò§Î•ò ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
            try:
                video.analysis_status = 'failed'
                video.save()
                progress_tracker.finish_tracking(video.id, success=False)
            except Exception as save_error:
                print(f"‚ö†Ô∏è Ïò§Î•ò ÏÉÅÌÉú Ï†ÄÏû• Ïã§Ìå®: {save_error}")
            
            # ÌïÑÏöîÏãú Ï†ïÎ¶¨ ÏûëÏóÖ
            try:
                if 'json_filepath' in locals() and os.path.exists(json_filepath):
                    os.remove(json_filepath)
                    print(f"üóëÔ∏è Ïã§Ìå®Ìïú Î∂ÑÏÑù Í≤∞Í≥º ÌååÏùº ÏÇ≠Ï†ú: {json_filepath}")
            except:
                pass
    
    def _get_video_path(self, video):
        """ÎπÑÎîîÏò§ ÌååÏùº Í≤ΩÎ°ú Ï∞æÍ∏∞"""
        import os
        
        possible_paths = [
            os.path.join(settings.MEDIA_ROOT, 'videos', video.filename),
            os.path.join(settings.MEDIA_ROOT, 'uploads', video.filename),
            getattr(video, 'file_path', None)
        ]
        
        # None Ï†úÍ±∞
        possible_paths = [p for p in possible_paths if p is not None]
        
        for path in possible_paths:
            if os.path.exists(path):
                return path
        return None
    
    def _get_analysis_type_name(self, analysis_type):
        """Î∂ÑÏÑù ÌÉÄÏûÖ Ïù¥Î¶Ñ Î∞òÌôò"""
        type_names = {
            'basic': 'Í∏∞Î≥∏ Î∂ÑÏÑù',
            'enhanced': 'Ìñ•ÏÉÅÎêú Î∂ÑÏÑù',
            'comprehensive': 'Ï¢ÖÌï© Î∂ÑÏÑù',
            'custom': 'ÏÇ¨Ïö©Ïûê Ï†ïÏùò Î∂ÑÏÑù'
        }
        return type_names.get(analysis_type, 'Ìñ•ÏÉÅÎêú Î∂ÑÏÑù')
    
    def _get_estimated_time(self, analysis_type):
        """Î∂ÑÏÑù ÌÉÄÏûÖÎ≥Ñ ÏòàÏÉÅ ÏãúÍ∞Ñ"""
        time_estimates = {
            'basic': '2-5Î∂Ñ',
            'enhanced': '5-10Î∂Ñ', 
            'comprehensive': '10-20Î∂Ñ',
            'custom': 'ÏÉÅÌô©Ïóê Îî∞Îùº Îã§Î¶Ñ'
        }
        return time_estimates.get(analysis_type, '5-10Î∂Ñ')

# views.py - AnalysisCapabilitiesView ÌÅ¥ÎûòÏä§ ÏàòÏ†ï
from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
# views.py - EnhancedVideoChatView ÌÅ¥ÎûòÏä§ ÏàòÏ†ïÎêú Î≤ÑÏ†Ñ
# views.py - EnhancedVideoChatView ÌÅ¥ÎûòÏä§ ÏàòÏ†ïÎêú Î≤ÑÏ†Ñ

@method_decorator(csrf_exempt, name='dispatch')
class EnhancedVideoChatView(APIView):
    """Í≥†Í∏â Î∂ÑÏÑù Í≤∞Í≥ºÎ•º ÌôúÏö©Ìïú ÎπÑÎîîÏò§ Ï±ÑÌåÖ - ÏàòÏ†ïÎêú Î≤ÑÏ†Ñ"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        try:
            from .llm_client import LLMClient
            self.llm_client = LLMClient()
        except ImportError:
            self.llm_client = None
            logger.warning("LLMClientÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
        
        try:
            from .video_analyzer import get_video_analyzer
            self.video_analyzer = get_video_analyzer()
        except ImportError:
            self.video_analyzer = None
            logger.warning("VideoAnalyzerÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
    
    def post(self, request):
        try:
            user_message = request.data.get('message', '').strip()
            video_id = request.data.get('video_id')
            
            logger.info(f"üí¨ Í≥†Í∏â Ï±ÑÌåÖ ÏöîÏ≤≠: '{user_message}', ÎπÑÎîîÏò§ID: {video_id}")
            
            if not user_message:
                return Response({'response': 'Î©îÏãúÏßÄÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'})
            
            # ÌòÑÏû¨ ÎπÑÎîîÏò§ Í∞ÄÏ†∏Ïò§Í∏∞
            if video_id:
                try:
                    current_video = Video.objects.get(id=video_id)
                except Video.DoesNotExist:
                    current_video = Video.objects.filter(is_analyzed=True).first()
            else:
                current_video = Video.objects.filter(is_analyzed=True).first()
            
            if not current_video:
                return Response({
                    'response': 'Î∂ÑÏÑùÎêú ÎπÑÎîîÏò§Í∞Ä ÏóÜÏäµÎãàÎã§. ÎπÑÎîîÏò§Î•º ÏóÖÎ°úÎìúÌïòÍ≥† Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî.'
                })
            
            # Í≥†Í∏â ÎπÑÎîîÏò§ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
            video_info = self._get_enhanced_video_info(current_video)
            
            # ÏøºÎ¶¨ ÌÉÄÏûÖ Î∂ÑÏÑù Î∞è Ï≤òÎ¶¨
            if self._is_search_query(user_message):
                return self._handle_enhanced_search(user_message, current_video, video_info)
            elif self._is_analysis_query(user_message):
                return self._handle_analysis_insights(user_message, current_video, video_info)
            elif self._is_comparison_query(user_message):
                return self._handle_comparison_query(user_message, current_video, video_info)
            else:
                # ÏùºÎ∞ò ÎåÄÌôî
                if self.llm_client:
                    bot_response = self.llm_client.generate_smart_response(
                        user_query=user_message,
                        search_results=None,
                        video_info=video_info,
                        use_multi_llm=True
                    )
                else:
                    bot_response = f"'{user_message}'Ïóê ÎåÄÌïú Í∏∞Î≥∏ ÏùëÎãµÏûÖÎãàÎã§. ÎπÑÎîîÏò§: {current_video.original_name}"
                
                return Response({'response': bot_response})
                
        except Exception as e:
            logger.error(f"‚ùå Í≥†Í∏â Ï±ÑÌåÖ Ïò§Î•ò: {e}")
            return Response({
                'response': 'Í≥†Í∏â Î∂ÑÏÑù Í∏∞Îä•Ïóê ÏùºÏãúÏ†ÅÏù∏ Î¨∏Ï†úÍ∞Ä ÏûàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.'
            })
    
    def _handle_enhanced_search(self, message, video, video_info):
        """Í≥†Í∏â Î∂ÑÏÑù Í≤∞Í≥ºÎ•º ÌôúÏö©Ìïú Í≤ÄÏÉâ - ÏàòÏ†ïÎêú Î≤ÑÏ†Ñ"""
        try:
            logger.info(f"üîç Í≥†Í∏â Í≤ÄÏÉâ ÏãúÏûë: {message}")
            
            # ÏãúÍ∞Ñ Î≤îÏúÑ ÌååÏã±
            time_range = self._parse_time_range(message)
            
            # Í≤ÄÏÉâ ÌÉÄÏûÖ Í≤∞Ï†ï
            search_type = self._determine_search_type(message, time_range)
            logger.info(f"üìã Í≤∞Ï†ïÎêú Í≤ÄÏÉâ ÌÉÄÏûÖ: {search_type}")
            
            if search_type == 'time-analysis':
                # ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù Î∑∞ Ìò∏Ï∂ú
                time_view = TimeBasedAnalysisView()
                fake_request = type('FakeRequest', (), {
                    'data': {
                        'video_id': video.id,
                        'time_range': time_range,
                        'analysis_type': message
                    }
                })()
                fake_request.data = {
                    'video_id': video.id,
                    'time_range': time_range,
                    'analysis_type': message
                }
                
                result = time_view.post(fake_request)
                
                if hasattr(result, 'data') and result.data.get('result'):
                    analysis_result = result.data['result']
                    
                    # ÏùëÎãµ Ìè¨Îß∑ÌåÖ
                    if analysis_result.get('total_persons') is not None:
                        response_text = f"üìä {time_range['start']}~{time_range['end']} ÏãúÍ∞ÑÎåÄ Î∂ÑÏÑù Í≤∞Í≥º:\n\n"
                        response_text += f"üë• Ï¥ù Ïù∏Ïõê: {analysis_result['total_persons']}Î™Ö\n"
                        response_text += f"üë® ÎÇ®ÏÑ±: {analysis_result['male_count']}Î™Ö ({analysis_result['gender_ratio']['male']}%)\n"
                        response_text += f"üë© Ïó¨ÏÑ±: {analysis_result['female_count']}Î™Ö ({analysis_result['gender_ratio']['female']}%)\n\n"
                        
                        if analysis_result.get('clothing_colors'):
                            response_text += "üëï Ï£ºÏöî ÏùòÏÉÅ ÏÉâÏÉÅ:\n"
                            for color, count in list(analysis_result['clothing_colors'].items())[:3]:
                                response_text += f"   ‚Ä¢ {color}: {count}Î™Ö\n"
                        
                        if analysis_result.get('peak_times'):
                            response_text += f"\n‚è∞ ÌôúÎèô ÌîºÌÅ¨ ÏãúÍ∞Ñ: {', '.join(analysis_result['peak_times'])}"
                    else:
                        response_text = "ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑùÏùÑ ÏàòÌñâÌñàÏßÄÎßå Ï∂©Î∂ÑÌïú Îç∞Ïù¥ÌÑ∞Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
                else:
                    response_text = "ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."
            
            elif search_type == 'object-tracking':
                # Í∞ùÏ≤¥ Ï∂îÏ†Å Î∑∞ Ìò∏Ï∂ú
                tracking_view = IntraVideoTrackingView()
                fake_request = type('FakeRequest', (), {
                    'data': {
                        'video_id': video.id,
                        'tracking_target': message,
                        'time_range': time_range or {}
                    }
                })()
                fake_request.data = {
                    'video_id': video.id,
                    'tracking_target': message,
                    'time_range': time_range or {}
                }
                
                result = tracking_view.post(fake_request)
                
                if hasattr(result, 'data') and result.data.get('tracking_results'):
                    tracking_results = result.data['tracking_results']
                    
                    if tracking_results:
                        response_text = f"üéØ '{message}' Ï∂îÏ†Å Í≤∞Í≥º:\n\n"
                        response_text += f"üìç Ï¥ù {len(tracking_results)}Í∞ú Ïû•Î©¥ÏóêÏÑú Î∞úÍ≤¨\n\n"
                        
                        for i, result_item in enumerate(tracking_results[:5]):
                            time_str = self._seconds_to_time_string(result_item['timestamp'])
                            response_text += f"{i+1}. {time_str} - {result_item['description']} "
                            response_text += f"(Ïã†Î¢∞ÎèÑ: {result_item['confidence']*100:.1f}%)\n"
                        
                        if len(tracking_results) > 5:
                            response_text += f"\n... Ïô∏ {len(tracking_results)-5}Í∞ú Ïû•Î©¥ Îçî"
                    else:
                        response_text = f"üîç '{message}'Ïóê Ìï¥ÎãπÌïòÎäî Í∞ùÏ≤¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
                else:
                    response_text = "Í∞ùÏ≤¥ Ï∂îÏ†Å Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."
            
            else:
                # ÏùºÎ∞ò ÌîÑÎ†àÏûÑ Í≤ÄÏÉâ
                search_results = self._perform_frame_search(message, video)
                response_text = self._format_search_response(message, search_results)
                search_type = 'frame-search'
                searchResults = search_results
            
            return Response({
                'response': response_text,
                'search_results': searchResults or [],
                'search_type': search_type
            })
            
        except Exception as e:
            logger.error(f"‚ùå Í≥†Í∏â Í≤ÄÏÉâ Ïã§Ìå®: {e}")
            return Response({
                'response': f'Í≤ÄÏÉâ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}',
                'search_results': [],
                'error': str(e)
            })
    
    def _parse_time_range(self, message):
        """ÏãúÍ∞Ñ Î≤îÏúÑ ÌååÏã±"""
        import re
        
        # "3:00~5:00", "3:00-5:00" Îì±Ïùò Ìå®ÌÑ¥ Í∞êÏßÄ
        time_patterns = [
            r'(\d+):(\d+)\s*[-~]\s*(\d+):(\d+)',  # 3:00-5:00 ÌòïÌÉú
            r'(\d+)Î∂Ñ\s*[-~]\s*(\d+)Î∂Ñ',          # 3Î∂Ñ-5Î∂Ñ ÌòïÌÉú
        ]
        
        for pattern in time_patterns:
            match = re.search(pattern, message)
            if match:
                if ':' in pattern:
                    return {
                        'start': f"{match.group(1)}:{match.group(2)}",
                        'end': f"{match.group(3)}:{match.group(4)}"
                    }
                else:
                    return {
                        'start': f"{match.group(1)}:00",
                        'end': f"{match.group(2)}:00"
                    }
        
        return None
    
    def _determine_search_type(self, message, time_range):
        """Í≤ÄÏÉâ ÌÉÄÏûÖ Í≤∞Ï†ï"""
        message_lower = message.lower()
        
        # ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù ÌÇ§ÏõåÎìú
        time_analysis_keywords = ['ÏÑ±ÎπÑ', 'Î∂ÑÌè¨', 'ÌÜµÍ≥Ñ', 'ÎπÑÏú®', 'Î™áÎ™Ö', 'ÏñºÎßàÎÇò']
        
        # Í∞ùÏ≤¥ Ï∂îÏ†Å ÌÇ§ÏõåÎìú
        tracking_keywords = ['Ï∂îÏ†Å', 'ÏßÄÎÇòÍ∞Ñ', 'ÏÉÅÏùò', 'Î™®Ïûê', 'ÏÉâÍπî', 'Ïò∑', 'ÎÇ®ÏÑ±', 'Ïó¨ÏÑ±']
        
        if time_range and any(keyword in message_lower for keyword in time_analysis_keywords):
            return 'time-analysis'
        elif any(keyword in message_lower for keyword in tracking_keywords):
            return 'object-tracking'
        else:
            return 'frame-search'
    
    def _perform_frame_search(self, query, video):
        """ÌîÑÎ†àÏûÑ Í≤ÄÏÉâ ÏàòÌñâ"""
        try:
            frames = Frame.objects.filter(video=video)
            search_results = []
            
            query_lower = query.lower()
            
            # Í≤ÄÏÉâÏñ¥ÏóêÏÑú Í∞ùÏ≤¥ ÌÉÄÏûÖ Ï∂îÏ∂ú
            search_terms = self._extract_search_terms(query)
            
            for frame in frames:
                frame_matches = []
                confidence_scores = []
                
                # Í∞êÏßÄÎêú Í∞ùÏ≤¥ÏóêÏÑú Í≤ÄÏÉâ
                if hasattr(frame, 'detected_objects') and frame.detected_objects:
                    for obj in frame.detected_objects:
                        obj_class = obj.get('class', '').lower()
                        obj_confidence = obj.get('confidence', 0)
                        
                        # Í≤ÄÏÉâÏñ¥ Îß§Ïπ≠ ÌôïÏù∏
                        for term in search_terms:
                            if term in obj_class or obj_class in term:
                                frame_matches.append({
                                    'type': 'object',
                                    'match': obj_class,
                                    'confidence': obj_confidence,
                                    'bbox': obj.get('bbox', [])
                                })
                                confidence_scores.append(obj_confidence)
                
                # Ï∫°ÏÖòÏóêÏÑú Í≤ÄÏÉâ
                captions = [
                    getattr(frame, 'final_caption', '') or '',
                    getattr(frame, 'enhanced_caption', '') or '',
                    getattr(frame, 'caption', '') or ''
                ]
                
                for caption in captions:
                    if caption and query_lower in caption.lower():
                        frame_matches.append({
                            'type': 'caption',
                            'match': caption,
                            'confidence': 0.8
                        })
                        confidence_scores.append(0.8)
                        break
                
                # Îß§Ïπ≠Îêú ÌîÑÎ†àÏûÑÏù¥ ÏûàÏúºÎ©¥ Í≤∞Í≥ºÏóê Ï∂îÍ∞Ä
                if frame_matches:
                    avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
                    
                    search_results.append({
                        'frame_id': frame.image_id,
                        'timestamp': frame.timestamp,
                        'match_score': avg_confidence,
                        'matches': frame_matches,
                        'caption': captions[0] if captions[0] else 'No caption'
                    })
            
            # Ïã†Î¢∞ÎèÑ ÏàúÏúºÎ°ú Ï†ïÎ†¨
            search_results.sort(key=lambda x: x['match_score'], reverse=True)
            
            return search_results[:10]  # ÏÉÅÏúÑ 10Í∞ú Í≤∞Í≥º
            
        except Exception as e:
            logger.error(f"‚ùå ÌîÑÎ†àÏûÑ Í≤ÄÏÉâ Ïò§Î•ò: {e}")
            return []
    
    def _perform_frame_search(self, query, video):
        """ÌîÑÎ†àÏûÑ Í≤ÄÏÉâ ÏàòÌñâ"""
        try:
            from .models import Frame
            
            frames = Frame.objects.filter(video=video)
            search_results = []
            
            query_lower = query.lower()
            
            # Í≤ÄÏÉâÏñ¥ÏóêÏÑú Í∞ùÏ≤¥ ÌÉÄÏûÖ Ï∂îÏ∂ú
            search_terms = self._extract_search_terms(query)
            
            for frame in frames:
                frame_matches = []
                confidence_scores = []
                
                # Í∞êÏßÄÎêú Í∞ùÏ≤¥ÏóêÏÑú Í≤ÄÏÉâ
                if hasattr(frame, 'detected_objects') and frame.detected_objects:
                    for obj in frame.detected_objects:
                        obj_class = obj.get('class', '').lower()
                        obj_confidence = obj.get('confidence', 0)
                        
                        # Í≤ÄÏÉâÏñ¥ Îß§Ïπ≠ ÌôïÏù∏
                        for term in search_terms:
                            if term in obj_class or obj_class in term:
                                frame_matches.append({
                                    'type': 'object',
                                    'match': obj_class,
                                    'confidence': obj_confidence,
                                    'bbox': obj.get('bbox', [])
                                })
                                confidence_scores.append(obj_confidence)
                
                # Ï∫°ÏÖòÏóêÏÑú Í≤ÄÏÉâ
                captions = [
                    getattr(frame, 'final_caption', '') or '',
                    getattr(frame, 'enhanced_caption', '') or '',
                    getattr(frame, 'caption', '') or ''
                ]
                
                for caption in captions:
                    if caption and query_lower in caption.lower():
                        frame_matches.append({
                            'type': 'caption',
                            'match': caption,
                            'confidence': 0.8
                        })
                        confidence_scores.append(0.8)
                        break
                
                # Îß§Ïπ≠Îêú ÌîÑÎ†àÏûÑÏù¥ ÏûàÏúºÎ©¥ Í≤∞Í≥ºÏóê Ï∂îÍ∞Ä
                if frame_matches:
                    avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
                    
                    search_results.append({
                        'frame_id': frame.image_id,
                        'timestamp': frame.timestamp,
                        'match_score': avg_confidence,
                        'matches': frame_matches,
                        'caption': captions[0] if captions[0] else 'No caption'
                    })
            
            # Ïã†Î¢∞ÎèÑ ÏàúÏúºÎ°ú Ï†ïÎ†¨
            search_results.sort(key=lambda x: x['match_score'], reverse=True)
            
            return search_results[:10]  # ÏÉÅÏúÑ 10Í∞ú Í≤∞Í≥º
            
        except Exception as e:
            logger.error(f"‚ùå ÌîÑÎ†àÏûÑ Í≤ÄÏÉâ Ïò§Î•ò: {e}")
            return []
    
    def _extract_search_terms(self, message):
        """Í≤ÄÏÉâÏñ¥ÏóêÏÑú Í¥ÄÎ†® Ïö©Ïñ¥ Ï∂îÏ∂ú"""
        message_lower = message.lower()
        
        # ÌïúÍµ≠Ïñ¥-ÏòÅÏñ¥ Í∞ùÏ≤¥ Îß§Ìïë
        object_mapping = {
            'ÏÇ¨Îûå': 'person', 'Ï∞®': 'car', 'ÏûêÎèôÏ∞®': 'car',
            'ÏûêÏ†ÑÍ±∞': 'bicycle', 'Í∞ú': 'dog', 'Í≥†ÏñëÏù¥': 'cat'
        }
        
        search_terms = []
        
        # ÏßÅÏ†ë Îß§ÌïëÎêòÎäî Ïö©Ïñ¥Îì§ Ï∂îÍ∞Ä
        for korean, english in object_mapping.items():
            if korean in message_lower:
                search_terms.append(english)
                search_terms.append(korean)
        
        # Í∏∞Î≥∏ Í≤ÄÏÉâÏñ¥ Ï∂îÍ∞Ä
        words = message_lower.split()
        for word in words:
            if len(word) > 1:
                search_terms.append(word)
        
        return list(set(search_terms))  # Ï§ëÎ≥µ Ï†úÍ±∞
    
    def _format_search_response(self, query, search_results):
        """Í≤ÄÏÉâ Í≤∞Í≥º Ìè¨Îß∑ÌåÖ"""
        if not search_results:
            return f"'{query}' Í≤ÄÏÉâ Í≤∞Í≥ºÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
        
        response_text = f"'{query}' Í≤ÄÏÉâ Í≤∞Í≥º {len(search_results)}Í∞úÎ•º Ï∞æÏïòÏäµÎãàÎã§.\n\n"
        
        for i, result in enumerate(search_results[:3]):
            time_str = self._seconds_to_time_string(result['timestamp'])
            response_text += f"{i+1}. ÌîÑÎ†àÏûÑ #{result['frame_id']} ({time_str})\n"
            response_text += f"   {result['caption'][:100]}...\n\n"
        
        response_text += "üñºÔ∏è ÏïÑÎûòÏóêÏÑú Ïã§Ï†ú ÌîÑÎ†àÏûÑ Ïù¥ÎØ∏ÏßÄÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî!"
        
        return response_text
        """Í≤ÄÏÉâÏñ¥ÏóêÏÑú Í¥ÄÎ†® Ïö©Ïñ¥ Ï∂îÏ∂ú"""
        message_lower = message.lower()
        
        # ÌïúÍµ≠Ïñ¥-ÏòÅÏñ¥ Í∞ùÏ≤¥ Îß§Ìïë
        object_mapping = {
            'ÏÇ¨Îûå': 'person', 'Ï∞®': 'car', 'ÏûêÎèôÏ∞®': 'car',
            'ÏûêÏ†ÑÍ±∞': 'bicycle', 'Í∞ú': 'dog', 'Í≥†ÏñëÏù¥': 'cat'
        }
        
        search_terms = []
        
        # ÏßÅÏ†ë Îß§ÌïëÎêòÎäî Ïö©Ïñ¥Îì§ Ï∂îÍ∞Ä
        for korean, english in object_mapping.items():
            if korean in message_lower:
                search_terms.append(english)
                search_terms.append(korean)
        
        # Í∏∞Î≥∏ Í≤ÄÏÉâÏñ¥ Ï∂îÍ∞Ä
        words = message_lower.split()
        for word in words:
            if len(word) > 1:
                search_terms.append(word)
        
        return list(set(search_terms))  # Ï§ëÎ≥µ Ï†úÍ±∞
    
    def _format_search_response(self, query, search_results):
        """Í≤ÄÏÉâ Í≤∞Í≥º Ìè¨Îß∑ÌåÖ"""
        if not search_results:
            return f"'{query}' Í≤ÄÏÉâ Í≤∞Í≥ºÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
        
        response_text = f"'{query}' Í≤ÄÏÉâ Í≤∞Í≥º {len(search_results)}Í∞úÎ•º Ï∞æÏïòÏäµÎãàÎã§.\n\n"
        
        for i, result in enumerate(search_results[:3]):
            time_str = self._seconds_to_time_string(result['timestamp'])
            response_text += f"{i+1}. ÌîÑÎ†àÏûÑ #{result['frame_id']} ({time_str})\n"
            response_text += f"   {result['caption'][:100]}...\n\n"
        
        return response_text
    
    def _seconds_to_time_string(self, seconds):
        """Ï¥àÎ•º ÏãúÍ∞Ñ Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò"""
        if not seconds:
            return "0:00"
        
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes}:{secs:02d}"
    
    # Í∏∞Ï°¥ Î©îÏÑúÎìúÎì§ Ïú†ÏßÄ
    def _get_enhanced_video_info(self, video):
        """Í≥†Í∏â Î∂ÑÏÑù Ï†ïÎ≥¥Î•º Ìè¨Ìï®Ìïú ÎπÑÎîîÏò§ Ï†ïÎ≥¥"""
        info = f"üìπ ÎπÑÎîîÏò§: {video.original_name}\n"
        
        if hasattr(video, 'analysis'):
            analysis = video.analysis
            stats = analysis.analysis_statistics
            
            info += f"üî¨ Î∂ÑÏÑù ÌÉÄÏûÖ: {stats.get('analysis_type', 'enhanced')}\n"
            info += f"üìä Í∞êÏßÄÎêú Í∞ùÏ≤¥: {stats.get('unique_objects', 0)}Ï¢ÖÎ•ò\n"
            
            # Ïî¨ ÌÉÄÏûÖ Ï†ïÎ≥¥
            scene_types = stats.get('scene_types', [])
            if scene_types:
                info += f"üé¨ Í∞êÏßÄÎêú Ïî¨ ÌÉÄÏûÖ: {', '.join(scene_types[:3])}\n"
        
        return info
    
    def _is_search_query(self, message):
        search_keywords = ['Ï∞æÏïÑ', 'Í≤ÄÏÉâ', 'Ïñ¥Îîî', 'find', 'search', 'where', 'Î≥¥Ïó¨Ï§ò', 'Ï∂îÏ†Å', 'ÏßÄÎÇòÍ∞Ñ']
        return any(keyword in message for keyword in search_keywords)
    
    def _is_analysis_query(self, message):
        analysis_keywords = ['Î∂ÑÏÑù', 'analysis', 'Í≤∞Í≥º', 'ÌÜµÍ≥Ñ', 'Ïù∏ÏÇ¨Ïù¥Ìä∏', 'ÏöîÏïΩ', 'summary']
        return any(keyword in message.lower() for keyword in analysis_keywords)
    
    def _is_comparison_query(self, message):
        comparison_keywords = ['ÎπÑÍµê', 'compare', 'Ï∞®Ïù¥', 'difference', 'ÎåÄÎπÑ', 'vs']
        return any(keyword in message.lower() for keyword in comparison_keywords)
    
    def _handle_analysis_insights(self, message, video, video_info):
        """Î∂ÑÏÑù Ïù∏ÏÇ¨Ïù¥Ìä∏ Ï†úÍ≥µ"""
        return Response({
            'response': 'Î∂ÑÏÑù Ïù∏ÏÇ¨Ïù¥Ìä∏ Í∏∞Îä•ÏùÄ Í∞úÎ∞ú Ï§ëÏûÖÎãàÎã§.'
        })
    
    def _handle_comparison_query(self, message, video, video_info):
        """ÎπÑÍµê Î∂ÑÏÑù Ï≤òÎ¶¨"""
        return Response({
            'response': 'ÎπÑÍµê Î∂ÑÏÑù Í∏∞Îä•ÏùÄ Í∞úÎ∞ú Ï§ëÏûÖÎãàÎã§.'
        })
class VideoListView(APIView):
    """ÎπÑÎîîÏò§ Î™©Î°ù Ï°∞Ìöå - Í≥†Í∏â Î∂ÑÏÑù Ï†ïÎ≥¥ Ìè¨Ìï®"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            print("üîç VideoListView: ÎπÑÎîîÏò§ Î™©Î°ù ÏöîÏ≤≠ (Í≥†Í∏â Î∂ÑÏÑù Ï†ïÎ≥¥ Ìè¨Ìï®)")
            videos = Video.objects.all()
            video_list = []
            
            for video in videos:
                video_data = {
                    'id': video.id,
                    'filename': video.filename,
                    'original_name': video.original_name,
                    'duration': video.duration,
                    'is_analyzed': video.is_analyzed,
                    'analysis_status': video.analysis_status,
                    'uploaded_at': video.uploaded_at,
                    'file_size': video.file_size
                }
                
                # Í≥†Í∏â Î∂ÑÏÑù Ï†ïÎ≥¥ Ï∂îÍ∞Ä
                if hasattr(video, 'analysis'):
                    analysis = video.analysis
                    stats = analysis.analysis_statistics
                    
                    video_data.update({
                        'enhanced_analysis': analysis.enhanced_analysis,
                        'success_rate': analysis.success_rate,
                        'processing_time': analysis.processing_time_seconds,
                        'analysis_type': stats.get('analysis_type', 'basic'),
                        'advanced_features_used': {
                            'clip': stats.get('clip_analysis', False),
                            'ocr': stats.get('text_extracted', False),
                            'vqa': stats.get('vqa_analysis', False),
                            'scene_graph': stats.get('scene_graph_analysis', False)
                        },
                        'scene_types': stats.get('scene_types', []),
                        'unique_objects': stats.get('unique_objects', 0)
                    })
                
                # ÏßÑÌñâÎ•† Ï†ïÎ≥¥ Ï∂îÍ∞Ä (Î∂ÑÏÑù Ï§ëÏù∏ Í≤ΩÏö∞)
                if video.analysis_status == 'processing':
                    progress_info = progress_tracker.get_progress(video.id)
                    if progress_info:
                        video_data['progress_info'] = progress_info
                
                video_list.append(video_data)
            
            print(f"‚úÖ VideoListView: {len(video_list)}Í∞ú ÎπÑÎîîÏò§ Î∞òÌôò (Í≥†Í∏â Î∂ÑÏÑù Ï†ïÎ≥¥ Ìè¨Ìï®)")
            return Response({
                'videos': video_list,
                'total_count': len(video_list),
                'analysis_capabilities': self._get_system_capabilities()
            })
            
        except Exception as e:
            print(f"‚ùå VideoListView Ïò§Î•ò: {e}")
            return Response({
                'error': f'ÎπÑÎîîÏò§ Î™©Î°ù Ï°∞Ìöå Ïò§Î•ò: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _get_system_capabilities(self):
        """ÏãúÏä§ÌÖú Î∂ÑÏÑù Í∏∞Îä• ÏÉÅÌÉú"""
        try:
            # ‚úÖ ÏàòÏ†ï: Ï†ÑÏó≠ VideoAnalyzer Ïù∏Ïä§ÌÑ¥Ïä§ ÏÇ¨Ïö©
            analyzer = get_video_analyzer()
            return {
                'clip_available': analyzer.clip_available,
                'ocr_available': analyzer.ocr_available,
                'vqa_available': analyzer.vqa_available,
                'scene_graph_available': analyzer.scene_graph_available
            }
        except:
            return {
                'clip_available': False,
                'ocr_available': False,
                'vqa_available': False,
                'scene_graph_available': False
            }

class AnalysisStatusView(APIView):
    """Î∂ÑÏÑù ÏÉÅÌÉú ÌôïÏù∏ - ÏßÑÌñâÎ•† Ï†ïÎ≥¥ Ìè¨Ìï®"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            response_data = {
                'status': video.analysis_status,
                'video_filename': video.filename,
                'is_analyzed': video.is_analyzed
            }
            
            # ÏßÑÌñâÎ•† Ï†ïÎ≥¥ Ï∂îÍ∞Ä
            if video.analysis_status == 'processing':
                progress_info = progress_tracker.get_progress(video.id)
                response_data.update(progress_info)
            
            # Î∂ÑÏÑù ÏôÑÎ£åÎêú Í≤ΩÏö∞ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï∂îÍ∞Ä
            if hasattr(video, 'analysis'):
                analysis = video.analysis
                response_data.update({
                    'enhanced_analysis': analysis.enhanced_analysis,
                    'success_rate': analysis.success_rate,
                    'processing_time': analysis.processing_time_seconds,
                    'stats': {
                        'objects': analysis.analysis_statistics.get('unique_objects', 0),
                        'scenes': Scene.objects.filter(video=video).count(),
                        'captions': analysis.caption_statistics.get('frames_with_caption', 0)
                    }
                })
            
            return Response(response_data)
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class AnalyzeVideoView(APIView):
    """ÎπÑÎîîÏò§ Î∂ÑÏÑù ÏãúÏûë - ÏßÑÌñâÎ•† Ï∂îÏ†Å Ìè¨Ìï®"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            enable_enhanced = request.data.get('enable_enhanced_analysis', True)
            
            video = Video.objects.get(id=video_id)
            
            # Ïù¥ÎØ∏ Î∂ÑÏÑù Ï§ëÏù∏ÏßÄ ÌôïÏù∏
            if video.analysis_status == 'processing':
                return Response({
                    'error': 'Ïù¥ÎØ∏ Î∂ÑÏÑùÏù¥ ÏßÑÌñâ Ï§ëÏûÖÎãàÎã§'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # Î∂ÑÏÑù ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
            video.analysis_status = 'processing'
            video.save()
            
            # ÏßÑÌñâÎ•† Ï∂îÏ†Å ÏãúÏûë
            progress_tracker.start_tracking(video.id)
            
            # Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú Î∂ÑÏÑù ÏãúÏûë
            analysis_thread = threading.Thread(
                target=self._run_analysis,
                args=(video, enable_enhanced)
            )
            analysis_thread.daemon = True
            analysis_thread.start()
            
            return Response({
                'success': True,
                'message': 'ÎπÑÎîîÏò§ Î∂ÑÏÑùÏù¥ ÏãúÏûëÎêòÏóàÏäµÎãàÎã§.',
                'video_id': video.id,
                'enhanced_analysis': enable_enhanced
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'Î∂ÑÏÑù ÏãúÏûë Ïò§Î•ò: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _run_analysis(self, video, enable_enhanced):
        """Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú Ïã§ÌñâÎêòÎäî Î∂ÑÏÑù Ìï®Ïàò"""
        try:
            print(f"üî¨ ÎπÑÎîîÏò§ {video.id} Î∂ÑÏÑù ÏãúÏûë")
            
            # 1Îã®Í≥Ñ: ÎπÑÎîîÏò§ ÌååÏùº ÌôïÏù∏
            progress_tracker.update_progress(
                video.id, 
                step="ÎπÑÎîîÏò§ ÌååÏùº ÌôïÏù∏ Ï§ë", 
                progress=5
            )
            
            video_path = self._get_video_path(video)
            if not video_path:
                raise Exception("ÎπÑÎîîÏò§ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
            
            # 2Îã®Í≥Ñ: ÌîÑÎ†àÏûÑ Ï†ïÎ≥¥ Ï∂îÏ∂ú
            progress_tracker.update_progress(
                video.id, 
                step="ÎπÑÎîîÏò§ Ï†ïÎ≥¥ Î∂ÑÏÑù Ï§ë", 
                progress=10
            )
            
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps if fps > 0 else 0
            cap.release()
            
            # Ï¥ù ÌîÑÎ†àÏûÑ Ïàò ÏóÖÎç∞Ïù¥Ìä∏
            progress_tracker.update_progress(
                video.id,
                step=f"Ï¥ù {total_frames}Í∞ú ÌîÑÎ†àÏûÑ Î∂ÑÏÑù Ï§ÄÎπÑ",
                progress=15
            )
            progress_tracker.progress_data[video.id]['totalFrames'] = total_frames
            
            # 3Îã®Í≥Ñ: VideoAnalyzer Ï¥àÍ∏∞Ìôî
            progress_tracker.update_progress(
                video.id,
                step="AI Î™®Îç∏ Ï¥àÍ∏∞Ìôî Ï§ë",
                progress=20
            )
            
            # ‚úÖ ÏàòÏ†ï: Ï†ÑÏó≠ VideoAnalyzer Ïù∏Ïä§ÌÑ¥Ïä§ ÏÇ¨Ïö©
            analyzer = get_video_analyzer()
            
            # 4Îã®Í≥Ñ: ÌîÑÎ†àÏûÑÎ≥Ñ Î∂ÑÏÑù
            progress_tracker.update_progress(
                video.id,
                step="ÌîÑÎ†àÏûÑ Î∂ÑÏÑù ÏãúÏûë",
                progress=25
            )
            
            # Ïã§Ï†ú Î∂ÑÏÑù Î°úÏßÅ (Í∞ÑÏÜåÌôîÎêú ÏãúÎÆ¨Î†àÏù¥ÏÖò)
            for i in range(0, total_frames, max(1, total_frames // 50)):  # 50Í∞ú ÏÉòÌîå ÌîÑÎ†àÏûÑ
                if i > 0:
                    progress = 25 + (i / total_frames) * 60  # 25%~85%
                    progress_tracker.update_progress(
                        video.id,
                        step=f"ÌîÑÎ†àÏûÑ {i}/{total_frames} Î∂ÑÏÑù Ï§ë",
                        progress=progress,
                        processed_frames=i
                    )
                
                # Ïã§Ï†ú Î∂ÑÏÑù ÏûëÏóÖ (ÏãúÎÆ¨Î†àÏù¥ÏÖò)
                time.sleep(0.1)  # Ïã§Ï†úÎ°úÎäî AI Î∂ÑÏÑù ÏãúÍ∞Ñ
            
            # 5Îã®Í≥Ñ: Í≤∞Í≥º Ï†ÄÏû•
            progress_tracker.update_progress(
                video.id,
                step="Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû• Ï§ë",
                progress=90
            )
            
            # VideoAnalysis Í∞ùÏ≤¥ ÏÉùÏÑ± (Ïã§Ï†ú Íµ¨ÌòÑ ÌïÑÏöî)
            analysis = VideoAnalysis.objects.create(
                video=video,
                enhanced_analysis=enable_enhanced,
                success_rate=95.0,
                processing_time_seconds=int(time.time() - 
                    datetime.fromisoformat(progress_tracker.get_progress(video.id)['startTime']).timestamp()),
                analysis_statistics={'unique_objects': 15, 'total_detections': 150},
                caption_statistics={'frames_with_caption': total_frames}
            )
            
            # 6Îã®Í≥Ñ: ÏôÑÎ£å
            video.analysis_status = 'completed'
            video.is_analyzed = True
            video.save()
            
            progress_tracker.finish_tracking(video.id)
            
            print(f"‚úÖ ÎπÑÎîîÏò§ {video.id} Î∂ÑÏÑù ÏôÑÎ£å")
            
        except Exception as e:
            print(f"‚ùå ÎπÑÎîîÏò§ {video.id} Î∂ÑÏÑù Ïã§Ìå®: {e}")
            
            # Ïò§Î•ò ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
            video.analysis_status = 'failed'
            video.save()
            
            progress_tracker.update_progress(
                video.id,
                step=f"Î∂ÑÏÑù Ïã§Ìå®: {str(e)}",
                progress=0
            )
    
    def _get_video_path(self, video):
        """ÎπÑÎîîÏò§ ÌååÏùº Í≤ΩÎ°ú Ï∞æÍ∏∞"""
        possible_paths = [
            os.path.join(settings.VIDEO_FOLDER, video.filename),
            os.path.join(settings.UPLOAD_FOLDER, video.filename),
            video.file_path
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                return path
        return None


class AnalysisProgressView(APIView):
    """Î∂ÑÏÑù ÏßÑÌñâÎ•† Ï†ÑÏö© API"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            progress_info = progress_tracker.get_progress(video_id)
            
            if not progress_info:
                return Response({
                    'error': 'ÏßÑÌñâ Ï§ëÏù∏ Î∂ÑÏÑùÏù¥ ÏóÜÏäµÎãàÎã§'
                }, status=status.HTTP_404_NOT_FOUND)
            
            return Response(progress_info)
            
        except Exception as e:
            return Response({
                'error': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


# Í∏∞Ï°¥Ïùò Îã§Î•∏ View ÌÅ¥ÎûòÏä§Îì§ÏùÄ Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ
class VideoUploadView(APIView):
    """ÎπÑÎîîÏò§ ÏóÖÎ°úÎìú"""
    permission_classes = [AllowAny]
    parser_classes = (MultiPartParser, FormParser)
    
    def post(self, request):
        try:
            if 'video' not in request.FILES:
                return Response({
                    'error': 'ÎπÑÎîîÏò§ ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            video_file = request.FILES['video']
            
            if not video_file.name.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.webm')):
                return Response({
                    'error': 'ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÌååÏùº ÌòïÏãùÏûÖÎãàÎã§'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # Generate unique filename
            timestamp = int(time.time())
            filename = f"upload_{timestamp}_{video_file.name}"
            
            # Save file
            file_path = default_storage.save(
                f'uploads/{filename}',
                ContentFile(video_file.read())
            )
            
            # Create Video model instance
            video = Video.objects.create(
                filename=filename,
                original_name=video_file.name,
                file_path=file_path,
                file_size=video_file.size,
                analysis_status='pending'
            )
            
            return Response({
                'success': True,
                'video_id': video.id,
                'filename': filename,
                'message': f'ÎπÑÎîîÏò§ "{video_file.name}"Ïù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏóÖÎ°úÎìúÎêòÏóàÏäµÎãàÎã§.'
            })
            
        except Exception as e:
            return Response({
                'error': f'ÏóÖÎ°úÎìú Ïò§Î•ò: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class APIStatusView(APIView):
    """API ÏÉÅÌÉú ÌôïÏù∏"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        # print("üîç APIStatusView: API ÏÉÅÌÉú ÏöîÏ≤≠ Î∞õÏùå")
        try:
            llm_client = LLMClient()
            status_info = llm_client.get_api_status()
            
            response_data = {
                'groq': status_info.get('groq', {'available': False}),
                'openai': status_info.get('openai', {'available': False}),
                'anthropic': status_info.get('anthropic', {'available': False}),
                'fallback_enabled': True,
                'timestamp': datetime.now().isoformat(),
                'server_status': 'running',
                'active_analyses': len([k for k, v in progress_tracker.progress_data.items() 
                                     if v.get('progress', 0) < 100])
            }
            
            # print(f"‚úÖ APIStatusView: ÏÉÅÌÉú Ï†ïÎ≥¥ Î∞òÌôò - {response_data}")
            return Response(response_data)
        except Exception as e:
            print(f"‚ùå APIStatusView Ïò§Î•ò: {e}")
            return Response({
                'error': str(e),
                'server_status': 'error'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)



class VideoChatView(APIView):
    """ÎπÑÎîîÏò§ Í¥ÄÎ†® Ï±ÑÌåÖ API - Í∏∞Ï°¥ ChatViewÏôÄ Íµ¨Î∂Ñ"""
    permission_classes = [AllowAny]  # üîß Í∂åÌïú ÏÑ§Ï†ï Ï∂îÍ∞Ä
    
    def __init__(self):
        super().__init__()
        self.llm_client = LLMClient()
        self.video_analyzer = VideoAnalyzer()
    
    def post(self, request):
        try:
            user_message = request.data.get('message', '').strip()
            video_id = request.data.get('video_id')
            
            if not user_message:
                return Response({'response': 'Î©îÏãúÏßÄÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'})
            
            print(f"üí¨ ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄ: {user_message}")
            
            # Get current video
            if video_id:
                try:
                    current_video = Video.objects.get(id=video_id)
                except Video.DoesNotExist:
                    current_video = Video.objects.filter(is_analyzed=True).first()
            else:
                current_video = Video.objects.filter(is_analyzed=True).first()
            
            if not current_video:
                return Response({
                    'response': 'Î∂ÑÏÑùÎêú ÎπÑÎîîÏò§Í∞Ä ÏóÜÏäµÎãàÎã§. ÎπÑÎîîÏò§Î•º ÏóÖÎ°úÎìúÌïòÍ≥† Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî.'
                })
            
            # Get video info
            video_info = self._get_video_info(current_video)
            
            # Determine if multi-LLM should be used
            use_multi_llm = "compare" in user_message.lower() or "ÎπÑÍµê" in user_message or "Î∂ÑÏÑù" in user_message
            
            # Handle different query types
            if self._is_search_query(user_message):
                return self._handle_search_query(user_message, current_video, video_info, use_multi_llm)
            
            elif self._is_highlight_query(user_message):
                return self._handle_highlight_query(user_message, current_video, video_info, use_multi_llm)
            
            elif self._is_summary_query(user_message):
                return self._handle_summary_query(user_message, current_video, video_info, use_multi_llm)
            
            elif self._is_info_query(user_message):
                return self._handle_info_query(user_message, current_video, video_info, use_multi_llm)
            
            else:
                # General conversation
                bot_response = self.llm_client.generate_smart_response(
                    user_query=user_message,
                    search_results=None,
                    video_info=video_info,
                    use_multi_llm=use_multi_llm
                )
                return Response({'response': bot_response})
                
        except Exception as e:
            print(f"‚ùå Chat error: {e}")
            error_response = self.llm_client.generate_smart_response(
                user_query="ÏãúÏä§ÌÖú Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. ÎèÑÏõÄÏùÑ ÏöîÏ≤≠Ìï©ÎãàÎã§.",
                search_results=None,
                video_info=None
            )
            return Response({'response': error_response})
    
    # ... Í∏∞Ï°¥ Î©îÏÑúÎìúÎì§ Ïú†ÏßÄ


class FrameView(APIView):
    """ÌîÑÎ†àÏûÑ Ïù¥ÎØ∏ÏßÄ Ï†úÍ≥µ"""
    permission_classes = [AllowAny]  # üîß Í∂åÌïú ÏÑ§Ï†ï Ï∂îÍ∞Ä
    
    def get(self, request, video_id, frame_number, frame_type='normal'):
        try:
            video = Video.objects.get(id=video_id)
            
            # Get video file path
            video_path = None
            possible_paths = [
                os.path.join(settings.VIDEO_FOLDER, video.filename),
                os.path.join(settings.UPLOAD_FOLDER, video.filename),
                video.file_path
            ]
            
            for path in possible_paths:
                if os.path.exists(path):
                    video_path = path
                    break
            
            if not video_path:
                return Response({
                    'error': 'ÎπÑÎîîÏò§ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§'
                }, status=status.HTTP_404_NOT_FOUND)
            
            # Extract frame
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                return Response({
                    'error': 'ÎπÑÎîîÏò§ ÌååÏùºÏùÑ Ïó¥ Ïàò ÏóÜÏäµÎãàÎã§'
                }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            cap.set(cv2.CAP_PROP_POS_FRAMES, max(0, frame_number - 1))
            ret, frame = cap.read()
            cap.release()
            
            if not ret:
                return Response({
                    'error': 'ÌîÑÎ†àÏûÑÏùÑ Ï∂îÏ∂úÌï† Ïàò ÏóÜÏäµÎãàÎã§'
                }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # Handle annotated frames
            if frame_type == 'annotated':
                target_class = request.GET.get('class', '').lower()
                frame = self._annotate_frame(frame, video, frame_number, target_class)
            
            # Resize frame if too large
            height, width = frame.shape[:2]
            if width > 800:
                ratio = 800 / width
                new_width = 800
                new_height = int(height * ratio)
                frame = cv2.resize(frame, (new_width, new_height))
            
            # Save temporary image
            temp_filename = f'frame_{video.id}_{frame_number}_{int(time.time())}.jpg'
            temp_path = os.path.join(settings.IMAGE_FOLDER, temp_filename)
            
            cv2.imwrite(temp_path, frame, [cv2.IMWRITE_JPEG_QUALITY, 90])
            
            return FileResponse(
                open(temp_path, 'rb'),
                content_type='image/jpeg',
                filename=temp_filename
            )
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class ScenesView(APIView):
    """Scene Î™©Î°ù Ï°∞Ìöå"""
    permission_classes = [AllowAny]  # üîß Í∂åÌïú ÏÑ§Ï†ï Ï∂îÍ∞Ä
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            scenes = Scene.objects.filter(video=video).order_by('scene_id')
            
            scene_list = []
            for scene in scenes:
                scene_data = {
                    'scene_id': scene.scene_id,
                    'start_time': scene.start_time,
                    'end_time': scene.end_time,
                    'duration': scene.duration,
                    'frame_count': scene.frame_count,
                    'dominant_objects': scene.dominant_objects,
                    'caption_type': 'enhanced' if scene.enhanced_captions_count > 0 else 'basic'
                }
                scene_list.append(scene_data)
            
            return Response({
                'scenes': scene_list,
                'total_scenes': len(scene_list),
                'analysis_type': 'enhanced' if hasattr(video, 'analysis') and video.analysis.enhanced_analysis else 'basic'
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        



import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.permissions import AllowAny
from datetime import datetime, timedelta
from collections import Counter
import threading
import queue

from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient


class AnalysisFeaturesView(APIView):
    """Î∂ÑÏÑù Í∏∞Îä•Î≥Ñ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï†úÍ≥µ"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            analyzer = VideoAnalyzer()
            
            features = {
                'object_detection': {
                    'name': 'Í∞ùÏ≤¥ Í∞êÏßÄ',
                    'description': 'YOLO Í∏∞Î∞ò Ïã§ÏãúÍ∞Ñ Í∞ùÏ≤¥ Í∞êÏßÄ Î∞è Î∂ÑÎ•ò',
                    'available': True,
                    'processing_time_factor': 1.0,
                    'icon': 'üéØ',
                    'details': 'ÎπÑÎîîÏò§ ÎÇ¥ ÏÇ¨Îûå, Ï∞®Îüâ, ÎèôÎ¨º Îì± Îã§ÏñëÌïú Í∞ùÏ≤¥Î•º Ï†ïÌôïÌïòÍ≤å Í∞êÏßÄÌï©ÎãàÎã§.'
                },
                'clip_analysis': {
                    'name': 'CLIP Ïî¨ Î∂ÑÏÑù',
                    'description': 'OpenAI CLIP Î™®Îç∏ÏùÑ ÌôúÏö©Ìïú Í≥†Í∏â Ïî¨ Ïù¥Ìï¥',
                    'available': analyzer.clip_available,
                    'processing_time_factor': 1.5,
                    'icon': 'üñºÔ∏è',
                    'details': 'Ïù¥ÎØ∏ÏßÄÏùò ÏùòÎØ∏Ï†Å Ïª®ÌÖçÏä§Ìä∏Î•º Ïù¥Ìï¥ÌïòÏó¨ Ïî¨ Î∂ÑÎ•ò Î∞è Î∂ÑÏÑùÏùÑ ÏàòÌñâÌï©ÎãàÎã§.'
                },
                'ocr': {
                    'name': 'OCR ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú',
                    'description': 'EasyOCRÏùÑ ÏÇ¨Ïö©Ìïú Îã§Íµ≠Ïñ¥ ÌÖçÏä§Ìä∏ Ïù∏Ïãù',
                    'available': analyzer.ocr_available,
                    'processing_time_factor': 1.2,
                    'icon': 'üìù',
                    'details': 'ÎπÑÎîîÏò§ ÎÇ¥ ÌïúÍ∏Ä, ÏòÅÎ¨∏ ÌÖçÏä§Ìä∏Î•º Ï†ïÌôïÌïòÍ≤å Ïù∏ÏãùÌïòÍ≥† Ï∂îÏ∂úÌï©ÎãàÎã§.'
                },
                'vqa': {
                    'name': 'VQA ÏßàÎ¨∏ÎãµÎ≥Ä',
                    'description': 'BLIP Î™®Îç∏ Í∏∞Î∞ò ÏãúÍ∞ÅÏ†Å ÏßàÎ¨∏ ÎãµÎ≥Ä',
                    'available': analyzer.vqa_available,
                    'processing_time_factor': 2.0,
                    'icon': '‚ùì',
                    'details': 'Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌïú ÏßàÎ¨∏ÏùÑ ÏÉùÏÑ±ÌïòÍ≥† ÎãµÎ≥ÄÌïòÏó¨ ÍπäÏù¥ ÏûàÎäî Î∂ÑÏÑùÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.'
                },
                'scene_graph': {
                    'name': 'Scene Graph',
                    'description': 'Í∞ùÏ≤¥Í∞Ñ Í¥ÄÍ≥Ñ Î∞è ÏÉÅÌò∏ÏûëÏö© Î∂ÑÏÑù',
                    'available': analyzer.scene_graph_available,
                    'processing_time_factor': 3.0,
                    'icon': 'üï∏Ô∏è',
                    'details': 'Í∞ùÏ≤¥Îì§ ÏÇ¨Ïù¥Ïùò Í¥ÄÍ≥ÑÏôÄ ÏÉÅÌò∏ÏûëÏö©ÏùÑ Î∂ÑÏÑùÌïòÏó¨ Î≥µÏû°Ìïú Ïî¨ÏùÑ Ïù¥Ìï¥Ìï©ÎãàÎã§.'
                },
                'enhanced_caption': {
                    'name': 'Í≥†Í∏â Ï∫°ÏÖò ÏÉùÏÑ±',
                    'description': 'Î™®Îì† Î∂ÑÏÑù Í≤∞Í≥ºÎ•º ÌÜµÌï©Ìïú ÏÉÅÏÑ∏ Ï∫°ÏÖò',
                    'available': True,
                    'processing_time_factor': 1.1,
                    'icon': 'üí¨',
                    'details': 'Ïó¨Îü¨ AI Î™®Îç∏Ïùò Í≤∞Í≥ºÎ•º Ï¢ÖÌï©ÌïòÏó¨ ÏÉÅÏÑ∏ÌïòÍ≥† Ï†ïÌôïÌïú Ï∫°ÏÖòÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.'
                }
            }
            
            return Response({
                'features': features,
                'device': analyzer.device,
                'total_available': sum(1 for f in features.values() if f['available']),
                'recommended_configs': {
                    'basic': ['object_detection', 'enhanced_caption'],
                    'enhanced': ['object_detection', 'clip_analysis', 'ocr', 'enhanced_caption'],
                    'comprehensive': list(features.keys())
                }
            })
            
        except Exception as e:
            return Response({
                'error': f'Î∂ÑÏÑù Í∏∞Îä• Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class AdvancedVideoSearchView(APIView):
    """Í≥†Í∏â ÎπÑÎîîÏò§ Í≤ÄÏÉâ API"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        self.video_analyzer = VideoAnalyzer()
        self.llm_client = LLMClient()
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            query = request.data.get('query', '').strip()
            search_options = request.data.get('search_options', {})
            
            if not query:
                return Response({
                    'error': 'Í≤ÄÏÉâÏñ¥Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            video = Video.objects.get(id=video_id)
            
            # Í≥†Í∏â Í≤ÄÏÉâ ÏàòÌñâ
            search_results = self.video_analyzer.search_comprehensive(video, query)
            
            # Í≥†Í∏â Î∂ÑÏÑù Í≤∞Í≥ºÍ∞Ä Ìè¨Ìï®Îêú ÌîÑÎ†àÏûÑÎì§Ïóê ÎåÄÌï¥ Ï∂îÍ∞Ä Ï†ïÎ≥¥ ÏàòÏßë
            enhanced_results = []
            for result in search_results[:10]:
                frame_id = result.get('frame_id')
                try:
                    frame = Frame.objects.get(video=video, image_id=frame_id)
                    enhanced_result = dict(result)
                    
                    # Í≥†Í∏â Î∂ÑÏÑù Í≤∞Í≥º Ï∂îÍ∞Ä
                    comprehensive_features = frame.comprehensive_features or {}
                    
                    if search_options.get('include_clip_analysis') and 'clip_features' in comprehensive_features:
                        enhanced_result['clip_analysis'] = comprehensive_features['clip_features']
                    
                    if search_options.get('include_ocr_text') and 'ocr_text' in comprehensive_features:
                        enhanced_result['ocr_text'] = comprehensive_features['ocr_text']
                    
                    if search_options.get('include_vqa_results') and 'vqa_results' in comprehensive_features:
                        enhanced_result['vqa_insights'] = comprehensive_features['vqa_results']
                    
                    if search_options.get('include_scene_graph') and 'scene_graph' in comprehensive_features:
                        enhanced_result['scene_graph'] = comprehensive_features['scene_graph']
                    
                    enhanced_results.append(enhanced_result)
                    
                except Frame.DoesNotExist:
                    enhanced_results.append(result)
            
            # AI Í∏∞Î∞ò Í≤ÄÏÉâ Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ±
            search_insights = self._generate_search_insights(query, enhanced_results, video)
            
            return Response({
                'search_results': enhanced_results,
                'query': query,
                'insights': search_insights,
                'total_matches': len(search_results),
                'search_type': 'advanced',
                'video_info': {
                    'id': video.id,
                    'name': video.original_name,
                    'analysis_type': getattr(video, 'analysis_type', 'basic')
                }
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'Í≥†Í∏â Í≤ÄÏÉâ Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _generate_search_insights(self, query, results, video):
        """Í≤ÄÏÉâ Í≤∞Í≥ºÏóê ÎåÄÌïú AI Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ±"""
        try:
            if not results:
                return "Í≤ÄÏÉâ Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§. Îã§Î•∏ Í≤ÄÏÉâÏñ¥Î•º ÏãúÎèÑÌï¥Î≥¥ÏÑ∏Ïöî."
            
            # Í≤ÄÏÉâ Í≤∞Í≥º ÏöîÏïΩ
            insights_prompt = f"""
            Í≤ÄÏÉâÏñ¥: "{query}"
            ÎπÑÎîîÏò§: {video.original_name}
            Í≤ÄÏÉâ Í≤∞Í≥º: {len(results)}Í∞ú Îß§Ïπ≠
            
            Ï£ºÏöî Î∞úÍ≤¨ÏÇ¨Ìï≠:
            {json.dumps(results[:3], ensure_ascii=False, indent=2)}
            
            Ïù¥ Í≤ÄÏÉâ Í≤∞Í≥ºÏóê ÎåÄÌïú Í∞ÑÎã®ÌïòÍ≥† Ïú†Ïö©Ìïú Ïù∏ÏÇ¨Ïù¥Ìä∏Î•º ÌïúÍµ≠Ïñ¥Î°ú Ï†úÍ≥µÌï¥Ï£ºÏÑ∏Ïöî.
            """
            
            insights = self.llm_client.generate_smart_response(
                user_query=insights_prompt,
                search_results=results[:5],
                video_info=f"ÎπÑÎîîÏò§: {video.original_name}",
                use_multi_llm=False
            )
            
            return insights
            
        except Exception as e:
            return f"Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ± Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"


class EnhancedFrameView(APIView):
    """Í≥†Í∏â Î∂ÑÏÑù Ï†ïÎ≥¥Í∞Ä Ìè¨Ìï®Îêú ÌîÑÎ†àÏûÑ Îç∞Ïù¥ÌÑ∞ Ï†úÍ≥µ"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id, frame_number):
        try:
            video = Video.objects.get(id=video_id)
            
            # ÌîÑÎ†àÏûÑ Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå
            try:
                frame = Frame.objects.get(video=video, image_id=frame_number)
                
                frame_data = {
                    'frame_id': frame.image_id,
                    'timestamp': frame.timestamp,
                    'caption': frame.caption,
                    'enhanced_caption': frame.enhanced_caption,
                    'final_caption': frame.final_caption,
                    'detected_objects': frame.detected_objects,
                    'comprehensive_features': frame.comprehensive_features,
                    'analysis_quality': frame.comprehensive_features.get('caption_quality', 'basic')
                }
                
                # Í≥†Í∏â Î∂ÑÏÑù Í≤∞Í≥º Î∂ÑÌï¥
                if frame.comprehensive_features:
                    features = frame.comprehensive_features
                    
                    frame_data['advanced_analysis'] = {
                        'clip_analysis': features.get('clip_features', {}),
                        'ocr_text': features.get('ocr_text', {}),
                        'vqa_results': features.get('vqa_results', {}),
                        'scene_graph': features.get('scene_graph', {}),
                        'scene_complexity': features.get('scene_complexity', 0)
                    }
                
                return Response(frame_data)
                
            except Frame.DoesNotExist:
                # ÌîÑÎ†àÏûÑ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏúºÎ©¥ Í∏∞Î≥∏ Ïù¥ÎØ∏ÏßÄÎßå Î∞òÌôò
                return Response({
                    'frame_id': frame_number,
                    'message': 'ÌîÑÎ†àÏûÑ Îç∞Ïù¥ÌÑ∞Îäî ÏóÜÏßÄÎßå Ïù¥ÎØ∏ÏßÄÎäî ÏÇ¨Ïö© Í∞ÄÎä•Ìï©ÎãàÎã§.',
                    'image_url': f'/frame/{video_id}/{frame_number}/'
                })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'ÌîÑÎ†àÏûÑ Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class EnhancedScenesView(APIView):
    """Í≥†Í∏â Î∂ÑÏÑù Ï†ïÎ≥¥Í∞Ä Ìè¨Ìï®Îêú Ïî¨ Îç∞Ïù¥ÌÑ∞ Ï†úÍ≥µ"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            scenes = Scene.objects.filter(video=video).order_by('scene_id')
            
            enhanced_scenes = []
            for scene in scenes:
                scene_data = {
                    'scene_id': scene.scene_id,
                    'start_time': scene.start_time,
                    'end_time': scene.end_time,
                    'duration': scene.duration,
                    'frame_count': scene.frame_count,
                    'dominant_objects': scene.dominant_objects,
                    'enhanced_captions_count': scene.enhanced_captions_count,
                    'caption_type': 'enhanced' if scene.enhanced_captions_count > 0 else 'basic'
                }
                
                # Ïî¨ ÎÇ¥ ÌîÑÎ†àÏûÑÎì§Ïùò Í≥†Í∏â Î∂ÑÏÑù Í≤∞Í≥º ÏßëÍ≥Ñ
                scene_frames = Frame.objects.filter(
                    video=video,
                    timestamp__gte=scene.start_time,
                    timestamp__lte=scene.end_time
                )
                
                if scene_frames.exists():
                    # Í≥†Í∏â Í∏∞Îä• ÏÇ¨Ïö© ÌÜµÍ≥Ñ
                    clip_count = sum(1 for f in scene_frames if f.comprehensive_features.get('clip_features'))
                    ocr_count = sum(1 for f in scene_frames if f.comprehensive_features.get('ocr_text', {}).get('texts'))
                    vqa_count = sum(1 for f in scene_frames if f.comprehensive_features.get('vqa_results'))
                    
                    scene_data['advanced_features'] = {
                        'clip_analysis_frames': clip_count,
                        'ocr_text_frames': ocr_count,
                        'vqa_analysis_frames': vqa_count,
                        'total_frames': scene_frames.count()
                    }
                    
                    # Ïî¨ Î≥µÏû°ÎèÑ ÌèâÍ∑†
                    complexities = [f.comprehensive_features.get('scene_complexity', 0) for f in scene_frames]
                    scene_data['average_complexity'] = sum(complexities) / len(complexities) if complexities else 0
                
                enhanced_scenes.append(scene_data)
            
            return Response({
                'scenes': enhanced_scenes,
                'total_scenes': len(enhanced_scenes),
                'analysis_type': 'enhanced' if any(s.get('advanced_features') for s in enhanced_scenes) else 'basic',
                'video_info': {
                    'id': video.id,
                    'name': video.original_name
                }
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'Í≥†Í∏â Ïî¨ Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class AnalysisResultsView(APIView):
    """Ï¢ÖÌï© Î∂ÑÏÑù Í≤∞Í≥º Ï†úÍ≥µ"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            if not video.is_analyzed:
                return Response({
                    'error': 'ÏïÑÏßÅ Î∂ÑÏÑùÏù¥ ÏôÑÎ£åÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            analysis = video.analysis
            scenes = Scene.objects.filter(video=video)
            frames = Frame.objects.filter(video=video)
            
            # Ï¢ÖÌï© Î∂ÑÏÑù Í≤∞Í≥º
            results = {
                'video_info': {
                    'id': video.id,
                    'name': video.original_name,
                    'duration': video.duration,
                    'analysis_type': analysis.analysis_statistics.get('analysis_type', 'basic'),
                    'processing_time': analysis.processing_time_seconds,
                    'success_rate': analysis.success_rate
                },
                'analysis_summary': {
                    'total_scenes': scenes.count(),
                    'total_frames_analyzed': frames.count(),
                    'unique_objects': analysis.analysis_statistics.get('unique_objects', 0),
                    'features_used': analysis.analysis_statistics.get('features_used', []),
                    'scene_types': analysis.analysis_statistics.get('scene_types', [])
                },
                'advanced_features': {
                    'clip_analysis': analysis.analysis_statistics.get('clip_analysis', False),
                    'ocr_text_extracted': analysis.analysis_statistics.get('text_extracted', False),
                    'vqa_analysis': analysis.analysis_statistics.get('vqa_analysis', False),
                    'scene_graph_analysis': analysis.analysis_statistics.get('scene_graph_analysis', False)
                },
                'content_insights': {
                    'dominant_objects': analysis.analysis_statistics.get('dominant_objects', []),
                    'text_content_length': analysis.caption_statistics.get('text_content_length', 0),
                    'enhanced_captions_count': analysis.caption_statistics.get('enhanced_captions', 0),
                    'average_confidence': analysis.caption_statistics.get('average_confidence', 0)
                }
            }
            
            # ÌîÑÎ†àÏûÑÎ≥Ñ Í≥†Í∏â Î∂ÑÏÑù ÌÜµÍ≥Ñ
            if frames.exists():
                clip_frames = sum(1 for f in frames if f.comprehensive_features.get('clip_features'))
                ocr_frames = sum(1 for f in frames if f.comprehensive_features.get('ocr_text', {}).get('texts'))
                vqa_frames = sum(1 for f in frames if f.comprehensive_features.get('vqa_results'))
                
                results['frame_statistics'] = {
                    'total_frames': frames.count(),
                    'clip_analyzed_frames': clip_frames,
                    'ocr_processed_frames': ocr_frames,
                    'vqa_analyzed_frames': vqa_frames,
                    'coverage': {
                        'clip': (clip_frames / frames.count()) * 100 if frames.count() > 0 else 0,
                        'ocr': (ocr_frames / frames.count()) * 100 if frames.count() > 0 else 0,
                        'vqa': (vqa_frames / frames.count()) * 100 if frames.count() > 0 else 0
                    }
                }
            
            return Response(results)
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'Î∂ÑÏÑù Í≤∞Í≥º Ï°∞Ìöå Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class AnalysisSummaryView(APIView):
    """Î∂ÑÏÑù Í≤∞Í≥º ÏöîÏïΩ Ï†úÍ≥µ"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        self.llm_client = LLMClient()
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            if not video.is_analyzed:
                return Response({
                    'error': 'ÏïÑÏßÅ Î∂ÑÏÑùÏù¥ ÏôÑÎ£åÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # Î∂ÑÏÑù Í≤∞Í≥º Îç∞Ïù¥ÌÑ∞ ÏàòÏßë
            analysis = video.analysis
            frames = Frame.objects.filter(video=video)[:10]  # ÏÉÅÏúÑ 10Í∞ú ÌîÑÎ†àÏûÑ
            
            # AI Í∏∞Î∞ò ÏöîÏïΩ ÏÉùÏÑ±
            summary_data = {
                'video_name': video.original_name,
                'analysis_type': analysis.analysis_statistics.get('analysis_type', 'basic'),
                'features_used': analysis.analysis_statistics.get('features_used', []),
                'dominant_objects': analysis.analysis_statistics.get('dominant_objects', []),
                'scene_types': analysis.analysis_statistics.get('scene_types', []),
                'processing_time': analysis.processing_time_seconds
            }
            
            # ÎåÄÌëú ÌîÑÎ†àÏûÑÎì§Ïùò Ï∫°ÏÖò ÏàòÏßë
            sample_captions = []
            for frame in frames:
                if frame.final_caption:
                    sample_captions.append(frame.final_caption)
            
            summary_prompt = f"""
            Îã§Ïùå ÎπÑÎîîÏò§ Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú ÏÉÅÏÑ∏ÌïòÍ≥† Ïú†Ïö©Ìïú ÏöîÏïΩÏùÑ ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî:
            
            ÎπÑÎîîÏò§: {video.original_name}
            Î∂ÑÏÑù Ïú†Ìòï: {summary_data['analysis_type']}
            ÏÇ¨Ïö©Îêú Í∏∞Îä•: {', '.join(summary_data['features_used'])}
            Ï£ºÏöî Í∞ùÏ≤¥: {', '.join(summary_data['dominant_objects'][:5])}
            Ïî¨ Ïú†Ìòï: {', '.join(summary_data['scene_types'][:3])}
            
            ÎåÄÌëú Ï∫°ÏÖòÎì§:
            {chr(10).join(sample_captions[:5])}
            
            Ïù¥ ÎπÑÎîîÏò§Ïùò Ï£ºÏöî ÎÇ¥Ïö©, ÌäπÏßï, ÌôúÏö© Î∞©ÏïàÏùÑ Ìè¨Ìï®ÌïòÏó¨ ÌïúÍµ≠Ïñ¥Î°ú ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî.
            """
            
            ai_summary = self.llm_client.generate_smart_response(
                user_query=summary_prompt,
                search_results=None,
                video_info=f"ÎπÑÎîîÏò§: {video.original_name}",
                use_multi_llm=True  # Í≥†ÌíàÏßà ÏöîÏïΩÏùÑ ÏúÑÌï¥ Îã§Ï§ë LLM ÏÇ¨Ïö©
            )
            
            return Response({
                'video_id': video.id,
                'video_name': video.original_name,
                'ai_summary': ai_summary,
                'analysis_data': summary_data,
                'key_insights': {
                    'total_objects': len(summary_data['dominant_objects']),
                    'scene_variety': len(summary_data['scene_types']),
                    'analysis_depth': len(summary_data['features_used']),
                    'processing_efficiency': f"{summary_data['processing_time']}Ï¥à"
                },
                'generated_at': datetime.now().isoformat()
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'ÏöîÏïΩ ÏÉùÏÑ± Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class AnalysisExportView(APIView):
    """Î∂ÑÏÑù Í≤∞Í≥º ÎÇ¥Î≥¥ÎÇ¥Í∏∞"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            if not video.is_analyzed:
                return Response({
                    'error': 'ÏïÑÏßÅ Î∂ÑÏÑùÏù¥ ÏôÑÎ£åÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            export_format = request.GET.get('format', 'json')
            
            # Ï†ÑÏ≤¥ Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ ÏàòÏßë
            analysis = video.analysis
            scenes = Scene.objects.filter(video=video)
            frames = Frame.objects.filter(video=video)
            
            export_data = {
                'export_info': {
                    'video_id': video.id,
                    'video_name': video.original_name,
                    'export_date': datetime.now().isoformat(),
                    'export_format': export_format
                },
                'video_metadata': {
                    'filename': video.filename,
                    'duration': video.duration,
                    'file_size': video.file_size,
                    'uploaded_at': video.uploaded_at.isoformat()
                },
                'analysis_metadata': {
                    'analysis_type': analysis.analysis_statistics.get('analysis_type', 'basic'),
                    'enhanced_analysis': analysis.enhanced_analysis,
                    'success_rate': analysis.success_rate,
                    'processing_time_seconds': analysis.processing_time_seconds,
                    'features_used': analysis.analysis_statistics.get('features_used', [])
                },
                'scenes': [
                    {
                        'scene_id': scene.scene_id,
                        'start_time': scene.start_time,
                        'end_time': scene.end_time,
                        'duration': scene.duration,
                        'frame_count': scene.frame_count,
                        'dominant_objects': scene.dominant_objects
                    }
                    for scene in scenes
                ],
                'frames': [
                    {
                        'frame_id': frame.image_id,
                        'timestamp': frame.timestamp,
                        'caption': frame.caption,
                        'enhanced_caption': frame.enhanced_caption,
                        'final_caption': frame.final_caption,
                        'detected_objects': frame.detected_objects,
                        'comprehensive_features': frame.comprehensive_features
                    }
                    for frame in frames
                ],
                'statistics': {
                    'total_scenes': scenes.count(),
                    'total_frames': frames.count(),
                    'unique_objects': analysis.analysis_statistics.get('unique_objects', 0),
                    'scene_types': analysis.analysis_statistics.get('scene_types', []),
                    'dominant_objects': analysis.analysis_statistics.get('dominant_objects', [])
                }
            }
            
            if export_format == 'json':
                response = JsonResponse(export_data, json_dumps_params={'ensure_ascii': False, 'indent': 2})
                response['Content-Disposition'] = f'attachment; filename="{video.original_name}_analysis.json"'
                return response
            
            elif export_format == 'csv':
                # CSV ÌòïÌÉúÎ°ú ÌîÑÎ†àÏûÑ Îç∞Ïù¥ÌÑ∞ ÎÇ¥Î≥¥ÎÇ¥Í∏∞
                import csv
                from io import StringIO
                
                output = StringIO()
                writer = csv.writer(output)
                
                # Ìó§Îçî
                writer.writerow(['frame_id', 'timestamp', 'caption', 'enhanced_caption', 'objects_count', 'scene_complexity'])
                
                # Îç∞Ïù¥ÌÑ∞
                for frame_data in export_data['frames']:
                    writer.writerow([
                        frame_data['frame_id'],
                        frame_data['timestamp'],
                        frame_data.get('caption', ''),
                        frame_data.get('enhanced_caption', ''),
                        len(frame_data.get('detected_objects', [])),
                        frame_data.get('comprehensive_features', {}).get('scene_complexity', 0)
                    ])
                
                response = HttpResponse(output.getvalue(), content_type='text/csv')
                response['Content-Disposition'] = f'attachment; filename="{video.original_name}_analysis.csv"'
                return response
            
            else:
                return Response({
                    'error': 'ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÎÇ¥Î≥¥ÎÇ¥Í∏∞ ÌòïÏãùÏûÖÎãàÎã§. json ÎòêÎäî csvÎ•º ÏÇ¨Ïö©Ìï¥Ï£ºÏÑ∏Ïöî.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'ÎÇ¥Î≥¥ÎÇ¥Í∏∞ Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


# Í≤ÄÏÉâ Í¥ÄÎ†® Î∑∞Îì§
class ObjectSearchView(APIView):
    """Í∞ùÏ≤¥Î≥Ñ Í≤ÄÏÉâ"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            object_type = request.GET.get('object', '')
            video_id = request.GET.get('video_id')
            
            if not object_type:
                return Response({
                    'error': 'Í≤ÄÏÉâÌï† Í∞ùÏ≤¥ ÌÉÄÏûÖÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # ÌäπÏ†ï ÎπÑÎîîÏò§ ÎòêÎäî Ï†ÑÏ≤¥ ÎπÑÎîîÏò§ÏóêÏÑú Í≤ÄÏÉâ
            if video_id:
                videos = Video.objects.filter(id=video_id, is_analyzed=True)
            else:
                videos = Video.objects.filter(is_analyzed=True)
            
            results = []
            for video in videos:
                frames = Frame.objects.filter(video=video)
                
                for frame in frames:
                    for obj in frame.detected_objects:
                        if object_type.lower() in obj.get('class', '').lower():
                            results.append({
                                'video_id': video.id,
                                'video_name': video.original_name,
                                'frame_id': frame.image_id,
                                'timestamp': frame.timestamp,
                                'object_class': obj.get('class'),
                                'confidence': obj.get('confidence'),
                                'caption': frame.final_caption or frame.caption
                            })
            
            return Response({
                'search_query': object_type,
                'results': results[:50],  # ÏµúÎåÄ 50Í∞ú Í≤∞Í≥º
                'total_matches': len(results)
            })
            
        except Exception as e:
            return Response({
                'error': f'Í∞ùÏ≤¥ Í≤ÄÏÉâ Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class TextSearchView(APIView):
    """ÌÖçÏä§Ìä∏ Í≤ÄÏÉâ (OCR Í≤∞Í≥º Í∏∞Î∞ò)"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            search_text = request.GET.get('text', '')
            video_id = request.GET.get('video_id')
            
            if not search_text:
                return Response({
                    'error': 'Í≤ÄÏÉâÌï† ÌÖçÏä§Ìä∏Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # ÌäπÏ†ï ÎπÑÎîîÏò§ ÎòêÎäî Ï†ÑÏ≤¥ ÎπÑÎîîÏò§ÏóêÏÑú Í≤ÄÏÉâ
            if video_id:
                videos = Video.objects.filter(id=video_id, is_analyzed=True)
            else:
                videos = Video.objects.filter(is_analyzed=True)
            
            results = []
            for video in videos:
                frames = Frame.objects.filter(video=video)
                
                for frame in frames:
                    ocr_data = frame.comprehensive_features.get('ocr_text', {})
                    if 'full_text' in ocr_data and search_text.lower() in ocr_data['full_text'].lower():
                        results.append({
                            'video_id': video.id,
                            'video_name': video.original_name,
                            'frame_id': frame.image_id,
                            'timestamp': frame.timestamp,
                            'extracted_text': ocr_data['full_text'],
                            'text_details': ocr_data.get('texts', []),
                            'caption': frame.final_caption or frame.caption
                        })
            
            return Response({
                'search_query': search_text,
                'results': results[:50],
                'total_matches': len(results)
            })
            
        except Exception as e:
            return Response({
                'error': f'ÌÖçÏä§Ìä∏ Í≤ÄÏÉâ Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class SceneSearchView(APIView):
    """Ïî¨ ÌÉÄÏûÖÎ≥Ñ Í≤ÄÏÉâ"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            scene_type = request.GET.get('scene', '')
            video_id = request.GET.get('video_id')
            
            if not scene_type:
                return Response({
                    'error': 'Í≤ÄÏÉâÌï† Ïî¨ ÌÉÄÏûÖÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # ÌäπÏ†ï ÎπÑÎîîÏò§ ÎòêÎäî Ï†ÑÏ≤¥ ÎπÑÎîîÏò§ÏóêÏÑú Í≤ÄÏÉâ
            if video_id:
                videos = Video.objects.filter(id=video_id, is_analyzed=True)
            else:
                videos = Video.objects.filter(is_analyzed=True)
            
            results = []
            for video in videos:
                if hasattr(video, 'analysis'):
                    scene_types = video.analysis.analysis_statistics.get('scene_types', [])
                    if any(scene_type.lower() in st.lower() for st in scene_types):
                        results.append({
                            'video_id': video.id,
                            'video_name': video.original_name,
                            'scene_types': scene_types,
                            'analysis_type': video.analysis.analysis_statistics.get('analysis_type', 'basic'),
                            'dominant_objects': video.analysis.analysis_statistics.get('dominant_objects', [])
                        })
            
            return Response({
                'search_query': scene_type,
                'results': results,
                'total_matches': len(results)
            })
            
        except Exception as e:
            return Response({
                'error': f'Ïî¨ Í≤ÄÏÉâ Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.shortcuts import get_object_or_404
from django.db import transaction
import json
import logging
import os
import time

logger = logging.getLogger(__name__)

@csrf_exempt
@require_http_methods(["DELETE"])
def delete_video(request, video_id):
    """Í∞úÏÑ†Îêú ÎπÑÎîîÏò§ ÏÇ≠Ï†ú - ÏÉÅÏÑ∏ Î°úÍπÖ Î∞è Í≤ÄÏ¶ù Ìè¨Ìï®"""
    
    logger.info(f"üóëÔ∏è ÎπÑÎîîÏò§ ÏÇ≠Ï†ú ÏöîÏ≤≠ ÏãúÏûë: ID={video_id}")
    
    try:
        # 1Îã®Í≥Ñ: ÎπÑÎîîÏò§ Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
        try:
            video = get_object_or_404(Video, id=video_id)
            logger.info(f"‚úÖ ÎπÑÎîîÏò§ Ï∞æÏùå: {video.original_name} (ÌååÏùº: {video.file_path})")
        except Video.DoesNotExist:
            logger.warning(f"‚ùå ÎπÑÎîîÏò§ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå: ID={video_id}")
            return JsonResponse({
                'error': 'Ìï¥Îãπ ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.',
                'video_id': video_id,
                'deleted': False
            }, status=404)
        
        # 2Îã®Í≥Ñ: ÏÇ≠Ï†ú Í∞ÄÎä• Ïó¨Î∂Ä ÌôïÏù∏
        if video.analysis_status == 'processing':
            logger.warning(f"‚ùå Î∂ÑÏÑù Ï§ëÏù∏ ÎπÑÎîîÏò§ ÏÇ≠Ï†ú ÏãúÎèÑ: ID={video_id}")
            return JsonResponse({
                'error': 'Î∂ÑÏÑù Ï§ëÏù∏ ÎπÑÎîîÏò§Îäî ÏÇ≠Ï†úÌï† Ïàò ÏóÜÏäµÎãàÎã§.',
                'video_id': video_id,
                'status': video.analysis_status,
                'deleted': False
            }, status=400)
        
        # 3Îã®Í≥Ñ: Ìä∏ÎûúÏû≠ÏÖòÏúºÎ°ú ÏïàÏ†ÑÌïú ÏÇ≠Ï†ú Ï≤òÎ¶¨
        video_info = {
            'id': video_id,
            'name': video.original_name,
            'file_path': video.file_path,
            'has_analysis': hasattr(video, 'analysis_results') and video.analysis_results.exists(),
            'has_scenes': hasattr(video, 'scenes') and video.scenes.exists()
        }
        
        with transaction.atomic():
            logger.info(f"üîÑ Ìä∏ÎûúÏû≠ÏÖò ÏãúÏûë: ÎπÑÎîîÏò§ {video_id} ÏÇ≠Ï†ú")
            
            # Í¥ÄÎ†® Îç∞Ïù¥ÌÑ∞ Î®ºÏ†Ä ÏÇ≠Ï†ú
            deleted_analysis_count = 0
            deleted_scenes_count = 0
            
            if hasattr(video, 'analysis_results'):
                deleted_analysis_count = video.analysis_results.count()
                video.analysis_results.all().delete()
                logger.info(f"üìä Î∂ÑÏÑù Í≤∞Í≥º ÏÇ≠Ï†ú: {deleted_analysis_count}Í∞ú")
            
            if hasattr(video, 'scenes'):
                deleted_scenes_count = video.scenes.count()
                video.scenes.all().delete()
                logger.info(f"üé¨ Ïî¨ Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú: {deleted_scenes_count}Í∞ú")
            
            # ÌååÏùº ÏãúÏä§ÌÖúÏóêÏÑú ÌååÏùº ÏÇ≠Ï†ú
            file_deleted = False
            if video.file_path and os.path.exists(video.file_path):
                try:
                    os.remove(video.file_path)
                    file_deleted = True
                    logger.info(f"üìÅ ÌååÏùº ÏÇ≠Ï†ú ÏÑ±Í≥µ: {video.file_path}")
                except Exception as file_error:
                    logger.error(f"‚ùå ÌååÏùº ÏÇ≠Ï†ú Ïã§Ìå®: {video.file_path} - {str(file_error)}")
                    # ÌååÏùº ÏÇ≠Ï†ú Ïã§Ìå®Ìï¥ÎèÑ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑúÎäî ÏÇ≠Ï†ú ÏßÑÌñâ
                    file_deleted = False
            else:
                logger.info(f"üìÅ ÏÇ≠Ï†úÌï† ÌååÏùº ÏóÜÏùå: {video.file_path}")
                file_deleted = True  # ÌååÏùºÏù¥ ÏóÜÏúºÎ©¥ ÏÇ≠Ï†úÎêú Í≤ÉÏúºÎ°ú Í∞ÑÏ£º
            
            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú ÎπÑÎîîÏò§ Î†àÏΩîÎìú ÏÇ≠Ï†ú
            video.delete()
            logger.info(f"üíæ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú ÎπÑÎîîÏò§ ÏÇ≠Ï†ú ÏôÑÎ£å: ID={video_id}")
            
            # Ìä∏ÎûúÏû≠ÏÖò Ïª§Î∞ã ÌõÑ Ïû†Ïãú ÎåÄÍ∏∞ (Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÎèôÍ∏∞Ìôî)
            time.sleep(0.1)
        
        # 4Îã®Í≥Ñ: ÏÇ≠Ï†ú Í≤ÄÏ¶ù
        try:
            verification_video = Video.objects.get(id=video_id)
            # ÎπÑÎîîÏò§Í∞Ä Ïó¨Ï†ÑÌûà Ï°¥Ïû¨ÌïòÎ©¥ Ïò§Î•ò
            logger.error(f"‚ùå ÏÇ≠Ï†ú Í≤ÄÏ¶ù Ïã§Ìå®: ÎπÑÎîîÏò§Í∞Ä Ïó¨Ï†ÑÌûà Ï°¥Ïû¨Ìï® ID={video_id}")
            return JsonResponse({
                'error': 'ÎπÑÎîîÏò§ ÏÇ≠Ï†úÏóê Ïã§Ìå®ÌñàÏäµÎãàÎã§. Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú Ï†úÍ±∞ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.',
                'video_id': video_id,
                'deleted': False,
                'verification_failed': True
            }, status=500)
        except Video.DoesNotExist:
            # ÎπÑÎîîÏò§Í∞Ä Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏúºÎ©¥ ÏÇ≠Ï†ú ÏÑ±Í≥µ
            logger.info(f"‚úÖ ÏÇ≠Ï†ú Í≤ÄÏ¶ù ÏÑ±Í≥µ: ÎπÑÎîîÏò§Í∞Ä ÏôÑÏ†ÑÌûà Ï†úÍ±∞Îê® ID={video_id}")
        
        # 5Îã®Í≥Ñ: ÏÑ±Í≥µ ÏùëÎãµ
        response_data = {
            'success': True,
            'message': f'ÎπÑÎîîÏò§ "{video_info["name"]}"Ïù¥(Í∞Ä) ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏÇ≠Ï†úÎêòÏóàÏäµÎãàÎã§.',
            'video_id': video_id,
            'deleted': True,
            'details': {
                'file_deleted': file_deleted,
                'analysis_results_deleted': deleted_analysis_count,
                'scenes_deleted': deleted_scenes_count,
                'file_path': video_info['file_path']
            }
        }
        
        logger.info(f"‚úÖ ÎπÑÎîîÏò§ ÏÇ≠Ï†ú ÏôÑÎ£å: {json.dumps(response_data, ensure_ascii=False)}")
        return JsonResponse(response_data)
        
    except Exception as e:
        logger.error(f"‚ùå ÎπÑÎîîÏò§ ÏÇ≠Ï†ú Ï§ë ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•ò: ID={video_id}, Ïò§Î•ò={str(e)}")
        return JsonResponse({
            'error': f'ÎπÑÎîîÏò§ ÏÇ≠Ï†ú Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}',
            'video_id': video_id,
            'deleted': False,
            'exception': str(e)
        }, status=500)

@csrf_exempt
@require_http_methods(["GET"])  
def video_detail(request, video_id):
    """ÎπÑÎîîÏò§ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï°∞Ìöå (Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏Ïö©)"""
    try:
        video = get_object_or_404(Video, id=video_id)
        return JsonResponse({
            'id': video.id,
            'original_name': video.original_name,
            'analysis_status': video.analysis_status,
            'exists': True
        })
    except Video.DoesNotExist:
        return JsonResponse({
            'error': 'Ìï¥Îãπ ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.',
            'video_id': video_id,
            'exists': False
        }, status=404)

# ÏÇ≠Ï†ú ÏÉÅÌÉú ÌôïÏù∏ÏùÑ ÏúÑÌïú Î≥ÑÎèÑ ÏóîÎìúÌè¨Ïù∏Ìä∏
@csrf_exempt
@require_http_methods(["GET"])
def check_video_exists(request, video_id):
    """ÎπÑÎîîÏò§ Ï°¥Ïû¨ Ïó¨Î∂ÄÎßå ÌôïÏù∏"""
    try:
        Video.objects.get(id=video_id)
        return JsonResponse({
            'exists': True,
            'video_id': video_id
        })
    except Video.DoesNotExist:
        return JsonResponse({
            'exists': False,
            'video_id': video_id
        })

# views.pyÏóê Ï∂îÍ∞ÄÌï† Î∞îÏö¥Îî© Î∞ïÏä§ Í∑∏Î¶¨Í∏∞ View ÌÅ¥ÎûòÏä§Îì§

import cv2
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import io
import os
from django.http import HttpResponse
from rest_framework.views import APIView
from rest_framework.permissions import AllowAny

class FrameWithBboxView(APIView):
    """ÌîÑÎ†àÏûÑÏóê Î∞îÏö¥Îî© Î∞ïÏä§Î•º Í∑∏Î†§ÏÑú Î∞òÌôòÌïòÎäî View"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id, frame_number):
        try:
            print(f"üñºÔ∏è Î∞îÏö¥Îî© Î∞ïÏä§ ÌîÑÎ†àÏûÑ ÏöîÏ≤≠: ÎπÑÎîîÏò§={video_id}, ÌîÑÎ†àÏûÑ={frame_number}")
            
            # ÎπÑÎîîÏò§ Î∞è ÌîÑÎ†àÏûÑ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
            video = Video.objects.get(id=video_id)
            
            # ÌîÑÎ†àÏûÑ Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå
            try:
                frame_obj = Frame.objects.get(video=video, image_id=frame_number)
                detected_objects = frame_obj.detected_objects
            except Frame.DoesNotExist:
                # ÌîÑÎ†àÏûÑ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏúºÎ©¥ Îπà Î∞îÏö¥Îî© Î∞ïÏä§Î°ú ÏßÑÌñâ
                detected_objects = []
            
            # ÏõêÎ≥∏ ÌîÑÎ†àÏûÑ Ïù¥ÎØ∏ÏßÄ Ï∂îÏ∂ú
            video_path = self._get_video_path(video)
            if not video_path:
                return HttpResponse("ÎπÑÎîîÏò§ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§", status=404)
            
            # OpenCVÎ°ú ÌîÑÎ†àÏûÑ Ï∂îÏ∂ú
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                return HttpResponse("ÎπÑÎîîÏò§ ÌååÏùºÏùÑ Ïó¥ Ïàò ÏóÜÏäµÎãàÎã§", status=500)
            
            cap.set(cv2.CAP_PROP_POS_FRAMES, max(0, frame_number - 1))
            ret, frame = cap.read()
            cap.release()
            
            if not ret:
                return HttpResponse("ÌîÑÎ†àÏûÑÏùÑ Ï∂îÏ∂úÌï† Ïàò ÏóÜÏäµÎãàÎã§", status=500)
            
            # ÌïÑÌÑ∞ÎßÅ ÏòµÏÖò Ï≤òÎ¶¨
            target_classes = request.GET.getlist('class')  # ÌäπÏ†ï ÌÅ¥ÎûòÏä§Îßå ÌëúÏãú
            min_confidence = float(request.GET.get('confidence', 0.0))
            
            # Î∞îÏö¥Îî© Î∞ïÏä§ Í∑∏Î¶¨Í∏∞
            annotated_frame = self._draw_bounding_boxes(
                frame, 
                detected_objects, 
                target_classes=target_classes,
                min_confidence=min_confidence
            )
            
            # Ïù¥ÎØ∏ÏßÄÎ•º JPEGÎ°ú Ïù∏ÏΩîÎî©
            success, encoded_image = cv2.imencode('.jpg', annotated_frame)
            if not success:
                return HttpResponse("Ïù¥ÎØ∏ÏßÄ Ïù∏ÏΩîÎî© Ïã§Ìå®", status=500)
            
            # HTTP ÏùëÎãµÏúºÎ°ú Î∞òÌôò
            response = HttpResponse(encoded_image.tobytes(), content_type='image/jpeg')
            response['Content-Disposition'] = f'inline; filename="frame_{video_id}_{frame_number}_bbox.jpg"'
            
            print(f"‚úÖ Î∞îÏö¥Îî© Î∞ïÏä§ ÌîÑÎ†àÏûÑ ÏÉùÏÑ± ÏôÑÎ£å: {len(detected_objects)}Í∞ú Í∞ùÏ≤¥")
            return response
            
        except Video.DoesNotExist:
            return HttpResponse("ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§", status=404)
        except Exception as e:
            print(f"‚ùå Î∞îÏö¥Îî© Î∞ïÏä§ ÌîÑÎ†àÏûÑ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return HttpResponse(f"Ïò§Î•ò Î∞úÏÉù: {str(e)}", status=500)
    
    def _get_video_path(self, video):
        """ÎπÑÎîîÏò§ ÌååÏùº Í≤ΩÎ°ú Ï∞æÍ∏∞"""
        possible_paths = [
            os.path.join(settings.MEDIA_ROOT, 'videos', video.filename),
            os.path.join(settings.MEDIA_ROOT, 'uploads', video.filename),
            getattr(video, 'file_path', None)
        ]
        
        for path in possible_paths:
            if path and os.path.exists(path):
                return path
        return None
    
    def _draw_bounding_boxes(self, frame, detected_objects, target_classes=None, min_confidence=0.0):
        """ÌîÑÎ†àÏûÑÏóê Î∞îÏö¥Îî© Î∞ïÏä§ Í∑∏Î¶¨Í∏∞"""
        try:
            # ÌîÑÎ†àÏûÑ Î≥µÏÇ¨
            annotated_frame = frame.copy()
            h, w = frame.shape[:2]
            
            # ÏÉâÏÉÅ Îßµ Ï†ïÏùò (ÌÅ¥ÎûòÏä§Î≥Ñ Í≥†Ïú† ÏÉâÏÉÅ)
            color_map = {
                'person': (255, 100, 100),      # Îπ®Í∞ÑÏÉâ
                'car': (100, 255, 100),         # Ï¥àÎ°ùÏÉâ  
                'bicycle': (100, 100, 255),     # ÌååÎûÄÏÉâ
                'motorcycle': (255, 255, 100),  # ÎÖ∏ÎûÄÏÉâ
                'dog': (255, 100, 255),         # ÎßàÏ††ÌÉÄ
                'cat': (100, 255, 255),         # ÏÇ¨Ïù¥Ïïà
                'chair': (200, 150, 100),       # Í∞àÏÉâ
                'cup': (150, 100, 200),         # Î≥¥ÎùºÏÉâ
                'cell_phone': (255, 200, 100),  # Ï£ºÌô©ÏÉâ
                'laptop': (100, 200, 255),      # ÌïòÎäòÏÉâ
                'bottle': (200, 255, 150),      # Ïó∞ÎëêÏÉâ
                'book': (255, 150, 200),        # Î∂ÑÌôçÏÉâ
            }
            default_color = (255, 255, 255)  # Í∏∞Î≥∏ Ìù∞ÏÉâ
            
            drawn_count = 0
            
            for obj in detected_objects:
                obj_class = obj.get('class', '')
                confidence = obj.get('confidence', 0)
                bbox = obj.get('bbox', [])
                
                # ÌïÑÌÑ∞ÎßÅ Ï°∞Í±¥ ÌôïÏù∏
                if target_classes and obj_class not in target_classes:
                    continue
                if confidence < min_confidence:
                    continue
                if len(bbox) != 4:
                    continue
                
                # Ï†ïÍ∑úÌôîÎêú Ï¢åÌëúÎ•º Ïã§Ï†ú ÌîΩÏÖÄ Ï¢åÌëúÎ°ú Î≥ÄÌôò
                x1 = int(bbox[0] * w)
                y1 = int(bbox[1] * h)
                x2 = int(bbox[2] * w) 
                y2 = int(bbox[3] * h)
                
                # Ï¢åÌëú Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨
                x1 = max(0, min(w-1, x1))
                y1 = max(0, min(h-1, y1))
                x2 = max(0, min(w-1, x2))
                y2 = max(0, min(h-1, y2))
                
                if x2 <= x1 or y2 <= y1:
                    continue
                
                # ÏÉâÏÉÅ ÏÑ†ÌÉù
                color = color_map.get(obj_class, default_color)
                
                # Î∞îÏö¥Îî© Î∞ïÏä§ Í∑∏Î¶¨Í∏∞
                thickness = max(2, min(6, int(min(w, h) / 200)))  # Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Ïóê Îî∞Î•∏ ÎëêÍªò Ï°∞Ï†ï
                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, thickness)
                
                # Î∞òÌà¨Î™Ö Î∞∞Í≤Ω (ÏÑ†ÌÉùÏÇ¨Ìï≠)
                overlay = annotated_frame.copy()
                cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1)
                cv2.addWeighted(annotated_frame, 0.9, overlay, 0.1, 0, annotated_frame)
                
                # ÎùºÎ≤® ÌÖçÏä§Ìä∏ Ï§ÄÎπÑ
                label = f"{obj_class} {confidence:.2f}"
                font = cv2.FONT_HERSHEY_SIMPLEX
                font_scale = max(0.5, min(1.2, min(w, h) / 800))  # Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Ïóê Îî∞Î•∏ Ìè∞Ìä∏ ÌÅ¨Í∏∞
                text_thickness = max(1, int(thickness / 2))
                
                # ÌÖçÏä§Ìä∏ ÌÅ¨Í∏∞ Í≥ÑÏÇ∞
                (text_width, text_height), baseline = cv2.getTextSize(label, font, font_scale, text_thickness)
                
                # ÎùºÎ≤® Î∞∞Í≤Ω Í∑∏Î¶¨Í∏∞
                label_y = max(text_height + 10, y1)  # Î∞ïÏä§ ÏúÑÏ™ΩÏóê Î∞∞Ïπò, Í≥µÍ∞ÑÏù¥ ÏóÜÏúºÎ©¥ ÏïÑÎûòÏ™Ω
                if label_y == y1 and y1 < text_height + 10:
                    label_y = y2 + text_height + 5  # Î∞ïÏä§ ÏïÑÎûòÏ™ΩÏóê Î∞∞Ïπò
                
                cv2.rectangle(annotated_frame, 
                            (x1, label_y - text_height - 5), 
                            (x1 + text_width + 10, label_y + 5), 
                            color, -1)
                
                # ÎùºÎ≤® ÌÖçÏä§Ìä∏ Í∑∏Î¶¨Í∏∞
                cv2.putText(annotated_frame, label, 
                          (x1 + 5, label_y - 5), 
                          font, font_scale, (255, 255, 255), text_thickness)
                
                drawn_count += 1
            
            print(f"‚úÖ Î∞îÏö¥Îî© Î∞ïÏä§ Í∑∏Î¶¨Í∏∞ ÏôÑÎ£å: {drawn_count}Í∞ú Í∞ùÏ≤¥ ÌëúÏãú")
            return annotated_frame
            
        except Exception as e:
            print(f"‚ùå Î∞îÏö¥Îî© Î∞ïÏä§ Í∑∏Î¶¨Í∏∞ Ïò§Î•ò: {e}")
            return frame  # Ïò§Î•ò Ïãú ÏõêÎ≥∏ ÌîÑÎ†àÏûÑ Î∞òÌôò


class AdvancedVideoSearchView(APIView):
    """Í≥†Í∏â ÎπÑÎîîÏò§ Í≤ÄÏÉâ View - Î∞îÏö¥Îî© Î∞ïÏä§ Ï†ïÎ≥¥ Ìè¨Ìï®"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        self.video_analyzer = get_video_analyzer()
        self.llm_client = LLMClient()
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            query = request.data.get('query', '').strip()
            search_options = request.data.get('search_options', {})
            
            print(f"üîç Í≥†Í∏â ÎπÑÎîîÏò§ Í≤ÄÏÉâ: ÎπÑÎîîÏò§={video_id}, ÏøºÎ¶¨='{query}'")
            
            if not query:
                return Response({
                    'error': 'Í≤ÄÏÉâÏñ¥Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({
                    'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'
                }, status=status.HTTP_404_NOT_FOUND)
            
            # Í≥†Í∏â Í≤ÄÏÉâ ÏàòÌñâ
            search_results = self._perform_advanced_search(video, query, search_options)
            
            # Î∞îÏö¥Îî© Î∞ïÏä§ Ï†ïÎ≥¥ Ï∂îÍ∞Ä
            enhanced_results = self._add_bbox_info(search_results, video)
            
            # AI Í∏∞Î∞ò Í≤ÄÏÉâ Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ±
            search_insights = self._generate_search_insights(query, enhanced_results, video)
            
            print(f"‚úÖ Í≥†Í∏â Í≤ÄÏÉâ ÏôÑÎ£å: {len(enhanced_results)}Í∞ú Í≤∞Í≥º")
            
            return Response({
                'search_results': enhanced_results,
                'query': query,
                'insights': search_insights,
                'total_matches': len(search_results),
                'search_type': 'advanced_search',
                'has_bbox_annotations': any(r.get('bbox_annotations') for r in enhanced_results),
                'video_info': {
                    'id': video.id,
                    'name': video.original_name,
                    'analysis_type': getattr(video, 'analysis_type', 'basic')
                }
            })
            
        except Exception as e:
            print(f"‚ùå Í≥†Í∏â ÎπÑÎîîÏò§ Í≤ÄÏÉâ Ïã§Ìå®: {e}")
            return Response({
                'error': f'Í≥†Í∏â Í≤ÄÏÉâ Ïã§Ìå®: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _perform_advanced_search(self, video, query, search_options):
        """Ïã§Ï†ú Í≥†Í∏â Í≤ÄÏÉâ ÏàòÌñâ"""
        try:
            # EnhancedVideoChatViewÏùò Í≤ÄÏÉâ Î°úÏßÅ Ïû¨ÏÇ¨Ïö©
            chat_view = EnhancedVideoChatView()
            video_info = chat_view._get_enhanced_video_info(video)
            
            # Í≤ÄÏÉâ ÏàòÌñâ
            response = chat_view._handle_enhanced_search(query, video, video_info)
            
            if hasattr(response, 'data') and 'search_results' in response.data:
                return response.data['search_results']
            else:
                return []
                
        except Exception as e:
            print(f"‚ùå Í≥†Í∏â Í≤ÄÏÉâ ÏàòÌñâ Ïò§Î•ò: {e}")
            return []
    
    def _add_bbox_info(self, search_results, video):
        """Í≤ÄÏÉâ Í≤∞Í≥ºÏóê Î∞îÏö¥Îî© Î∞ïÏä§ Ï†ïÎ≥¥ Ï∂îÍ∞Ä"""
        enhanced_results = []
        
        for result in search_results:
            enhanced_result = dict(result)
            
            # Î∞îÏö¥Îî© Î∞ïÏä§ Ïñ¥ÎÖ∏ÌÖåÏù¥ÏÖò Ï†ïÎ≥¥ ÌôïÏù∏ Î∞è Ï∂îÍ∞Ä
            if 'matches' in result:
                bbox_annotations = []
                for match in result['matches']:
                    if match.get('type') == 'object' and 'bbox' in match:
                        bbox_annotations.append({
                            'match': match['match'],
                            'confidence': match['confidence'],
                            'bbox': match['bbox'],
                            'colors': match.get('colors', []),
                            'color_description': match.get('color_description', '')
                        })
                
                enhanced_result['bbox_annotations'] = bbox_annotations
                
                # Î∞îÏö¥Îî© Î∞ïÏä§ Ïù¥ÎØ∏ÏßÄ URL Ï∂îÍ∞Ä
                if bbox_annotations:
                    bbox_url = f"/frame/{video.id}/{result['frame_id']}/bbox/"
                    enhanced_result['bbox_image_url'] = bbox_url
            
            enhanced_results.append(enhanced_result)
        
        return enhanced_results
    
    def _generate_search_insights(self, query, results, video):
        """Í≤ÄÏÉâ Í≤∞Í≥ºÏóê ÎåÄÌïú AI Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ±"""
        try:
            if not results:
                return "Í≤ÄÏÉâ Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§. Îã§Î•∏ Í≤ÄÏÉâÏñ¥Î•º ÏãúÎèÑÌï¥Î≥¥ÏÑ∏Ïöî."
            
            bbox_count = sum(1 for r in results if r.get('bbox_annotations'))
            total_objects = sum(len(r.get('bbox_annotations', [])) for r in results)
            
            insights_prompt = f"""
            Í≤ÄÏÉâÏñ¥: "{query}"
            ÎπÑÎîîÏò§: {video.original_name}
            Í≤ÄÏÉâ Í≤∞Í≥º: {len(results)}Í∞ú Îß§Ïπ≠
            Î∞îÏö¥Îî© Î∞ïÏä§ ÌëúÏãú Í∞ÄÎä•: {bbox_count}Í∞ú ÌîÑÎ†àÏûÑ
            Ï¥ù Í∞êÏßÄÎêú Í∞ùÏ≤¥: {total_objects}Í∞ú
            
            Ï£ºÏöî Î∞úÍ≤¨ÏÇ¨Ìï≠ÏùÑ Î∞îÌÉïÏúºÎ°ú Í∞ÑÎã®ÌïòÍ≥† Ïú†Ïö©Ìïú Ïù∏ÏÇ¨Ïù¥Ìä∏Î•º ÌïúÍµ≠Ïñ¥Î°ú Ï†úÍ≥µÌï¥Ï£ºÏÑ∏Ïöî.
            Î∞îÏö¥Îî© Î∞ïÏä§ ÌëúÏãú Í∏∞Îä•Ïóê ÎåÄÌïú ÏïàÎÇ¥ÎèÑ Ìè¨Ìï®Ìï¥Ï£ºÏÑ∏Ïöî.
            """
            
            insights = self.llm_client.generate_smart_response(
                user_query=insights_prompt,
                search_results=results[:3],
                video_info=f"ÎπÑÎîîÏò§: {video.original_name}",
                use_multi_llm=False
            )
            
            return insights
            
        except Exception as e:
            return f"Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ± Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"


# Í∏∞Ï°¥ FrameView ÌÅ¥ÎûòÏä§Ïóê Î∞îÏö¥Îî© Î∞ïÏä§ ÏòµÏÖò Ï∂îÍ∞Ä
class EnhancedFrameView(FrameView):
    """Í∏∞Ï°¥ FrameViewÎ•º ÌôïÏû•Ìïú Í≥†Í∏â ÌîÑÎ†àÏûÑ View"""
    
    def get(self, request, video_id, frame_number):
        try:
            # Î∞îÏö¥Îî© Î∞ïÏä§ ÌëúÏãú ÏòµÏÖò ÌôïÏù∏
            show_bbox = request.GET.get('bbox', '').lower() in ['true', '1', 'yes']
            
            if show_bbox:
                # Î∞îÏö¥Îî© Î∞ïÏä§Í∞Ä Ìè¨Ìï®Îêú Ïù¥ÎØ∏ÏßÄ Î∞òÌôò
                bbox_view = FrameWithBboxView()
                return bbox_view.get(request, video_id, frame_number)
            else:
                # Í∏∞Î≥∏ ÌîÑÎ†àÏûÑ Î∞òÌôò
                return super().get(request, video_id, frame_number)
                
        except Exception as e:
            print(f"‚ùå Í≥†Í∏â ÌîÑÎ†àÏûÑ Î∑∞ Ïò§Î•ò: {e}")
            return super().get(request, video_id, frame_number)

# chat/views.pyÏóê Îã§Ïùå ÌÅ¥ÎûòÏä§Î•º Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî

class AnalysisCapabilitiesView(APIView):
    """ÏãúÏä§ÌÖú Î∂ÑÏÑù Í∏∞Îä• ÏÉÅÌÉú ÌôïÏù∏"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            print("üîç AnalysisCapabilitiesView: Î∂ÑÏÑù Í∏∞Îä• ÏÉÅÌÉú ÏöîÏ≤≠")
            
            # VideoAnalyzer Ïù∏Ïä§ÌÑ¥Ïä§ Í∞ÄÏ†∏Ïò§Í∏∞
            try:
                analyzer = get_video_analyzer()
                analyzer_available = True
                print("‚úÖ VideoAnalyzer Ïù∏Ïä§ÌÑ¥Ïä§ Î°úÎî© ÏÑ±Í≥µ")
            except Exception as e:
                print(f"‚ö†Ô∏è VideoAnalyzer Î°úÎî© Ïã§Ìå®: {e}")
                analyzer = None
                analyzer_available = False
            
            # ÏãúÏä§ÌÖú Í∏∞Îä• ÏÉÅÌÉú ÌôïÏù∏
            capabilities = {
                'system_status': {
                    'analyzer_available': analyzer_available,
                    'device': getattr(analyzer, 'device', 'unknown') if analyzer else 'none',
                    'timestamp': datetime.now().isoformat()
                },
                'core_features': {
                    'object_detection': {
                        'name': 'Í∞ùÏ≤¥ Í∞êÏßÄ',
                        'available': analyzer.model is not None if analyzer else False,
                        'description': 'YOLO Í∏∞Î∞ò Ïã§ÏãúÍ∞Ñ Í∞ùÏ≤¥ Í∞êÏßÄ',
                        'icon': 'üéØ'
                    },
                    'enhanced_captions': {
                        'name': 'Í≥†Í∏â Ï∫°ÏÖò ÏÉùÏÑ±',
                        'available': True,
                        'description': 'AI Í∏∞Î∞ò ÏÉÅÏÑ∏ Ï∫°ÏÖò ÏÉùÏÑ±',
                        'icon': 'üí¨'
                    }
                },
                'advanced_features': {
                    'clip_analysis': {
                        'name': 'CLIP Î∂ÑÏÑù',
                        'available': getattr(analyzer, 'clip_available', False) if analyzer else False,
                        'description': 'OpenAI CLIP Î™®Îç∏ Í∏∞Î∞ò Ïî¨ Ïù¥Ìï¥',
                        'icon': 'üñºÔ∏è'
                    },
                    'ocr_text_extraction': {
                        'name': 'OCR ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú',
                        'available': getattr(analyzer, 'ocr_available', False) if analyzer else False,  
                        'description': 'EasyOCR Í∏∞Î∞ò Îã§Íµ≠Ïñ¥ ÌÖçÏä§Ìä∏ Ïù∏Ïãù',
                        'icon': 'üìù'
                    },
                    'vqa_analysis': {
                        'name': 'VQA ÏßàÎ¨∏ÎãµÎ≥Ä',
                        'available': getattr(analyzer, 'vqa_available', False) if analyzer else False,
                        'description': 'BLIP Î™®Îç∏ Í∏∞Î∞ò ÏãúÍ∞ÅÏ†Å ÏßàÎ¨∏ ÎãµÎ≥Ä',
                        'icon': '‚ùì'
                    },
                    'scene_graph': {
                        'name': 'Scene Graph',
                        'available': getattr(analyzer, 'scene_graph_available', False) if analyzer else False,
                        'description': 'NetworkX Í∏∞Î∞ò Í∞ùÏ≤¥ Í¥ÄÍ≥Ñ Î∂ÑÏÑù',
                        'icon': 'üï∏Ô∏è'
                    }
                },
                'api_status': {
                    'groq_available': True,  # LLMClientÏóêÏÑú ÌôïÏù∏ ÌïÑÏöî
                    'openai_available': True,
                    'anthropic_available': True
                }
            }
            
            # ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Í∏∞Îä• Ïàò Í≥ÑÏÇ∞
            total_features = len(capabilities['core_features']) + len(capabilities['advanced_features'])
            available_features = sum(1 for features in [capabilities['core_features'], capabilities['advanced_features']] 
                                   for feature in features.values() if feature.get('available', False))
            
            capabilities['summary'] = {
                'total_features': total_features,
                'available_features': available_features,
                'availability_rate': (available_features / total_features * 100) if total_features > 0 else 0,
                'system_ready': analyzer_available and available_features > 0
            }
            
            print(f"‚úÖ Î∂ÑÏÑù Í∏∞Îä• ÏÉÅÌÉú: {available_features}/{total_features} ÏÇ¨Ïö© Í∞ÄÎä•")
            
            return Response(capabilities)
            
        except Exception as e:
            print(f"‚ùå AnalysisCapabilitiesView Ïò§Î•ò: {e}")
            import traceback
            print(f"üîç ÏÉÅÏÑ∏ Ïò§Î•ò: {traceback.format_exc()}")
            
            # Ïò§Î•ò Î∞úÏÉùÏãú Í∏∞Î≥∏ ÏÉÅÌÉú Î∞òÌôò
            error_response = {
                'system_status': {
                    'analyzer_available': False,
                    'device': 'error',
                    'error': str(e)
                },
                'core_features': {},
                'advanced_features': {},
                'api_status': {},
                'summary': {
                    'total_features': 0,
                    'available_features': 0,
                    'availability_rate': 0,
                    'system_ready': False,
                    'error': str(e)
                }
            }
            
            return Response(error_response, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# views.pyÏóê Ï∂îÍ∞ÄÌï† Í≥†Í∏â Í≤ÄÏÉâ API ÌÅ¥ÎûòÏä§Îì§

class CrossVideoSearchView(APIView):
    """ÏòÅÏÉÅ Í∞Ñ Í≤ÄÏÉâ - Ïó¨Îü¨ ÎπÑÎîîÏò§ÏóêÏÑú Ï°∞Í±¥ Í≤ÄÏÉâ"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            query = request.data.get('query', '').strip()
            search_filters = request.data.get('filters', {})
            
            if not query:
                return Response({'error': 'Í≤ÄÏÉâÏñ¥Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'}, status=400)
            
            # ÏøºÎ¶¨ Î∂ÑÏÑù - ÎÇ†Ïî®, ÏãúÍ∞ÑÎåÄ, Ïû•ÏÜå Îì± Ï∂îÏ∂ú
            query_analysis = self._analyze_query(query)
            
            # Î∂ÑÏÑùÎêú ÎπÑÎîîÏò§Îì§ Ï§ëÏóêÏÑú Í≤ÄÏÉâ
            videos = Video.objects.filter(is_analyzed=True)
            matching_videos = []
            
            for video in videos:
                match_score = self._calculate_video_match_score(video, query_analysis, search_filters)
                if match_score > 0.3:  # ÏûÑÍ≥ÑÍ∞í
                    matching_videos.append({
                        'video_id': video.id,
                        'video_name': video.original_name,
                        'match_score': match_score,
                        'match_reasons': self._get_match_reasons(video, query_analysis),
                        'metadata': self._get_video_metadata(video),
                        'thumbnail_url': f'/api/frame/{video.id}/100/',
                    })
            
            # Ï†êÏàòÏàú Ï†ïÎ†¨
            matching_videos.sort(key=lambda x: x['match_score'], reverse=True)
            
            return Response({
                'query': query,
                'total_matches': len(matching_videos),
                'results': matching_videos[:20],  # ÏÉÅÏúÑ 20Í∞ú
                'query_analysis': query_analysis,
                'search_type': 'cross_video'
            })
            
        except Exception as e:
            return Response({'error': str(e)}, status=500)
    
    def _analyze_query(self, query):
        """ÏøºÎ¶¨ÏóêÏÑú ÎÇ†Ïî®, ÏãúÍ∞ÑÎåÄ, Ïû•ÏÜå Îì± Ï∂îÏ∂ú"""
        analysis = {
            'weather': None,
            'time_of_day': None,
            'location': None,
            'objects': [],
            'activities': []
        }
        
        query_lower = query.lower()
        
        # ÎÇ†Ïî® ÌÇ§ÏõåÎìú
        weather_keywords = {
            'ÎπÑ': 'rainy', 'ÎπÑÍ∞Ä': 'rainy', 'Ïö∞Ï≤ú': 'rainy',
            'ÎßëÏùÄ': 'sunny', 'ÌôîÏ∞ΩÌïú': 'sunny', 'ÌñáÎπõ': 'sunny',
            'ÌùêÎ¶∞': 'cloudy', 'Íµ¨Î¶Ñ': 'cloudy'
        }
        
        # ÏãúÍ∞ÑÎåÄ ÌÇ§ÏõåÎìú
        time_keywords = {
            'Î∞§': 'night', 'ÏïºÍ∞Ñ': 'night', 'Ï†ÄÎÖÅ': 'evening',
            'ÎÇÆ': 'day', 'Ïò§ÌõÑ': 'afternoon', 'ÏïÑÏπ®': 'morning'
        }
        
        # Ïû•ÏÜå ÌÇ§ÏõåÎìú
        location_keywords = {
            'Ïã§ÎÇ¥': 'indoor', 'Í±¥Î¨º': 'indoor', 'Î∞©': 'indoor',
            'Ïã§Ïô∏': 'outdoor', 'ÎèÑÎ°ú': 'outdoor', 'Í±∞Î¶¨': 'outdoor'
        }
        
        for keyword, value in weather_keywords.items():
            if keyword in query_lower:
                analysis['weather'] = value
                break
        
        for keyword, value in time_keywords.items():
            if keyword in query_lower:
                analysis['time_of_day'] = value
                break
                
        for keyword, value in location_keywords.items():
            if keyword in query_lower:
                analysis['location'] = value
                break
        
        return analysis
    
    def _calculate_video_match_score(self, video, query_analysis, filters):
        """ÎπÑÎîîÏò§ÏôÄ ÏøºÎ¶¨ Í∞ÑÏùò Îß§Ïπ≠ Ï†êÏàò Í≥ÑÏÇ∞"""
        score = 0.0
        
        try:
            # Î∂ÑÏÑù Í≤∞Í≥ºÍ∞Ä ÏûàÎäî Í≤ΩÏö∞
            if hasattr(video, 'analysis'):
                stats = video.analysis.analysis_statistics
                scene_types = stats.get('scene_types', [])
                
                # ÎÇ†Ïî® Îß§Ïπ≠
                if query_analysis['weather']:
                    weather_scenes = [s for s in scene_types if query_analysis['weather'] in s.lower()]
                    if weather_scenes:
                        score += 0.4
                
                # ÏãúÍ∞ÑÎåÄ Îß§Ïπ≠
                if query_analysis['time_of_day']:
                    time_scenes = [s for s in scene_types if query_analysis['time_of_day'] in s.lower()]
                    if time_scenes:
                        score += 0.3
                
                # Ïû•ÏÜå Îß§Ïπ≠
                if query_analysis['location']:
                    location_scenes = [s for s in scene_types if query_analysis['location'] in s.lower()]
                    if location_scenes:
                        score += 0.3
            
            return min(score, 1.0)
            
        except Exception:
            return 0.0
    
    def _get_match_reasons(self, video, query_analysis):
        """Îß§Ïπ≠ Ïù¥Ïú† ÏÉùÏÑ±"""
        reasons = []
        
        if query_analysis['weather']:
            reasons.append(f"{query_analysis['weather']} ÎÇ†Ïî® Ï°∞Í±¥")
        if query_analysis['time_of_day']:
            reasons.append(f"{query_analysis['time_of_day']} ÏãúÍ∞ÑÎåÄ")
        if query_analysis['location']:
            reasons.append(f"{query_analysis['location']} ÌôòÍ≤Ω")
            
        return reasons
    
    def _get_video_metadata(self, video):
        """ÎπÑÎîîÏò§ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î∞òÌôò"""
        metadata = {
            'duration': video.duration,
            'file_size': video.file_size,
            'uploaded_at': video.uploaded_at.isoformat(),
            'analysis_type': 'basic'
        }
        
        if hasattr(video, 'analysis'):
            stats = video.analysis.analysis_statistics
            metadata.update({
                'analysis_type': stats.get('analysis_type', 'basic'),
                'scene_types': stats.get('scene_types', []),
                'dominant_objects': stats.get('dominant_objects', [])
            })
        
        return metadata

# views.py - Í≥†Í∏â Í≤ÄÏÉâ Í¥ÄÎ†® Î∑∞ ÏàòÏ†ïÎêú Î≤ÑÏ†Ñ
# views.py - IntraVideoTrackingView Ìñ•ÏÉÅÎêú Î≤ÑÏ†Ñ (ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞ ÏßÄÏõê)

@method_decorator(csrf_exempt, name='dispatch')
class IntraVideoTrackingView(APIView):
    """ÏòÅÏÉÅ ÎÇ¥ Í∞ùÏ≤¥ Ï∂îÏ†Å - Ìñ•ÏÉÅÎêú Î≤ÑÏ†Ñ (ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞ ÏßÄÏõê)"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            tracking_target = request.data.get('tracking_target', '').strip()
            time_range = request.data.get('time_range', {})
            
            logger.info(f"üéØ Í∞ùÏ≤¥ Ï∂îÏ†Å ÏöîÏ≤≠: ÎπÑÎîîÏò§={video_id}, ÎåÄÏÉÅ='{tracking_target}', ÏãúÍ∞ÑÎ≤îÏúÑ={time_range}")
            
            if not video_id or not tracking_target:
                return Response({'error': 'ÎπÑÎîîÏò§ IDÏôÄ Ï∂îÏ†Å ÎåÄÏÉÅÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.'}, status=400)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'}, status=404)
            
            # Frame Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏ Î∞è ÏÉùÏÑ±
            self._ensure_frame_data(video)
            
            # ÌÉÄÍ≤ü Î∂ÑÏÑù (ÏÉâÏÉÅ, Í∞ùÏ≤¥ ÌÉÄÏûÖ Îì± Ï∂îÏ∂ú)
            target_analysis = self._analyze_tracking_target(tracking_target)
            logger.info(f"üìã ÌÉÄÍ≤ü Î∂ÑÏÑù Í≤∞Í≥º: {target_analysis}")
            
            # ÌîÑÎ†àÏûÑÎ≥Ñ Ï∂îÏ†Å Í≤∞Í≥º
            tracking_results = self._perform_object_tracking(video, target_analysis, time_range)
            
            logger.info(f"‚úÖ Í∞ùÏ≤¥ Ï∂îÏ†Å ÏôÑÎ£å: {len(tracking_results)}Í∞ú Í≤∞Í≥º")
            
            # Í≤∞Í≥ºÍ∞Ä ÏóÜÏúºÎ©¥ Îçî Í¥ÄÎåÄÌïú Í≤ÄÏÉâ ÏàòÌñâ
            if not tracking_results:
                logger.info("üîÑ Í¥ÄÎåÄÌïú Í≤ÄÏÉâ Î™®ÎìúÎ°ú Ïû¨ÏãúÎèÑ...")
                tracking_results = self._perform_lenient_tracking(video, target_analysis, time_range)
            
            return Response({
                'video_id': video_id,
                'tracking_target': tracking_target,
                'target_analysis': target_analysis,
                'tracking_results': tracking_results,
                'total_detections': len(tracking_results),
                'search_type': 'object_tracking'
            })
            
        except Exception as e:
            logger.error(f"‚ùå Í∞ùÏ≤¥ Ï∂îÏ†Å Ïò§Î•ò: {e}")
            import traceback
            logger.error(f"üîç ÏÉÅÏÑ∏ Ïò§Î•ò: {traceback.format_exc()}")
            return Response({'error': str(e)}, status=500)
    
    def _ensure_frame_data(self, video):
        """Frame Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏ Î∞è ÏÉùÏÑ±"""
        try:
            frame_count = video.frames.count()
            if frame_count == 0:
                logger.warning(f"‚ö†Ô∏è ÎπÑÎîîÏò§ {video.original_name}Ïóê Frame Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§. ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.")
                from .models import create_dummy_frame_data
                create_dummy_frame_data(video, frame_count=30)
                logger.info(f"‚úÖ ÎçîÎØ∏ Frame Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± ÏôÑÎ£å: 30Í∞ú")
                return True
            else:
                logger.info(f"üìä Í∏∞Ï°¥ Frame Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏: {frame_count}Í∞ú")
                return False
        except Exception as e:
            logger.error(f"‚ùå Frame Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏ Ïã§Ìå®: {e}")
            return False
    
    def _analyze_tracking_target(self, target):
        """Ï∂îÏ†Å ÎåÄÏÉÅ Î∂ÑÏÑù - Ìñ•ÏÉÅÎêú Î≤ÑÏ†Ñ"""
        analysis = {
            'object_type': None,
            'colors': [],
            'gender': None,
            'clothing': [],
            'keywords': target.lower().split(),
            'original_target': target
        }
        
        target_lower = target.lower()
        
        # Í∞ùÏ≤¥ ÌÉÄÏûÖ Îß§Ìïë ÌôïÏû•
        object_mappings = {
            ('ÏÇ¨Îûå', 'ÎÇ®ÏÑ±', 'Ïó¨ÏÑ±', 'Ïù∏Î¨º'): 'person',
            ('Ï∞®', 'ÏûêÎèôÏ∞®', 'Ï∞®Îüâ', 'ÏäπÏö©Ï∞®'): 'car',
            ('ÏûêÏ†ÑÍ±∞',): 'bicycle',
            ('Í∞ú', 'Í∞ïÏïÑÏßÄ', 'Î©çÎ©çÏù¥'): 'dog',
            ('Í≥†ÏñëÏù¥', 'ÎÉ•Ïù¥'): 'cat',
            ('ÏùòÏûê',): 'chair',
            ('ÎÖ∏Ìä∏Î∂Å', 'Ïª¥Ìì®ÌÑ∞'): 'laptop',
            ('Ìï∏ÎìúÌè∞', 'Ìú¥ÎåÄÌè∞', 'Ìè∞'): 'cell_phone'
        }
        
        for keywords, obj_type in object_mappings.items():
            if any(keyword in target_lower for keyword in keywords):
                analysis['object_type'] = obj_type
                break
        
        # ÏÉâÏÉÅ Ï∂îÏ∂ú ÌôïÏû•
        color_keywords = {
            'Îπ®Í∞Ñ': 'red', 'Îπ®Í∞ï': 'red', 'Ï†ÅÏÉâ': 'red',
            'Ï£ºÌô©': 'orange', 'Ïò§Î†åÏßÄ': 'orange',
            'ÎÖ∏ÎûÄ': 'yellow', 'ÎÖ∏Îûë': 'yellow', 'Ìô©ÏÉâ': 'yellow',
            'Ï¥àÎ°ù': 'green', 'ÎÖπÏÉâ': 'green',
            'ÌååÎûÄ': 'blue', 'ÌååÎûë': 'blue', 'Ï≤≠ÏÉâ': 'blue',
            'Î≥¥Îùº': 'purple', 'ÏûêÏ£º': 'purple',
            'Í≤ÄÏùÄ': 'black', 'Í≤ÄÏ†ï': 'black',
            'Ìù∞': 'white', 'ÌïòÏñÄ': 'white', 'Î∞±ÏÉâ': 'white',
            'ÌöåÏÉâ': 'gray', 'Í∑∏Î†àÏù¥': 'gray'
        }
        
        for keyword, color in color_keywords.items():
            if keyword in target_lower:
                analysis['colors'].append(color)
        
        # ÏÑ±Î≥Ñ Î∞è ÏùòÏÉÅ Ï†ïÎ≥¥
        if any(word in target_lower for word in ['ÎÇ®ÏÑ±', 'ÎÇ®Ïûê', 'ÏïÑÏ†ÄÏî®']):
            analysis['gender'] = 'male'
        elif any(word in target_lower for word in ['Ïó¨ÏÑ±', 'Ïó¨Ïûê', 'ÏïÑÏ£ºÎ®∏Îãà']):
            analysis['gender'] = 'female'
        
        if any(word in target_lower for word in ['ÏÉÅÏùò', 'Ìã∞ÏÖîÏ∏†', 'ÏÖîÏ∏†', 'Ïò∑']):
            analysis['clothing'].append('top')
        if any(word in target_lower for word in ['Î™®Ïûê', 'Ï∫°', 'Ìñá']):
            analysis['clothing'].append('hat')
        
        return analysis
    
    def _perform_object_tracking(self, video, target_analysis, time_range):
        """Ïã§Ï†ú Í∞ùÏ≤¥ Ï∂îÏ†Å ÏàòÌñâ - Ìñ•ÏÉÅÎêú Î≤ÑÏ†Ñ"""
        tracking_results = []
        
        try:
            # Frame Î™®Îç∏ÏóêÏÑú Ìï¥Îãπ ÎπÑÎîîÏò§Ïùò ÌîÑÎ†àÏûÑÎì§ Í∞ÄÏ†∏Ïò§Í∏∞
            frames_query = Frame.objects.filter(video=video).order_by('timestamp')
            
            # ÏãúÍ∞Ñ Î≤îÏúÑ ÌïÑÌÑ∞ÎßÅ
            if time_range.get('start') and time_range.get('end'):
                start_time = self._parse_time_to_seconds(time_range['start'])
                end_time = self._parse_time_to_seconds(time_range['end'])
                frames_query = frames_query.filter(timestamp__gte=start_time, timestamp__lte=end_time)
                logger.info(f"‚è∞ ÏãúÍ∞Ñ ÌïÑÌÑ∞ÎßÅ: {start_time}s ~ {end_time}s")
            
            frames = list(frames_query)
            logger.info(f"üìä Î∂ÑÏÑùÌï† ÌîÑÎ†àÏûÑ Ïàò: {len(frames)}Í∞ú")
            
            if not frames:
                logger.warning("‚ö†Ô∏è Î∂ÑÏÑùÌï† ÌîÑÎ†àÏûÑÏù¥ ÏóÜÏäµÎãàÎã§.")
                return []
            
            for frame in frames:
                try:
                    matches = self._find_matching_objects(frame, target_analysis)
                    for match in matches:
                        tracking_results.append({
                            'frame_id': frame.image_id,
                            'timestamp': frame.timestamp,
                            'confidence': match['confidence'],
                            'bbox': match['bbox'],
                            'description': match['description'],
                            'tracking_id': match.get('tracking_id', f"obj_{frame.image_id}"),
                            'match_reasons': match['match_reasons']
                        })
                except Exception as frame_error:
                    logger.warning(f"‚ö†Ô∏è ÌîÑÎ†àÏûÑ {frame.image_id} Ï≤òÎ¶¨ Ïã§Ìå®: {frame_error}")
                    continue
            
            # ÏãúÍ∞ÑÏàú Ï†ïÎ†¨
            tracking_results.sort(key=lambda x: x['timestamp'])
            
            return tracking_results
            
        except Exception as e:
            logger.error(f"‚ùå Ï∂îÏ†Å ÏàòÌñâ Ïò§Î•ò: {e}")
            return []
    
    def _perform_lenient_tracking(self, video, target_analysis, time_range):
        """Í¥ÄÎåÄÌïú Ï∂îÏ†Å Î™®Îìú - Îß§Ïπ≠ Í∏∞Ï§ÄÏùÑ ÎÇÆÏ∂§"""
        try:
            frames_query = Frame.objects.filter(video=video).order_by('timestamp')
            
            if time_range.get('start') and time_range.get('end'):
                start_time = self._parse_time_to_seconds(time_range['start'])
                end_time = self._parse_time_to_seconds(time_range['end'])
                frames_query = frames_query.filter(timestamp__gte=start_time, timestamp__lte=end_time)
            
            frames = list(frames_query)
            tracking_results = []
            
            for frame in frames:
                try:
                    # Í¥ÄÎåÄÌïú Îß§Ïπ≠ Ï°∞Í±¥
                    detected_objects = frame.get_detected_objects()
                    
                    for obj in detected_objects:
                        match_score = 0.0
                        match_reasons = []
                        
                        # Í∞ùÏ≤¥ ÌÉÄÏûÖ Îß§Ïπ≠ (Îçî Í¥ÄÎåÄÌïòÍ≤å)
                        obj_class = obj.get('class', '').lower()
                        if target_analysis.get('object_type'):
                            if target_analysis['object_type'] in obj_class or obj_class in target_analysis['object_type']:
                                match_score += 0.3
                                match_reasons.append(f"{obj_class} Í∞ùÏ≤¥ ÌÉÄÏûÖ Îß§Ïπ≠")
                        
                        # ÌÇ§ÏõåÎìú Îß§Ïπ≠
                        for keyword in target_analysis.get('keywords', []):
                            if keyword in obj_class or any(keyword in str(v) for v in obj.values()):
                                match_score += 0.2
                                match_reasons.append(f"ÌÇ§ÏõåÎìú '{keyword}' Îß§Ïπ≠")
                        
                        # ÏÉâÏÉÅ Îß§Ïπ≠ (Í¥ÄÎåÄÌïòÍ≤å)
                        if target_analysis.get('colors'):
                            color_desc = obj.get('color_description', '').lower()
                            for color in target_analysis['colors']:
                                if color in color_desc or any(color in str(c) for c in obj.get('colors', [])):
                                    match_score += 0.2
                                    match_reasons.append(f"{color} ÏÉâÏÉÅ Îß§Ïπ≠")
                        
                        # ÎÇÆÏùÄ ÏûÑÍ≥ÑÍ∞íÏúºÎ°ú Îß§Ïπ≠ (0.2 Ïù¥ÏÉÅ)
                        if match_score >= 0.2:
                            tracking_results.append({
                                'frame_id': frame.image_id,
                                'timestamp': frame.timestamp,
                                'confidence': min(match_score, obj.get('confidence', 0.5)),
                                'bbox': obj.get('bbox', []),
                                'description': self._generate_match_description(obj, target_analysis),
                                'tracking_id': obj.get('track_id', f"obj_{frame.image_id}"),
                                'match_reasons': match_reasons
                            })
                
                except Exception as frame_error:
                    continue
            
            tracking_results.sort(key=lambda x: x['timestamp'])
            logger.info(f"üîç Í¥ÄÎåÄÌïú Í≤ÄÏÉâ Í≤∞Í≥º: {len(tracking_results)}Í∞ú")
            return tracking_results
            
        except Exception as e:
            logger.error(f"‚ùå Í¥ÄÎåÄÌïú Ï∂îÏ†Å Ïò§Î•ò: {e}")
            return []
    
    def _find_matching_objects(self, frame, target_analysis):
        """ÌîÑÎ†àÏûÑÏóêÏÑú Îß§Ïπ≠ÎêòÎäî Í∞ùÏ≤¥ Ï∞æÍ∏∞ - Ìñ•ÏÉÅÎêú Î≤ÑÏ†Ñ"""
        matches = []
        
        try:
            # detected_objects ÏïàÏ†ÑÌïòÍ≤å Í∞ÄÏ†∏Ïò§Í∏∞
            detected_objects = frame.get_detected_objects()
            
            if not detected_objects:
                return matches
            
            for obj in detected_objects:
                match_score = 0.0
                match_reasons = []
                
                # Í∞ùÏ≤¥ ÌÉÄÏûÖ Îß§Ïπ≠
                if target_analysis.get('object_type') and obj.get('class') == target_analysis['object_type']:
                    match_score += 0.4
                    match_reasons.append(f"{target_analysis['object_type']} Í∞ùÏ≤¥ Îß§Ïπ≠")
                
                # ÏÉâÏÉÅ Îß§Ïπ≠
                if target_analysis.get('colors'):
                    color_desc = obj.get('color_description', '').lower()
                    obj_colors = obj.get('colors', [])
                    
                    for color in target_analysis['colors']:
                        if (color in color_desc or 
                            any(color in str(c).lower() for c in obj_colors)):
                            match_score += 0.3
                            match_reasons.append(f"{color} ÏÉâÏÉÅ Îß§Ïπ≠")
                            break
                
                # ÌÇ§ÏõåÎìú Îß§Ïπ≠ (Î≥¥Ï°∞)
                obj_class = obj.get('class', '').lower()
                for keyword in target_analysis.get('keywords', []):
                    if keyword in obj_class:
                        match_score += 0.2
                        match_reasons.append(f"ÌÇ§ÏõåÎìú '{keyword}' Îß§Ïπ≠")
                        break
                
                # ÏûÑÍ≥ÑÍ∞í Ïù¥ÏÉÅÏù¥Î©¥ Îß§ÏπòÎ°ú Í∞ÑÏ£º
                if match_score > 0.3:
                    matches.append({
                        'confidence': min(match_score, obj.get('confidence', 0.5)),
                        'bbox': obj.get('bbox', []),
                        'description': self._generate_match_description(obj, target_analysis),
                        'match_reasons': match_reasons,
                        'tracking_id': obj.get('track_id', f"obj_{frame.image_id}")
                    })
            
            return matches
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Í∞ùÏ≤¥ Îß§Ïπ≠ Ïò§Î•ò: {e}")
            return []
    
    def _generate_match_description(self, obj, target_analysis):
        """Îß§Ïπ≠ ÏÑ§Î™Ö ÏÉùÏÑ± - Ìñ•ÏÉÅÎêú Î≤ÑÏ†Ñ"""
        desc_parts = []
        
        # ÏÉâÏÉÅ Ï†ïÎ≥¥
        color_desc = obj.get('color_description', '')
        if color_desc and color_desc != 'unknown':
            desc_parts.append(color_desc)
        
        # Í∞ùÏ≤¥ ÌÅ¥ÎûòÏä§
        obj_class = obj.get('class', 'Í∞ùÏ≤¥')
        desc_parts.append(obj_class)
        
        # ÏÑ±Î≥Ñ Ï†ïÎ≥¥ (ÏûàÎäî Í≤ΩÏö∞)
        if target_analysis.get('gender'):
            desc_parts.append(f"({target_analysis['gender']})")
        
        # ÏùòÏÉÅ Ï†ïÎ≥¥ (ÏûàÎäî Í≤ΩÏö∞)
        if target_analysis.get('clothing'):
            clothing_desc = ', '.join(target_analysis['clothing'])
            desc_parts.append(f"[{clothing_desc}]")
        
        description = ' '.join(desc_parts) + ' Í∞êÏßÄ'
        
        return description
    
    def _parse_time_to_seconds(self, time_str):
        """ÏãúÍ∞Ñ Î¨∏ÏûêÏó¥ÏùÑ Ï¥àÎ°ú Î≥ÄÌôò - Ìñ•ÏÉÅÎêú Î≤ÑÏ†Ñ"""
        try:
            if not time_str:
                return 0
            
            time_str = str(time_str).strip()
            
            if ':' in time_str:
                parts = time_str.split(':')
                minutes = int(parts[0])
                seconds = int(parts[1]) if len(parts) > 1 else 0
                return minutes * 60 + seconds
            else:
                # ÏàúÏàò Ïà´ÏûêÏù∏ Í≤ΩÏö∞
                return int(float(time_str))
        except (ValueError, TypeError) as e:
            logger.warning(f"‚ö†Ô∏è ÏãúÍ∞Ñ ÌååÏã± Ïã§Ìå®: {time_str} -> {e}")
            return 0

@method_decorator(csrf_exempt, name='dispatch')
class TimeBasedAnalysisView(APIView):
    """ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù - ÏàòÏ†ïÎêú Î≤ÑÏ†Ñ"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            time_range = request.data.get('time_range', {})
            analysis_type = request.data.get('analysis_type', 'ÏÑ±ÎπÑ Î∂ÑÌè¨')
            
            logger.info(f"üìä ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù ÏöîÏ≤≠: ÎπÑÎîîÏò§={video_id}, ÏãúÍ∞ÑÎ≤îÏúÑ={time_range}, ÌÉÄÏûÖ='{analysis_type}'")
            
            if not video_id or not time_range.get('start') or not time_range.get('end'):
                return Response({'error': 'ÎπÑÎîîÏò§ IDÏôÄ ÏãúÍ∞Ñ Î≤îÏúÑÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.'}, status=400)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({'error': 'ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'}, status=404)
            
            # ÏãúÍ∞Ñ Î≤îÏúÑ ÌååÏã±
            start_time = self._parse_time_to_seconds(time_range['start'])
            end_time = self._parse_time_to_seconds(time_range['end'])
            
            logger.info(f"‚è∞ Î∂ÑÏÑù ÏãúÍ∞Ñ: {start_time}Ï¥à ~ {end_time}Ï¥à")
            
            # Ìï¥Îãπ ÏãúÍ∞ÑÎåÄÏùò ÌîÑÎ†àÏûÑÎì§ Î∂ÑÏÑù
            analysis_result = self._perform_time_based_analysis(
                video, start_time, end_time, analysis_type
            )
            
            logger.info(f"‚úÖ ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù ÏôÑÎ£å")
            
            return Response({
                'video_id': video_id,
                'time_range': time_range,
                'analysis_type': analysis_type,
                'result': analysis_result,
                'search_type': 'time_analysis'
            })
            
        except Exception as e:
            logger.error(f"‚ùå ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù Ïò§Î•ò: {e}")
            return Response({'error': str(e)}, status=500)
    
    def _perform_time_based_analysis(self, video, start_time, end_time, analysis_type):
        """ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù ÏàòÌñâ"""
        
        # Ìï¥Îãπ ÏãúÍ∞ÑÎåÄ ÌîÑÎ†àÏûÑÎì§ Í∞ÄÏ†∏Ïò§Í∏∞
        frames = Frame.objects.filter(
            video=video,
            timestamp__gte=start_time,
            timestamp__lte=end_time
        ).order_by('timestamp')
        
        frame_list = list(frames)
        logger.info(f"üìä Î∂ÑÏÑù ÎåÄÏÉÅ ÌîÑÎ†àÏûÑ: {len(frame_list)}Í∞ú")
        
        if 'ÏÑ±ÎπÑ' in analysis_type or 'ÏÇ¨Îûå' in analysis_type:
            return self._analyze_gender_distribution(frame_list, start_time, end_time)
        elif 'Ï∞®Îüâ' in analysis_type or 'ÍµêÌÜµ' in analysis_type:
            return self._analyze_vehicle_distribution(frame_list, start_time, end_time)
        else:
            return self._analyze_general_statistics(frame_list, start_time, end_time)
    
    def _analyze_gender_distribution(self, frames, start_time, end_time):
        """ÏÑ±ÎπÑ Î∂ÑÏÑù"""
        person_detections = []
        
        for frame in frames:
            if not hasattr(frame, 'detected_objects') or not frame.detected_objects:
                continue
                
            for obj in frame.detected_objects:
                if obj.get('class') == 'person':
                    person_detections.append({
                        'timestamp': frame.timestamp,
                        'confidence': obj.get('confidence', 0.5),
                        'bbox': obj.get('bbox', []),
                        'colors': obj.get('colors', []),
                        'color_description': obj.get('color_description', '')
                    })
        
        # ÏÑ±Î≥Ñ Ï∂îÏ†ï (Í∞ÑÎã®Ìïú Ìú¥Î¶¨Ïä§Ìã± - Ïã§Ï†úÎ°úÎäî Îçî Ï†ïÍµêÌïú AI Î™®Îç∏ ÌïÑÏöî)
        male_count = 0
        female_count = 0
        
        for detection in person_detections:
            # ÏÉâÏÉÅ Í∏∞Î∞ò Í∞ÑÎã®Ìïú ÏÑ±Î≥Ñ Ï∂îÏ†ï
            colors = detection['color_description'].lower()
            if 'blue' in colors or 'black' in colors or 'gray' in colors:
                male_count += 1
            elif 'pink' in colors or 'red' in colors:
                female_count += 1
            else:
                # 50:50ÏúºÎ°ú Î∂ÑÎ∞∞
                if len(person_detections) % 2 == 0:
                    male_count += 1
                else:
                    female_count += 1
        
        total_persons = male_count + female_count
        
        # ÏùòÏÉÅ ÏÉâÏÉÅ Î∂ÑÌè¨
        clothing_colors = {}
        for detection in person_detections:
            color = detection['color_description']
            if color and color != 'unknown':
                clothing_colors[color] = clothing_colors.get(color, 0) + 1
        
        # ÌîºÌÅ¨ ÏãúÍ∞ÑÎåÄ Î∂ÑÏÑù
        time_distribution = {}
        for detection in person_detections:
            time_bucket = int(detection['timestamp'] // 30) * 30  # 30Ï¥à Îã®ÏúÑ
            time_distribution[time_bucket] = time_distribution.get(time_bucket, 0) + 1
        
        peak_times = sorted(time_distribution.items(), key=lambda x: x[1], reverse=True)[:2]
        peak_time_strings = [f"{self._seconds_to_time_string(t[0])}-{self._seconds_to_time_string(t[0]+30)}" 
                           for t in peak_times]
        
        return {
            'total_persons': total_persons,
            'male_count': male_count,
            'female_count': female_count,
            'gender_ratio': {
                'male': round((male_count / total_persons * 100), 1) if total_persons > 0 else 0,
                'female': round((female_count / total_persons * 100), 1) if total_persons > 0 else 0
            },
            'clothing_colors': dict(sorted(clothing_colors.items(), key=lambda x: x[1], reverse=True)),
            'peak_times': peak_time_strings,
            'movement_patterns': 'left_to_right_dominant',  # Í∞ÑÎã®Ìïú ÏòàÏãú
            'analysis_period': f"{self._seconds_to_time_string(start_time)} - {self._seconds_to_time_string(end_time)}"
        }
    
    def _analyze_vehicle_distribution(self, frames, start_time, end_time):
        """Ï∞®Îüâ Î∂ÑÌè¨ Î∂ÑÏÑù"""
        vehicles = []
        
        for frame in frames:
            if not hasattr(frame, 'detected_objects') or not frame.detected_objects:
                continue
                
            for obj in frame.detected_objects:
                if obj.get('class') in ['car', 'truck', 'bus', 'motorcycle']:
                    vehicles.append({
                        'type': obj.get('class'),
                        'timestamp': frame.timestamp,
                        'confidence': obj.get('confidence', 0.5)
                    })
        
        vehicle_types = {}
        for v in vehicles:
            vehicle_types[v['type']] = vehicle_types.get(v['type'], 0) + 1
        
        duration_minutes = (end_time - start_time) / 60
        
        return {
            'total_vehicles': len(vehicles),
            'vehicle_types': vehicle_types,
            'average_per_minute': round(len(vehicles) / max(1, duration_minutes), 1),
            'analysis_period': f"{self._seconds_to_time_string(start_time)} - {self._seconds_to_time_string(end_time)}"
        }
    
    def _analyze_general_statistics(self, frames, start_time, end_time):
        """ÏùºÎ∞ò ÌÜµÍ≥Ñ Î∂ÑÏÑù"""
        all_objects = []
        
        for frame in frames:
            if hasattr(frame, 'detected_objects') and frame.detected_objects:
                all_objects.extend(frame.detected_objects)
        
        object_counts = {}
        for obj in all_objects:
            obj_class = obj.get('class', 'unknown')
            object_counts[obj_class] = object_counts.get(obj_class, 0) + 1
        
        return {
            'total_objects': len(all_objects),
            'object_distribution': dict(sorted(object_counts.items(), key=lambda x: x[1], reverse=True)),
            'frames_analyzed': len(frames),
            'average_objects_per_frame': round(len(all_objects) / max(1, len(frames)), 1),
            'analysis_period': f"{self._seconds_to_time_string(start_time)} - {self._seconds_to_time_string(end_time)}"
        }
    
    def _parse_time_to_seconds(self, time_str):
        """ÏãúÍ∞Ñ Î¨∏ÏûêÏó¥ÏùÑ Ï¥àÎ°ú Î≥ÄÌôò"""
        try:
            if ':' in time_str:
                parts = time_str.split(':')
                minutes = int(parts[0])
                seconds = int(parts[1]) if len(parts) > 1 else 0
                return minutes * 60 + seconds
            else:
                return int(time_str)
        except:
            return 0
    
    def _seconds_to_time_string(self, seconds):
        """Ï¥àÎ•º ÏãúÍ∞Ñ Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò"""
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes}:{secs:02d}"


@method_decorator(csrf_exempt, name='dispatch')
class CrossVideoSearchView(APIView):
    """ÏòÅÏÉÅ Í∞Ñ Í≤ÄÏÉâ - ÏàòÏ†ïÎêú Î≤ÑÏ†Ñ"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            query = request.data.get('query', '').strip()
            search_filters = request.data.get('filters', {})
            
            logger.info(f"üîç ÌÅ¨Î°úÏä§ ÎπÑÎîîÏò§ Í≤ÄÏÉâ ÏöîÏ≤≠: '{query}'")
            
            if not query:
                return Response({'error': 'Í≤ÄÏÉâÏñ¥Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'}, status=400)
            
            # ÏøºÎ¶¨ Î∂ÑÏÑù
            query_analysis = self._analyze_query(query)
            
            # Î∂ÑÏÑùÎêú ÎπÑÎîîÏò§Îì§ Ï§ëÏóêÏÑú Í≤ÄÏÉâ
            videos = Video.objects.filter(is_analyzed=True)
            matching_videos = []
            
            for video in videos:
                match_score = self._calculate_video_match_score(video, query_analysis, search_filters)
                if match_score > 0.3:  # ÏûÑÍ≥ÑÍ∞í
                    matching_videos.append({
                        'video_id': video.id,
                        'video_name': video.original_name,
                        'match_score': match_score,
                        'match_reasons': self._get_match_reasons(video, query_analysis),
                        'metadata': self._get_video_metadata(video),
                        'thumbnail_url': f'/frame/{video.id}/100/',
                    })
            
            # Ï†êÏàòÏàú Ï†ïÎ†¨
            matching_videos.sort(key=lambda x: x['match_score'], reverse=True)
            
            logger.info(f"‚úÖ ÌÅ¨Î°úÏä§ ÎπÑÎîîÏò§ Í≤ÄÏÉâ ÏôÑÎ£å: {len(matching_videos)}Í∞ú Í≤∞Í≥º")
            
            return Response({
                'query': query,
                'total_matches': len(matching_videos),
                'results': matching_videos[:20],  # ÏÉÅÏúÑ 20Í∞ú
                'query_analysis': query_analysis,
                'search_type': 'cross_video'
            })
            
        except Exception as e:
            logger.error(f"‚ùå ÌÅ¨Î°úÏä§ ÎπÑÎîîÏò§ Í≤ÄÏÉâ Ïò§Î•ò: {e}")
            return Response({'error': str(e)}, status=500)
    
    def _analyze_query(self, query):
        """ÏøºÎ¶¨ÏóêÏÑú ÎÇ†Ïî®, ÏãúÍ∞ÑÎåÄ, Ïû•ÏÜå Îì± Ï∂îÏ∂ú"""
        analysis = {
            'weather': None,
            'time_of_day': None,
            'location': None,
            'objects': [],
            'activities': []
        }
        
        query_lower = query.lower()
        
        # ÎÇ†Ïî® ÌÇ§ÏõåÎìú
        weather_keywords = {
            'ÎπÑ': 'rainy', 'ÎπÑÍ∞Ä': 'rainy', 'Ïö∞Ï≤ú': 'rainy',
            'ÎßëÏùÄ': 'sunny', 'ÌôîÏ∞ΩÌïú': 'sunny', 'ÌñáÎπõ': 'sunny',
            'ÌùêÎ¶∞': 'cloudy', 'Íµ¨Î¶Ñ': 'cloudy'
        }
        
        # ÏãúÍ∞ÑÎåÄ ÌÇ§ÏõåÎìú
        time_keywords = {
            'Î∞§': 'night', 'ÏïºÍ∞Ñ': 'night', 'Ï†ÄÎÖÅ': 'evening',
            'ÎÇÆ': 'day', 'Ïò§ÌõÑ': 'afternoon', 'ÏïÑÏπ®': 'morning'
        }
        
        # Ïû•ÏÜå ÌÇ§ÏõåÎìú
        location_keywords = {
            'Ïã§ÎÇ¥': 'indoor', 'Í±¥Î¨º': 'indoor', 'Î∞©': 'indoor',
            'Ïã§Ïô∏': 'outdoor', 'ÎèÑÎ°ú': 'outdoor', 'Í±∞Î¶¨': 'outdoor'
        }
        
        for keyword, value in weather_keywords.items():
            if keyword in query_lower:
                analysis['weather'] = value
                break
        
        for keyword, value in time_keywords.items():
            if keyword in query_lower:
                analysis['time_of_day'] = value
                break
                
        for keyword, value in location_keywords.items():
            if keyword in query_lower:
                analysis['location'] = value
                break
        
        return analysis
    
    def _calculate_video_match_score(self, video, query_analysis, filters):
        """ÎπÑÎîîÏò§ÏôÄ ÏøºÎ¶¨ Í∞ÑÏùò Îß§Ïπ≠ Ï†êÏàò Í≥ÑÏÇ∞"""
        score = 0.0
        
        try:
            # VideoAnalysisÏóêÏÑú Î∂ÑÏÑù Í≤∞Í≥ºÍ∞Ä ÏûàÎäî Í≤ΩÏö∞
            if hasattr(video, 'analysis'):
                analysis = video.analysis
                stats = analysis.analysis_statistics
                scene_types = stats.get('scene_types', [])
                
                # ÎÇ†Ïî® Îß§Ïπ≠
                if query_analysis['weather']:
                    weather_scenes = [s for s in scene_types if query_analysis['weather'] in s.lower()]
                    if weather_scenes:
                        score += 0.4
                
                # ÏãúÍ∞ÑÎåÄ Îß§Ïπ≠
                if query_analysis['time_of_day']:
                    time_scenes = [s for s in scene_types if query_analysis['time_of_day'] in s.lower()]
                    if time_scenes:
                        score += 0.3
                
                # Ïû•ÏÜå Îß§Ïπ≠
                if query_analysis['location']:
                    location_scenes = [s for s in scene_types if query_analysis['location'] in s.lower()]
                    if location_scenes:
                        score += 0.3
            
            return min(score, 1.0)
            
        except Exception:
            return 0.0
    
    def _get_match_reasons(self, video, query_analysis):
        """Îß§Ïπ≠ Ïù¥Ïú† ÏÉùÏÑ±"""
        reasons = []
        
        if query_analysis['weather']:
            reasons.append(f"{query_analysis['weather']} ÎÇ†Ïî® Ï°∞Í±¥")
        if query_analysis['time_of_day']:
            reasons.append(f"{query_analysis['time_of_day']} ÏãúÍ∞ÑÎåÄ")
        if query_analysis['location']:
            reasons.append(f"{query_analysis['location']} ÌôòÍ≤Ω")
            
        return reasons
    
    def _get_video_metadata(self, video):
        """ÎπÑÎîîÏò§ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î∞òÌôò"""
        metadata = {
            'duration': video.duration,
            'file_size': video.file_size,
            'uploaded_at': video.uploaded_at.isoformat(),
            'analysis_type': 'basic'
        }
        
        if hasattr(video, 'analysis'):
            stats = video.analysis.analysis_statistics
            metadata.update({
                'analysis_type': stats.get('analysis_type', 'basic'),
                'scene_types': stats.get('scene_types', []),
                'dominant_objects': stats.get('dominant_objects', [])
            })
        
        return metadata


class AdvancedSearchAutoView(APIView):
    """ÌÜµÌï© Í≥†Í∏â Í≤ÄÏÉâ - ÏûêÎèô ÌÉÄÏûÖ Í∞êÏßÄ"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            query = request.data.get('query', '').strip()
            video_id = request.data.get('video_id')
            time_range = request.data.get('time_range', {})
            options = request.data.get('options', {})
            
            if not query:
                return Response({'error': 'Í≤ÄÏÉâÏñ¥Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'}, status=400)
            
            # Í≤ÄÏÉâ ÌÉÄÏûÖ ÏûêÎèô Í∞êÏßÄ
            search_type = self._detect_search_type(query, video_id, time_range, options)
            
            # Ìï¥Îãπ Í≤ÄÏÉâ ÌÉÄÏûÖÏóê Îî∞Îùº Ï†ÅÏ†àÌïú View Ìò∏Ï∂ú
            if search_type == 'cross-video':
                view = CrossVideoSearchView()
                return view.post(request)
            elif search_type == 'object-tracking':
                view = IntraVideoTrackingView()
                return view.post(request)
            elif search_type == 'time-analysis':
                view = TimeBasedAnalysisView()
                return view.post(request)
            else:
                # Í∏∞Î≥∏ Í≤ÄÏÉâÏúºÎ°ú fallback
                view = EnhancedVideoChatView()
                return view.post(request)
                
        except Exception as e:
            return Response({'error': str(e)}, status=500)
    
    def _detect_search_type(self, query, video_id, time_range, options):
        """Í≤ÄÏÉâ ÌÉÄÏûÖ ÏûêÎèô Í∞êÏßÄ Î°úÏßÅ"""
        query_lower = query.lower()
        
        # ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù ÌÇ§ÏõåÎìú
        time_analysis_keywords = [
            'ÏÑ±ÎπÑ', 'Î∂ÑÌè¨', 'ÌÜµÍ≥Ñ', 'ÏãúÍ∞ÑÎåÄ', 'Íµ¨Í∞Ñ', 'ÏÇ¨Ïù¥', 
            'Î™áÎ™Ö', 'ÏñºÎßàÎÇò', 'ÌèâÍ∑†', 'ÎπÑÏú®', 'Ìå®ÌÑ¥', 'Î∂ÑÏÑù'
        ]
        
        # Í∞ùÏ≤¥ Ï∂îÏ†Å ÌÇ§ÏõåÎìú
        tracking_keywords = [
            'Ï∂îÏ†Å', 'Îî∞ÎùºÍ∞Ä', 'Ïù¥Îèô', 'Í≤ΩÎ°ú', 'ÏßÄÎÇòÍ∞Ñ', 
            'ÏÉÅÏùò', 'Î™®Ïûê', 'ÏÉâÍπî', 'Ïò∑', 'ÏÇ¨Îûå', 'Ï∞®Îüâ'
        ]
        
        # ÏòÅÏÉÅ Í∞Ñ Í≤ÄÏÉâ ÌÇ§ÏõåÎìú
        cross_video_keywords = [
            'Ï¥¨ÏòÅÎêú', 'ÏòÅÏÉÅ', 'ÎπÑÎîîÏò§', 'Ï∞æÏïÑ', 'ÎπÑÍ∞Ä', 'Î∞§', 
            'ÎÇÆ', 'Ïã§ÎÇ¥', 'Ïã§Ïô∏', 'Ïû•ÏÜå', 'ÎÇ†Ïî®'
        ]
        
        # ÏãúÍ∞Ñ Î≤îÏúÑÍ∞Ä ÏûàÍ≥† Î∂ÑÏÑù ÌÇ§ÏõåÎìúÍ∞Ä ÏûàÏúºÎ©¥ ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÏÑù
        if (time_range.get('start') and time_range.get('end')) or \
           any(keyword in query_lower for keyword in time_analysis_keywords):
            return 'time-analysis'
        
        # ÌäπÏ†ï ÎπÑÎîîÏò§ IDÍ∞Ä ÏûàÍ≥† Ï∂îÏ†Å ÌÇ§ÏõåÎìúÍ∞Ä ÏûàÏúºÎ©¥ Í∞ùÏ≤¥ Ï∂îÏ†Å
        if video_id and any(keyword in query_lower for keyword in tracking_keywords):
            return 'object-tracking'
        
        # ÌÅ¨Î°úÏä§ ÎπÑÎîîÏò§ ÌÇ§ÏõåÎìúÍ∞Ä ÏûàÏúºÎ©¥ ÏòÅÏÉÅ Í∞Ñ Í≤ÄÏÉâ
        if any(keyword in query_lower for keyword in cross_video_keywords):
            return 'cross-video'
        
        # Í∏∞Î≥∏Í∞í: ÎπÑÎîîÏò§ IDÍ∞Ä ÏûàÏúºÎ©¥ Ï∂îÏ†Å, ÏóÜÏúºÎ©¥ ÌÅ¨Î°úÏä§ ÎπÑÎîîÏò§
        return 'object-tracking' if video_id else 'cross-video'
