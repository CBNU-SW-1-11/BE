import time
from django.conf import settings
from groq import Groq
import openai
import anthropic

class LLMClient:
    """Enhanced LLM í´ë¼ì´ì–¸íŠ¸ - Django ë²„ì „"""
    
    def __init__(self):
        # Initialize clients
        self.groq_client = Groq(api_key=settings.GROQ_API_KEY)
        
        # OpenAI client
        try:
            self.openai_client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
            self.openai_available = True
        except Exception as e:
            print(f"âš ï¸ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.openai_client = None
            self.openai_available = False
        
        # Anthropic client
        try:
            self.anthropic_client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
            self.anthropic_available = True
        except Exception as e:
            print(f"âš ï¸ Anthropic ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.anthropic_client = None
            self.anthropic_available = False
    
    def call_groq_llm_enhanced(self, prompt, system_prompt="", model="llama3-70b-8192", max_retries=3):
        """Groq LLM í˜¸ì¶œ with fallback"""
        
        # Groq ì‹œë„
        for attempt in range(max_retries):
            try:
                response = self.groq_client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=1024,
                    top_p=0.9
                )
                content = response.choices[0].message.content.strip()
                # print(f"âœ… Groq API ì„±ê³µ (ì‹œë„ {attempt + 1})")
                return content
                
            except Exception as e:
                print(f"âš ï¸ Groq API ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(1)
                    continue
                else:
                    break
        
        # OpenAI fallback
        if self.openai_available and self.openai_client:
            try:
                response = self.openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=1024
                )
                content = response.choices[0].message.content.strip()
                print("âœ… OpenAI API fallback ì„±ê³µ")
                return content
                
            except Exception as e:
                print(f"âŒ OpenAI APIë„ ì‹¤íŒ¨: {e}")
        
        # Anthropic fallback
        if self.anthropic_available and self.anthropic_client:
            try:
                response = self.anthropic_client.messages.create(
                    model="claude-3-haiku-20240307",
                    max_tokens=1024,
                    temperature=0.7,
                    system=system_prompt,
                    messages=[{"role": "user", "content": prompt}]
                )
                content = response.content[0].text.strip()
                print("âœ… Anthropic API fallback ì„±ê³µ")
                return content
                
            except Exception as e:
                print(f"âŒ Anthropic APIë„ ì‹¤íŒ¨: {e}")
        
        # ëª¨ë“  API ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì‘ë‹µ
        return self._generate_fallback_response(prompt)
    
    def _generate_fallback_response(self, prompt):
        """ê¸°ë³¸ ì‘ë‹µ ìƒì„±"""
        if "ê²€ìƒ‰" in prompt or "find" in prompt.lower():
            return "API í˜¸ì¶œì— ì‹¤íŒ¨í–ˆì§€ë§Œ, ê²€ìƒ‰ ê¸°ëŠ¥ì€ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”."
        elif "í•˜ì´ë¼ì´íŠ¸" in prompt or "highlight" in prompt.lower():
            return "í•˜ì´ë¼ì´íŠ¸ ê°ì§€ ê¸°ëŠ¥ì´ ì‹¤í–‰ë˜ì—ˆì§€ë§Œ, AI ë¶„ì„ ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤."
        elif "ìš”ì•½" in prompt or "summary" in prompt.lower():
            return "ë¹„ë””ì˜¤ ìš”ì•½ ê¸°ëŠ¥ì´ ìš”ì²­ë˜ì—ˆìŠµë‹ˆë‹¤. AI ì„œë¹„ìŠ¤ ë³µêµ¬ í›„ ë” ìì„¸í•œ ë¶„ì„ì„ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤."
        else:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. AI ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆì–´ ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def call_multi_llm_enhanced(self, prompt, system_prompt=""):
        """ë‹¤ì¤‘ LLM í˜¸ì¶œ"""
        results = []
        
        # Groq ì‹œë„
        try:
            res_groq = self.call_groq_llm_enhanced(
                prompt, 
                system_prompt + "\në°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.", 
                model="llama3-70b-8192"
            )
            if "API í˜¸ì¶œì— ì‹¤íŒ¨" not in res_groq:
                results.append("ğŸ¤– Groq (Llama3-70B): " + res_groq)
        except Exception as e:
            results.append(f"âš ï¸ Groq ì˜¤ë¥˜: {e}")

        # OpenAI ì‹œë„
        if self.openai_available and self.openai_client:
            try:
                res_gpt = self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt + "\në°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=1024
                )
                results.append("ğŸ§  GPT-4o: " + res_gpt.choices[0].message.content.strip())
            except Exception as e:
                results.append(f"âš ï¸ GPT ì˜¤ë¥˜: {e}")

        # Anthropic ì‹œë„
        if self.anthropic_available and self.anthropic_client:
            try:
                res_claude = self.anthropic_client.messages.create(
                    model="claude-3-opus-20240229",
                    max_tokens=1024,
                    temperature=0.7,
                    system=system_prompt + "\në°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.",
                    messages=[{"role": "user", "content": prompt}]
                )
                results.append("ğŸ­ Claude Opus: " + res_claude.content[0].text.strip())
            except Exception as e:
                results.append(f"âš ï¸ Claude ì˜¤ë¥˜: {e}")

        # ê²°ê³¼ í†µí•©
        if results:
            return "\n\n".join(results)
        else:
            return "ëª¨ë“  AI ì„œë¹„ìŠ¤ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def generate_smart_response(self, user_query, search_results=None, video_info=None, use_multi_llm=False):
        """ìŠ¤ë§ˆíŠ¸í•œ LLM ì‘ë‹µ ìƒì„±"""
        system_prompt = """ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
        ê²€ìƒ‰ ê²°ê³¼ë‚˜ ë¹„ë””ì˜¤ ì •ë³´ê°€ ì œê³µë˜ë©´ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”."""
        
        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        user_prompt = f"ì‚¬ìš©ì ì§ˆë¬¸: {user_query}\n"
        
        if search_results:
            user_prompt += f"\nğŸ” ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„:\n"
            for i, result in enumerate(search_results[:3], 1):
                user_prompt += f"{i}. í”„ë ˆì„ {result.get('frame_id', 'N/A')} ({result.get('timestamp', 0):.1f}ì´ˆ)\n"
                user_prompt += f"   - ë§¤ì¹­ ì ìˆ˜: {result.get('match_score', 0)}\n"
                user_prompt += f"   - ê°ì§€ëœ ê°ì²´: {', '.join(result.get('detected_objects', []))}\n"
                
                # Enhanced ìº¡ì…˜ ì •ë³´ í™œìš©
                if 'caption' in result:
                    user_prompt += f"   - ì„¤ëª…: {result['caption'][:100]}...\n"
        
        if video_info:
            user_prompt += f"ğŸ“Š ë¹„ë””ì˜¤ ì •ë³´:\n{video_info}\n\n"
        
        user_prompt += """
        ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ë‹µë³€í•´ì£¼ì„¸ìš”:
        1. ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆë‹¤ë©´ êµ¬ì²´ì ì¸ ì‹œê°„ëŒ€ì™€ ë‚´ìš©ì„ ì–¸ê¸‰
        2. ë°œê²¬ëœ ê°ì²´ë‚˜ ì¥ë©´ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…
        3. í•„ìš”í•˜ë‹¤ë©´ ì¶”ê°€ ê²€ìƒ‰ ë°©ë²•ì´ë‚˜ ê´€ë ¨ ê¸°ëŠ¥ ì•ˆë‚´
        4. ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ì‘ì„±
        
        ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”.
        """
        
        # ë©€í‹° LLM ë˜ëŠ” ë‹¨ì¼ LLM ì„ íƒ
        if use_multi_llm:
            return self.call_multi_llm_enhanced(user_prompt, system_prompt)
        else:
            return self.call_groq_llm_enhanced(user_prompt, system_prompt)
    
    def get_api_status(self):
        """API ìƒíƒœ í™•ì¸"""
        status = {
            'groq': {'available': True, 'status': 'ok'},
            'openai': {'available': self.openai_available, 'status': 'ok' if self.openai_available else 'unavailable'},
            'anthropic': {'available': self.anthropic_available, 'status': 'ok' if self.anthropic_available else 'unavailable'}
        }
        
        # Groq API í…ŒìŠ¤íŠ¸
        try:
            test_response = self.call_groq_llm_enhanced("í…ŒìŠ¤íŠ¸", "ê°„ë‹¨íˆ 'ì •ìƒ'ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.", max_retries=1)
            if "ì •ìƒ" not in test_response and "ëª¨ë“  AI ì„œë¹„ìŠ¤" in test_response:
                status['groq']['status'] = 'error'
        except:
            status['groq']['status'] = 'error'
        
        return status